{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8befa2-c90d-4f9b-94f3-706a43529c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture Lab: Bronze ‚Üí Silver ‚Üí Gold (PySpark) ü•âü•àü•á\n",
    "\n",
    "Welcome to the Medallion Architecture lab! In this hands-on lab, you'll build a complete data pipeline using PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. ‚úÖ Understand the medallion architecture (bronze, silver, gold)\n",
    "2. ‚úÖ Use **Auto Loader** to ingest raw data into bronze tables\n",
    "3. ‚úÖ Use **Delta merge API** to clean and deduplicate data for silver tables\n",
    "4. ‚úÖ Create aggregated business metrics in gold tables\n",
    "5. ‚úÖ Build an end-to-end data pipeline using PySpark\n",
    "6. ‚úÖ Apply data quality checks and transformations\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è What is Medallion Architecture?\n",
    "\n",
    "The **Medallion Architecture** organizes data into three layers:\n",
    "\n",
    "### **ü•â Bronze Layer (Raw)**\n",
    "* **Purpose:** Ingest raw data as-is\n",
    "* **Characteristics:** Minimal processing, append-only, full history\n",
    "* **Quality:** May have duplicates, nulls, bad data\n",
    "* **Method:** Auto Loader for incremental ingestion\n",
    "\n",
    "### **ü•à Silver Layer (Cleaned)**\n",
    "* **Purpose:** Clean, validate, and deduplicate\n",
    "* **Characteristics:** Business-ready, no duplicates, validated\n",
    "* **Quality:** High quality, consistent schema\n",
    "* **Method:** Delta merge API for upserts and deduplication\n",
    "\n",
    "### **ü•á Gold Layer (Aggregated)**\n",
    "* **Purpose:** Business-level aggregations and metrics\n",
    "* **Characteristics:** Optimized for analytics, pre-aggregated\n",
    "* **Quality:** Report-ready, fast queries\n",
    "* **Method:** PySpark aggregations and DataFrame API\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Lab Scenario: E-Commerce Orders\n",
    "\n",
    "You're building a data pipeline for an e-commerce company:\n",
    "\n",
    "**Raw data:** Order files arrive in cloud storage (CSV format)\n",
    "\n",
    "**Your pipeline:**\n",
    "1. **Bronze:** Ingest raw order files with Auto Loader\n",
    "2. **Silver:** Clean data, remove duplicates, validate with Delta merge\n",
    "3. **Gold:** Create daily sales metrics using PySpark aggregations\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Lab Structure\n",
    "\n",
    "This lab has **10 tasks** to complete:\n",
    "\n",
    "**Setup (Tasks 1-2):**\n",
    "1. Create volumes and generate sample order files\n",
    "2. Explore the raw data\n",
    "\n",
    "**Bronze Layer (Tasks 3-4):**\n",
    "3. Create bronze table\n",
    "4. Use Auto Loader to ingest raw data\n",
    "\n",
    "**Silver Layer (Tasks 5-6):**\n",
    "5. Create silver table with data quality rules\n",
    "6. Use Delta merge API to clean and deduplicate\n",
    "\n",
    "**Gold Layer (Tasks 7-8):**\n",
    "7. Create gold table with daily metrics\n",
    "8. Build aggregation with PySpark\n",
    "\n",
    "**Validation (Tasks 9-10):**\n",
    "9. Verify data quality across layers\n",
    "10. Test incremental updates\n",
    "\n",
    "**Each task includes:**\n",
    "* üìù Clear instructions\n",
    "* üí° Hints to guide you\n",
    "* ‚úÖ Solutions at the end (try first!)\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "348e84c2-d2a2-4f09-8c41-de9d5565ef7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Setup - Create Volume and Generate Raw Data üõ†Ô∏è\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Create a Unity Catalog volume and generate sample order files to simulate raw data arriving from an e-commerce system.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "**Part A: Create Volume**\n",
    "1. Create a volume: `main.default.ecommerce_raw_data`\n",
    "2. This will store your raw CSV files\n",
    "\n",
    "**Part B: Generate Sample Order Files**\n",
    "1. Create 2 batches of order data (simulating data arriving at different times)\n",
    "2. Each batch should be a CSV file with these columns:\n",
    "   * `order_id` - INT\n",
    "   * `customer_id` - INT\n",
    "   * `order_date` - STRING (YYYY-MM-DD format)\n",
    "   * `product_name` - STRING\n",
    "   * `quantity` - INT\n",
    "   * `unit_price` - DOUBLE\n",
    "   * `status` - STRING ('completed', 'pending', 'cancelled')\n",
    "\n",
    "**Batch 1:** 100 orders (order_id 1-100)  \n",
    "**Batch 2:** 50 orders (order_id 101-150), **including 5 duplicates** from Batch 1 (order_id 1-5)\n",
    "\n",
    "**Data quality issues to include:**\n",
    "* Some null values in product_name\n",
    "* Some negative quantities (data errors)\n",
    "* Duplicates in Batch 2\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d24f48f8-4c49-4b3a-af26-d596ffc9d46b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Setup"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create volume and generate 2 batches of CSV files\n",
    "# Batch 1: 100 orders\n",
    "# Batch 2: 50 orders + 5 duplicates from Batch 1\n",
    "# Include data quality issues (nulls, negatives, duplicates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239b1454-1938-476c-a207-274663349e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 1\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Create volume (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  CREATE VOLUME IF NOT EXISTS main.default.ecommerce_raw_data\n",
    "  COMMENT 'Raw order files for medallion architecture lab'\n",
    "\"\"\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Generate sample data (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "products = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones', None]  # Include None for nulls\n",
    "statuses = ['completed', 'pending', 'cancelled']\n",
    "\n",
    "data_batch1 = [\n",
    "    (i, \n",
    "     random.randint(1, 50),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 30))).strftime(\"%Y-%m-%d\"),\n",
    "     random.choice(products),\n",
    "     random.randint(-2, 10),  # Include negative for errors\n",
    "     round(random.uniform(10, 500), 2),\n",
    "     random.choice(statuses))\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Write CSV files (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"order_date\", StringType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"unit_price\", DoubleType()),\n",
    "    StructField(\"status\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/main/default/ecommerce_raw_data/batch1\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 4:</b> Create duplicates in Batch 2 (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Batch 2: New orders + duplicates from Batch 1\n",
    "data_batch2 = data_batch1[0:5]  # First 5 orders (duplicates)\n",
    "data_batch2 += [(i, ...) for i in range(101, 151)]  # New orders\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a797020-5b76-484d-b4f7-11e3feb77cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Explore the Raw Data üîç\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Examine the raw CSV files you created to understand the data quality issues using PySpark.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use `spark.read.csv()` to load the CSV files\n",
    "2. Look for:\n",
    "   * Null values in product_name\n",
    "   * Negative quantities\n",
    "   * Duplicate order_ids (between batch1 and batch2)\n",
    "3. Count total rows across both batches\n",
    "\n",
    "**Questions to answer:**\n",
    "* How many total rows are in the raw files?\n",
    "* How many rows have null product_name?\n",
    "* How many rows have negative quantity?\n",
    "* Are there duplicate order_ids?\n",
    "\n",
    "**PySpark approach:**\n",
    "```python\n",
    "df = spark.read.csv(\n",
    "    \"/Volumes/main/default/ecommerce_raw_data/\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a69a2380-0a74-43e2-b86b-fd539cbb867e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Explore Raw Data"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Use spark.read.csv() to explore the raw CSV data\n",
    "# Check for nulls, negatives, and duplicates\n",
    "# Use display() to show results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d946bbd-91f4-4057-b524-66a7ad2b07c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 2\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Read all CSV files (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "df_raw = spark.read.csv(\n",
    "    \"/Volumes/main/default/ecommerce_raw_data/\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "display(df_raw.limit(20))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Count data quality issues (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "quality_check = df_raw.select(\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") < 0, 1)).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "display(quality_check)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Find duplicates (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "duplicates = df_raw.groupBy(\"order_id\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .orderBy(\"order_id\")\n",
    "\n",
    "display(duplicates)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018e6d8d-fe6a-4fbf-a584-89376b0d77aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# ü•â Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "The bronze layer stores raw data exactly as it arrives - no cleaning, no transformations.\n",
    "\n",
    "**Characteristics:**\n",
    "* Append-only (keep all data)\n",
    "* Minimal processing\n",
    "* May contain duplicates and errors\n",
    "* Full audit trail\n",
    "* Uses **Auto Loader** for incremental loading\n",
    "\n",
    "**Auto Loader vs COPY INTO:**\n",
    "* Auto Loader is the PySpark streaming approach\n",
    "* Better for continuous ingestion\n",
    "* Automatic schema inference and evolution\n",
    "* Scalable file discovery\n",
    "* Checkpoint-based progress tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57153f26-95b5-4fcc-bf18-f2412417acf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Create Bronze Table ü•â\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Create a Delta table for the bronze layer using PySpark.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Table name: `main.default.orders_bronze`\n",
    "2. Columns (match the CSV structure):\n",
    "   * `order_id` INT\n",
    "   * `customer_id` INT\n",
    "   * `order_date` STRING (we'll convert to DATE in silver)\n",
    "   * `product_name` STRING\n",
    "   * `quantity` INT\n",
    "   * `unit_price` DOUBLE\n",
    "   * `status` STRING\n",
    "   * `ingestion_timestamp` TIMESTAMP (add this for tracking)\n",
    "   * `_rescued_data` STRING (Auto Loader metadata column)\n",
    "3. Use Delta format\n",
    "\n",
    "**You can create the table using SQL or let Auto Loader create it automatically.**\n",
    "\n",
    "**Bronze layer principle:** Store raw data as-is, add metadata columns for tracking.\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7382524d-3209-4eab-ab91-247f944fa03b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Create Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create the bronze table\n",
    "# You can use spark.sql() or create a DataFrame with the schema\n",
    "# Or skip this and let Auto Loader create it automatically\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da0fc67e-43b0-405a-ad4b-019db2e3d35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 3\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> CREATE TABLE with SQL (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS main.default.orders_bronze (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date STRING,\n",
    "    product_name STRING,\n",
    "    quantity INT,\n",
    "    unit_price DOUBLE,\n",
    "    status STRING,\n",
    "    ingestion_timestamp TIMESTAMP,\n",
    "    _rescued_data STRING\n",
    "  )\n",
    "  USING DELTA\n",
    "  COMMENT 'Raw order data - Bronze layer'\n",
    "\"\"\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Or let Auto Loader create it (click to expand)</summary>\n",
    "\n",
    "You can skip creating the table manually.\n",
    "Auto Loader will create it automatically when you run the stream in Task 4.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Why _rescued_data? (click to expand)</summary>\n",
    "\n",
    "Auto Loader adds `_rescued_data` column:\n",
    "* Captures data that doesn't match the schema\n",
    "* Prevents data loss from schema mismatches\n",
    "* Useful for debugging\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf072ed2-7855-4935-a746-010f4020bdad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Ingest Data with Auto Loader üì•\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Use Auto Loader to incrementally load data from CSV files into the bronze table.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use `spark.readStream.format(\"cloudFiles\")` (Auto Loader)\n",
    "2. Set options:\n",
    "   * `cloudFiles.format` = \"csv\"\n",
    "   * `cloudFiles.schemaLocation` = checkpoint path\n",
    "   * `header` = \"true\"\n",
    "3. Add `ingestion_timestamp` column with `current_timestamp()`\n",
    "4. Write as a stream to the bronze table\n",
    "5. Use `.trigger(availableNow=True)` for one-time processing\n",
    "6. Set checkpoint location\n",
    "\n",
    "**Auto Loader benefits:**\n",
    "* Automatically discovers new files\n",
    "* Tracks processed files (idempotent)\n",
    "* Handles schema evolution\n",
    "* Scalable for millions of files\n",
    "\n",
    "**Expected results:**\n",
    "* After first run: 155 rows (batch1 + batch2)\n",
    "* Rerunning loads no duplicates (idempotent)\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43fd0401-621f-4408-aa52-fe763b80c003",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Auto Loader to Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Use Auto Loader to load CSV files into bronze\n",
    "# Use cloudFiles format with trigger(availableNow=True)\n",
    "# Add ingestion_timestamp column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa255a0-3a1d-456d-a4e1-1cf4395b9167",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Verify Bronze Data"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Check the bronze table\n",
    "# Count total rows, check for duplicates and data quality issues\n",
    "# Use display() to show results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "207fd501-c00d-4400-8a14-3275929321d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 4\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Auto Loader basic structure (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/main/default/ecommerce_raw_data/_schema\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/Volumes/main/default/ecommerce_raw_data/\") \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Write stream to table (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "df_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/main/default/ecommerce_raw_data/_checkpoint\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"main.default.orders_bronze\") \\\n",
    "    .awaitTermination()\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Verify data (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, count, when, countDistinct\n",
    "\n",
    "df_bronze = spark.table(\"main.default.orders_bronze\")\n",
    "\n",
    "# Count total rows\n",
    "print(f\"Total rows: {df_bronze.count()}\")\n",
    "\n",
    "# Check for issues\n",
    "quality_metrics = df_bronze.select(\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") < 0, 1)).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "display(quality_metrics)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f93c6398-25d4-4e52-8f5a-47c5209cc015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# ü•à Silver Layer: Cleaned & Validated Data\n",
    "\n",
    "The silver layer contains cleaned, validated, and deduplicated data ready for business use.\n",
    "\n",
    "**Transformations:**\n",
    "* Remove duplicates (keep latest version)\n",
    "* Filter out invalid data (nulls, negatives)\n",
    "* Convert data types (STRING ‚Üí DATE)\n",
    "* Add calculated columns (total_amount)\n",
    "* Enforce data quality rules\n",
    "\n",
    "**Method:** Delta merge API (PySpark equivalent of MERGE INTO)\n",
    "\n",
    "**Delta merge API vs SQL MERGE:**\n",
    "* Programmatic control in Python\n",
    "* Chainable methods (.whenMatchedUpdate(), .whenNotMatchedInsert())\n",
    "* Better for complex transformations\n",
    "* Integrates with PySpark DataFrame operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b513f4d-9759-4035-8788-76371cc6a32e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 5: Create Silver Table ü•à\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Create a cleaned version of the orders table with proper data types and calculated columns.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Table name: `main.default.orders_silver`\n",
    "2. Columns:\n",
    "   * `order_id` INT (primary key)\n",
    "   * `customer_id` INT\n",
    "   * `order_date` DATE (converted from STRING)\n",
    "   * `product_name` STRING\n",
    "   * `quantity` INT\n",
    "   * `unit_price` DOUBLE\n",
    "   * `total_amount` DOUBLE (calculated: quantity * unit_price)\n",
    "   * `status` STRING\n",
    "   * `created_at` TIMESTAMP (when first inserted)\n",
    "   * `updated_at` TIMESTAMP (when last updated)\n",
    "3. Use Delta format\n",
    "\n",
    "**Key differences from bronze:**\n",
    "* order_date is DATE (not STRING)\n",
    "* Added total_amount (calculated column)\n",
    "* Added created_at and updated_at for tracking\n",
    "* No _rescued_data column\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "684cbbb5-f482-4c91-b60d-627f85885a65",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Create Silver Table"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create the silver table with proper data types\n",
    "# Use spark.sql() or DeltaTable.create()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24e98cb4-f054-488e-91e4-4a2f8379a819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 5\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Using SQL (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS main.default.orders_silver (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date DATE,\n",
    "    product_name STRING,\n",
    "    quantity INT,\n",
    "    unit_price DOUBLE,\n",
    "    total_amount DOUBLE,\n",
    "    status STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP\n",
    "  )\n",
    "  USING DELTA\n",
    "  COMMENT 'Cleaned and validated orders - Silver layer'\n",
    "\"\"\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Using DeltaTable API (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"order_date\", DateType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"unit_price\", DoubleType()),\n",
    "    StructField(\"total_amount\", DoubleType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"created_at\", TimestampType()),\n",
    "    StructField(\"updated_at\", TimestampType())\n",
    "])\n",
    "\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"main.default.orders_silver\") \\\n",
    "    .addColumns(schema) \\\n",
    "    .comment(\"Cleaned and validated orders - Silver layer\") \\\n",
    "    .execute()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a06f0f8-adbd-4166-b329-0a00af009a7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 6: Clean and Load Silver with Delta Merge API ‚ú®\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Use Delta merge API to load cleaned data from bronze to silver.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "**Data Quality Rules:**\n",
    "1. **Remove duplicates** - Keep only one row per order_id (latest by ingestion_timestamp)\n",
    "2. **Filter out invalid data:**\n",
    "   * Skip rows where product_name IS NULL\n",
    "   * Skip rows where quantity <= 0\n",
    "   * Skip rows where unit_price <= 0\n",
    "3. **Transform data:**\n",
    "   * Convert order_date from STRING to DATE\n",
    "   * Calculate total_amount = quantity * unit_price\n",
    "4. **Use Delta merge API:**\n",
    "   * `.whenMatchedUpdate()` - Update existing orders\n",
    "   * `.whenNotMatchedInsert()` - Insert new orders\n",
    "\n",
    "**Steps:**\n",
    "1. Read bronze table and apply transformations\n",
    "2. Deduplicate using window functions\n",
    "3. Use DeltaTable.forName().merge() API\n",
    "4. Set created_at on INSERT, updated_at on both\n",
    "\n",
    "**Expected results:**\n",
    "* Should have ~145 rows (155 minus ~10 invalid rows)\n",
    "* No duplicates (order_id is unique)\n",
    "* All data quality rules applied\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e91069f-d523-466c-8b29-8e67bc35640f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Delta Merge to Silver"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Clean bronze data and use Delta merge API\n",
    "# 1. Read bronze, filter invalid data, deduplicate\n",
    "# 2. Use DeltaTable.forName().merge() with whenMatchedUpdate and whenNotMatchedInsert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d83049-f4f8-4108-9b1b-d4c81399ee41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Verify Silver Data"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Verify silver table\n",
    "# Check row count, no duplicates, no invalid data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ceb71dd-302e-4e9a-a271-4826a731a700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 6\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Clean and deduplicate (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, to_date, current_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Read and clean bronze data\n",
    "df_bronze = spark.table(\"main.default.orders_bronze\")\n",
    "\n",
    "# Filter invalid data\n",
    "df_cleaned = df_bronze.filter(\n",
    "    col(\"product_name\").isNotNull() &\n",
    "    (col(\"quantity\") > 0) &\n",
    "    (col(\"unit_price\") > 0)\n",
    ")\n",
    "\n",
    "# Deduplicate (keep latest)\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "df_deduped = df_cleaned.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "# Transform\n",
    "df_transformed = df_deduped \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\"))) \\\n",
    "    .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n",
    "    .drop(\"ingestion_timestamp\", \"_rescued_data\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Delta merge API structure (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_silver = DeltaTable.forName(spark, \"main.default.orders_silver\")\n",
    "\n",
    "delta_silver.alias(\"target\").merge(\n",
    "    df_transformed.alias(\"source\"),\n",
    "    \"target.order_id = source.order_id\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"order_date\": \"source.order_date\",\n",
    "    # ... other columns\n",
    "    \"updated_at\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"order_id\": \"source.order_id\",\n",
    "    # ... all columns\n",
    "    \"created_at\": \"current_timestamp()\",\n",
    "    \"updated_at\": \"current_timestamp()\"\n",
    "}).execute()\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Complete merge (click to expand)</summary>\n",
    "\n",
    "You need to:\n",
    "1. Clean and transform bronze data\n",
    "2. Create DeltaTable reference\n",
    "3. Call merge() with join condition\n",
    "4. Chain whenMatchedUpdate() and whenNotMatchedInsert()\n",
    "5. Call execute() to run the merge\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fb4484-2f12-49e4-bd46-7a4c594f6f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# ü•á Gold Layer: Business Metrics\n",
    "\n",
    "The gold layer contains aggregated, business-ready data optimized for analytics and reporting.\n",
    "\n",
    "**Characteristics:**\n",
    "* Pre-aggregated metrics\n",
    "* Optimized for dashboards\n",
    "* Fast query performance\n",
    "* Business-friendly column names\n",
    "* Often materialized views or summary tables\n",
    "\n",
    "**Method:** PySpark DataFrame aggregations with groupBy() and agg()\n",
    "\n",
    "**Common patterns:**\n",
    "* Daily/monthly aggregations\n",
    "* Customer metrics\n",
    "* Product performance\n",
    "* KPIs and business metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e2c5331-9e50-4ad7-a54f-04d2fdf9c77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 7: Create Gold Table ü•á\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Create a gold table for daily sales metrics.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Table name: `main.default.daily_sales_gold`\n",
    "2. Columns:\n",
    "   * `order_date` DATE\n",
    "   * `total_orders` INT\n",
    "   * `total_revenue` DOUBLE\n",
    "   * `avg_order_value` DOUBLE\n",
    "   * `total_quantity_sold` INT\n",
    "   * `unique_customers` INT\n",
    "   * `completed_orders` INT\n",
    "   * `cancelled_orders` INT\n",
    "   * `updated_at` TIMESTAMP\n",
    "3. Use Delta format\n",
    "\n",
    "**This table will store one row per date with aggregated metrics.**\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ca3b303-2805-4589-bb4d-a750acdf37c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Create Gold Table"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create the gold table for daily metrics\n",
    "# Use spark.sql() or DeltaTable.create()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eda0d86-d091-40ab-acd1-da8ab9e80d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 7\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Using SQL (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS main.default.daily_sales_gold (\n",
    "    order_date DATE,\n",
    "    total_orders INT,\n",
    "    total_revenue DOUBLE,\n",
    "    avg_order_value DOUBLE,\n",
    "    total_quantity_sold INT,\n",
    "    unique_customers INT,\n",
    "    completed_orders INT,\n",
    "    cancelled_orders INT,\n",
    "    updated_at TIMESTAMP\n",
    "  )\n",
    "  USING DELTA\n",
    "  COMMENT 'Daily sales metrics - Gold layer'\n",
    "\"\"\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Why these metrics? (click to expand)</summary>\n",
    "\n",
    "These are common business metrics:\n",
    "* `total_orders` - Volume metric\n",
    "* `total_revenue` - Financial metric\n",
    "* `avg_order_value` - Performance metric\n",
    "* `unique_customers` - Customer metric\n",
    "* `completed_orders` / `cancelled_orders` - Status metrics\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcda3592-843c-4304-9353-55f95ee647b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 8: Populate Gold Table with PySpark Aggregations üìà\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Use PySpark DataFrame API to calculate daily metrics from silver and write to gold.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Read the silver table\n",
    "2. Group by order_date using `.groupBy()`\n",
    "3. Calculate metrics using `.agg()`:\n",
    "   * `total_orders` - count()\n",
    "   * `total_revenue` - sum(\"total_amount\")\n",
    "   * `avg_order_value` - avg(\"total_amount\")\n",
    "   * `total_quantity_sold` - sum(\"quantity\")\n",
    "   * `unique_customers` - countDistinct(\"customer_id\")\n",
    "   * `completed_orders` - sum(when(status='completed', 1))\n",
    "   * `cancelled_orders` - sum(when(status='cancelled', 1))\n",
    "4. Add updated_at column with current_timestamp()\n",
    "5. Write using `.write.mode(\"overwrite\").saveAsTable()`\n",
    "\n",
    "**PySpark aggregation functions:**\n",
    "* `count()`, `sum()`, `avg()`, `countDistinct()`\n",
    "* `when()` for conditional logic\n",
    "* `round()` for formatting\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "682efde2-558c-47aa-8349-b8d15891f47f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Populate Gold Table"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Read silver, group by order_date, calculate metrics\n",
    "# Use .agg() with multiple aggregation functions\n",
    "# Write with mode(\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34c5a1b-e40f-4278-bdd1-62c7abfc7e0f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Verify Gold Data"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Query the gold table to see daily metrics\n",
    "# Use display() and orderBy(\"order_date\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "717ac044-664c-4e5e-88f5-8c7963fd75da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 8\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Import functions (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum, avg, countDistinct, \n",
    "    when, round, current_timestamp\n",
    ")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Aggregation structure (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "df_silver = spark.table(\"main.default.orders_silver\")\n",
    "\n",
    "df_gold = df_silver.groupBy(\"order_date\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    round(sum(\"total_amount\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"total_amount\"), 2).alias(\"avg_order_value\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(when(col(\"status\") == \"completed\", 1).otherwise(0)).alias(\"completed_orders\"),\n",
    "    sum(when(col(\"status\") == \"cancelled\", 1).otherwise(0)).alias(\"cancelled_orders\")\n",
    ").withColumn(\"updated_at\", current_timestamp())\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Write to table (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "df_gold.write.mode(\"overwrite\").saveAsTable(\"main.default.daily_sales_gold\")\n",
    "\n",
    "print(\"‚úÖ Gold table populated\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca70d9e6-b5ff-47a8-9502-5cf2d1f1acc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ Validation: Verify Your Pipeline\n",
    "\n",
    "Let's verify that your medallion architecture pipeline works correctly!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b1c06e3-37e9-4025-8b6e-13576b9bf5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 9: Validate Data Quality Across Layers üîç\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Verify that data quality improves as it flows through the pipeline using PySpark.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "Create a DataFrame that compares all three layers:\n",
    "\n",
    "1. **Bronze metrics:**\n",
    "   * Total rows\n",
    "   * Duplicate count\n",
    "   * Null product_name count\n",
    "   * Negative quantity count\n",
    "\n",
    "2. **Silver metrics:**\n",
    "   * Total rows (should be less than bronze)\n",
    "   * Duplicate count (should be 0)\n",
    "   * Null count (should be 0)\n",
    "   * Negative count (should be 0)\n",
    "\n",
    "3. **Gold metrics:**\n",
    "   * Total rows (number of unique dates)\n",
    "   * Total revenue sum\n",
    "\n",
    "**Expected results:**\n",
    "* Bronze: ~155 rows with issues\n",
    "* Silver: ~145 rows, clean\n",
    "* Gold: ~30 rows (one per date)\n",
    "\n",
    "**Use `.union()` to combine metrics from all three layers.**\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd60c271-2327-4d25-b29e-fe67f1a160d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Validate Data Quality"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create DataFrames with metrics from each layer\n",
    "# Use union() to combine them\n",
    "# Use display() to show the comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee17b4e6-3749-486b-9840-0d0c13eb9ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 9\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Calculate metrics for each layer (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import lit, count, countDistinct, when, col\n",
    "\n",
    "# Bronze metrics\n",
    "df_bronze = spark.table(\"main.default.orders_bronze\")\n",
    "bronze_metrics = df_bronze.select(\n",
    "    lit(\"Bronze\").alias(\"layer\"),\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    (count(\"*\") - countDistinct(\"order_id\")).alias(\"duplicates\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") < 0, 1)).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "# Similar for silver and gold...\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Union all layers (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Combine all metrics\n",
    "all_metrics = bronze_metrics \\\n",
    "    .union(silver_metrics) \\\n",
    "    .union(gold_metrics)\n",
    "\n",
    "display(all_metrics)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> What to verify (click to expand)</summary>\n",
    "\n",
    "**Bronze should have:**\n",
    "* More rows than silver\n",
    "* Duplicates present\n",
    "* Null values present\n",
    "* Negative quantities present\n",
    "\n",
    "**Silver should have:**\n",
    "* Fewer rows (invalid data removed)\n",
    "* No duplicates\n",
    "* No nulls\n",
    "* No negative values\n",
    "\n",
    "**Gold should have:**\n",
    "* Much fewer rows (aggregated by date)\n",
    "* Summary metrics only\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f16776c-92a6-41cc-92b8-9c91a95b8137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 10: Test Incremental Updates üîÑ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Test that your pipeline handles new data correctly.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "**Part A: Add new raw data**\n",
    "1. Generate a 3rd batch of orders (order_id 151-200)\n",
    "2. Write to `/Volumes/main/default/ecommerce_raw_data/batch3/`\n",
    "\n",
    "**Part B: Run the pipeline**\n",
    "1. Run Auto Loader again to load batch3 into bronze\n",
    "2. Run Delta merge to update silver\n",
    "3. Recalculate gold metrics\n",
    "\n",
    "**Part C: Verify**\n",
    "1. Check row counts increased appropriately\n",
    "2. Verify no duplicates in silver\n",
    "3. Verify gold metrics updated\n",
    "\n",
    "**This tests the end-to-end incremental processing!**\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cells below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0c1b14-5a5a-405a-9a11-3fdd01811446",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Generate Batch 3"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Generate batch 3 data (50 more orders, order_id 151-200)\n",
    "# Write to batch3 directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e349f1-5fef-4df5-a93d-8f97f83d35ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Load Batch 3 to Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Run Auto Loader again to load batch3\n",
    "# Same code as Task 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f675428-73e8-47db-a4cf-b96bf096f35c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Update Silver"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Run your Delta merge code again to update silver\n",
    "# Same code as Task 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bbe88f8-a53d-429b-80cd-9f176662e366",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Update Gold"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Recalculate gold metrics\n",
    "# Same aggregation code as Task 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a8d5a69-ea39-4c3f-8f2d-14a9d5f19ccc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Verify Final Counts"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Check row counts in all three layers\n",
    "# Bronze: should have ~205 rows\n",
    "# Silver: should have ~195 rows\n",
    "# Gold: should have ~30 rows (dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8b65d2a-6268-465b-93bf-e649ed804a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 10\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Generate batch 3 (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Similar to Task 1, but different ID range\n",
    "data_batch3 = [\n",
    "    (i, \n",
    "     random.randint(1, 50),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 30))).strftime(\"%Y-%m-%d\"),\n",
    "     random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']),\n",
    "     random.randint(1, 10),\n",
    "     round(random.uniform(10, 500), 2),\n",
    "     random.choice(['completed', 'pending', 'cancelled']))\n",
    "    for i in range(151, 201)\n",
    "]\n",
    "\n",
    "df_batch3 = spark.createDataFrame(data_batch3, schema)\n",
    "df_batch3.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/main/default/ecommerce_raw_data/batch3\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Rerun the pipeline (click to expand)</summary>\n",
    "\n",
    "Just run the same code again:\n",
    "1. Auto Loader for bronze (automatically loads only batch3)\n",
    "2. Delta merge for silver (processes new bronze data)\n",
    "3. Aggregation for gold (recalculates all metrics)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Verify counts (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "bronze_count = spark.table(\"main.default.orders_bronze\").select(lit(\"Bronze\").alias(\"layer\"), count(\"*\").alias(\"row_count\"))\n",
    "silver_count = spark.table(\"main.default.orders_silver\").select(lit(\"Silver\").alias(\"layer\"), count(\"*\").alias(\"row_count\"))\n",
    "gold_count = spark.table(\"main.default.daily_sales_gold\").select(lit(\"Gold\").alias(\"layer\"), count(\"*\").alias(\"row_count\"))\n",
    "\n",
    "all_counts = bronze_count.union(silver_count).union(gold_count)\n",
    "display(all_counts)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7296a736-86cf-4e0b-b204-a50d8f597b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# üìù Complete Solutions\n",
    "\n",
    "**‚ö†Ô∏è Only look at these if you're stuck or want to verify your work!**\n",
    "\n",
    "Try to solve the challenges yourself first. Learning happens through problem-solving!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ff25bd2-012f-4331-ac18-1e81ced520b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 1 (Setup)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Create volume\n",
    "spark.sql(\"\"\"\n",
    "  CREATE VOLUME IF NOT EXISTS main.default.ecommerce_raw_data\n",
    "  COMMENT 'Raw order files for medallion architecture lab'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Volume created\")\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"order_date\", StringType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"unit_price\", DoubleType()),\n",
    "    StructField(\"status\", StringType())\n",
    "])\n",
    "\n",
    "products = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones', None]  # Include None\n",
    "statuses = ['completed', 'pending', 'cancelled']\n",
    "\n",
    "# Batch 1: 100 orders\n",
    "data_batch1 = [\n",
    "    (i, \n",
    "     random.randint(1, 50),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 30))).strftime(\"%Y-%m-%d\"),\n",
    "     random.choice(products),\n",
    "     random.randint(-2, 10),  # Include negatives\n",
    "     round(random.uniform(10, 500), 2),\n",
    "     random.choice(statuses))\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "\n",
    "df_batch1 = spark.createDataFrame(data_batch1, schema)\n",
    "df_batch1.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/main/default/ecommerce_raw_data/batch1\")\n",
    "print(\"‚úÖ Batch 1 created: 100 orders\")\n",
    "\n",
    "# Batch 2: 50 new orders + 5 duplicates\n",
    "data_batch2 = data_batch1[0:5]  # Duplicates\n",
    "data_batch2 += [\n",
    "    (i, \n",
    "     random.randint(1, 50),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 30))).strftime(\"%Y-%m-%d\"),\n",
    "     random.choice(products),\n",
    "     random.randint(-2, 10),\n",
    "     round(random.uniform(10, 500), 2),\n",
    "     random.choice(statuses))\n",
    "    for i in range(101, 151)\n",
    "]\n",
    "\n",
    "df_batch2 = spark.createDataFrame(data_batch2, schema)\n",
    "df_batch2.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/main/default/ecommerce_raw_data/batch2\")\n",
    "print(\"‚úÖ Batch 2 created: 50 orders + 5 duplicates\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete! Raw data files ready.\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbd584f-4c0c-430c-aff9-3c607f0f3b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 2 (Explore Raw Data)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "**View raw data:**\n",
    "```python\n",
    "df_raw = spark.read.csv(\n",
    "    \"/Volumes/main/default/ecommerce_raw_data/\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(\"Sample raw data:\")\n",
    "display(df_raw.limit(20))\n",
    "```\n",
    "\n",
    "**Check data quality issues:**\n",
    "```python\n",
    "from pyspark.sql.functions import col, count, countDistinct, when\n",
    "\n",
    "quality_check = df_raw.select(\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "    (count(\"*\") - countDistinct(\"order_id\")).alias(\"duplicates\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") < 0, 1)).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "print(\"Data quality issues:\")\n",
    "display(quality_check)\n",
    "```\n",
    "\n",
    "**Find duplicate order_ids:**\n",
    "```python\n",
    "duplicates = df_raw.groupBy(\"order_id\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .orderBy(\"order_id\")\n",
    "\n",
    "print(\"Duplicate order_ids:\")\n",
    "display(duplicates)\n",
    "```\n",
    "\n",
    "**Expected findings:**\n",
    "* Total: 155 rows\n",
    "* Duplicates: 5 (order_id 1-5)\n",
    "* Nulls: ~10-15 rows\n",
    "* Negatives: ~10-15 rows\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b90ed3-aee6-4cbf-9c14-9633fc355b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 3 (Create Bronze Table)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "**Option 1: Using SQL**\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS main.default.orders_bronze (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date STRING,\n",
    "    product_name STRING,\n",
    "    quantity INT,\n",
    "    unit_price DOUBLE,\n",
    "    status STRING,\n",
    "    ingestion_timestamp TIMESTAMP,\n",
    "    _rescued_data STRING\n",
    "  )\n",
    "  USING DELTA\n",
    "  COMMENT 'Raw order data - Bronze layer'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Bronze table created\")\n",
    "```\n",
    "\n",
    "**Option 2: Let Auto Loader create it**\n",
    "```python\n",
    "# Skip table creation - Auto Loader will create it automatically\n",
    "print(\"‚úÖ Will let Auto Loader create the table\")\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "* `order_date` is STRING (raw format from CSV)\n",
    "* Added `ingestion_timestamp` for tracking\n",
    "* Added `_rescued_data` for Auto Loader\n",
    "* Using DELTA format for ACID properties\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973353ae-9085-4bdc-9857-6b993d4871c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 4 (Ingest with Auto Loader)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "**Load data with Auto Loader:**\n",
    "```python\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Read stream with Auto Loader\n",
    "df_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/main/default/ecommerce_raw_data/_schema\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/Volumes/main/default/ecommerce_raw_data/\") \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "# Write stream to bronze table\n",
    "query = df_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/main/default/ecommerce_raw_data/_checkpoint\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"main.default.orders_bronze\")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"‚úÖ Data loaded to bronze\")\n",
    "```\n",
    "\n",
    "**Verify:**\n",
    "```python\n",
    "from pyspark.sql.functions import col, count, countDistinct, when\n",
    "\n",
    "df_bronze = spark.table(\"main.default.orders_bronze\")\n",
    "\n",
    "print(f\"Total rows: {df_bronze.count()}\")\n",
    "\n",
    "# Check data quality\n",
    "quality_metrics = df_bronze.select(\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") < 0, 1)).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "display(quality_metrics)\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "* Auto Loader automatically tracks processed files\n",
    "* `trigger(availableNow=True)` processes all available files once\n",
    "* Checkpoint ensures idempotency\n",
    "* `_rescued_data` captures schema mismatches\n",
    "* Bronze contains ALL data (including bad data)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3927c866-5812-4979-ac50-052ba978ab8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 5 (Create Silver Table)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS main.default.orders_silver (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date DATE,\n",
    "    product_name STRING,\n",
    "    quantity INT,\n",
    "    unit_price DOUBLE,\n",
    "    total_amount DOUBLE,\n",
    "    status STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP\n",
    "  )\n",
    "  USING DELTA\n",
    "  COMMENT 'Cleaned and validated orders - Silver layer'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Silver table created\")\n",
    "```\n",
    "\n",
    "**Key differences from bronze:**\n",
    "* `order_date` is DATE (not STRING)\n",
    "* Added `total_amount` (calculated column)\n",
    "* Added `created_at` and `updated_at` for tracking\n",
    "* No ingestion_timestamp or _rescued_data\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3a0f77-dfb5-4f40-a5f8-cd08612c6fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 6 (Clean and Load Silver)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, to_date, current_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Read and clean bronze data\n",
    "df_bronze = spark.table(\"main.default.orders_bronze\")\n",
    "\n",
    "# Step 2: Filter invalid data\n",
    "df_cleaned = df_bronze.filter(\n",
    "    col(\"product_name\").isNotNull() &\n",
    "    (col(\"quantity\") > 0) &\n",
    "    (col(\"unit_price\") > 0)\n",
    ")\n",
    "\n",
    "# Step 3: Deduplicate (keep latest by ingestion_timestamp)\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "df_deduped = df_cleaned.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "# Step 4: Transform data\n",
    "df_transformed = df_deduped \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\"))) \\\n",
    "    .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n",
    "    .drop(\"ingestion_timestamp\", \"_rescued_data\") \\\n",
    "    .select(\n",
    "        \"order_id\", \"customer_id\", \"order_date\", \"product_name\",\n",
    "        \"quantity\", \"unit_price\", \"total_amount\", \"status\"\n",
    "    )\n",
    "\n",
    "# Step 5: Merge into silver using Delta merge API\n",
    "delta_silver = DeltaTable.forName(spark, \"main.default.orders_silver\")\n",
    "\n",
    "delta_silver.alias(\"target\").merge(\n",
    "    df_transformed.alias(\"source\"),\n",
    "    \"target.order_id = source.order_id\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"order_date\": \"source.order_date\",\n",
    "    \"product_name\": \"source.product_name\",\n",
    "    \"quantity\": \"source.quantity\",\n",
    "    \"unit_price\": \"source.unit_price\",\n",
    "    \"total_amount\": \"source.total_amount\",\n",
    "    \"status\": \"source.status\",\n",
    "    \"updated_at\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"order_id\": \"source.order_id\",\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"order_date\": \"source.order_date\",\n",
    "    \"product_name\": \"source.product_name\",\n",
    "    \"quantity\": \"source.quantity\",\n",
    "    \"unit_price\": \"source.unit_price\",\n",
    "    \"total_amount\": \"source.total_amount\",\n",
    "    \"status\": \"source.status\",\n",
    "    \"created_at\": \"current_timestamp()\",\n",
    "    \"updated_at\": \"current_timestamp()\"\n",
    "}).execute()\n",
    "\n",
    "print(\"‚úÖ Silver table updated\")\n",
    "```\n",
    "\n",
    "**Verify:**\n",
    "```python\n",
    "df_silver = spark.table(\"main.default.orders_silver\")\n",
    "\n",
    "quality_check = df_silver.select(\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") <= 0, 1)).alias(\"invalid_quantities\")\n",
    ")\n",
    "\n",
    "display(quality_check)\n",
    "```\n",
    "\n",
    "**What this does:**\n",
    "1. Filters out invalid data (nulls, negatives)\n",
    "2. Window function deduplicates (keeps latest)\n",
    "3. Converts order_date to DATE type\n",
    "4. Calculates total_amount\n",
    "5. Delta merge API upserts into silver\n",
    "6. Sets created_at on INSERT, updated_at on both\n",
    "\n",
    "**Expected results:**\n",
    "* ~145 rows (155 minus ~10 invalid)\n",
    "* 0 duplicates\n",
    "* 0 nulls\n",
    "* 0 negative quantities\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9bf8385-f223-40ef-8c89-33c46b34cdaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 7 (Create Gold Table)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS main.default.daily_sales_gold (\n",
    "    order_date DATE,\n",
    "    total_orders INT,\n",
    "    total_revenue DOUBLE,\n",
    "    avg_order_value DOUBLE,\n",
    "    total_quantity_sold INT,\n",
    "    unique_customers INT,\n",
    "    completed_orders INT,\n",
    "    cancelled_orders INT,\n",
    "    updated_at TIMESTAMP\n",
    "  )\n",
    "  USING DELTA\n",
    "  COMMENT 'Daily sales metrics - Gold layer'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Gold table created\")\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "* One row per order_date\n",
    "* All metrics are aggregated\n",
    "* Business-friendly column names\n",
    "* Ready for dashboards and reports\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c5878b3-31b3-4bd7-967c-5e63099a509e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 8 (Populate Gold Table)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum, avg, countDistinct, \n",
    "    when, round, current_timestamp\n",
    ")\n",
    "\n",
    "# Read silver table\n",
    "df_silver = spark.table(\"main.default.orders_silver\")\n",
    "\n",
    "# Calculate daily metrics\n",
    "df_gold = df_silver.groupBy(\"order_date\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    round(sum(\"total_amount\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"total_amount\"), 2).alias(\"avg_order_value\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(when(col(\"status\") == \"completed\", 1).otherwise(0)).alias(\"completed_orders\"),\n",
    "    sum(when(col(\"status\") == \"cancelled\", 1).otherwise(0)).alias(\"cancelled_orders\")\n",
    ").withColumn(\"updated_at\", current_timestamp()) \\\n",
    " .orderBy(\"order_date\")\n",
    "\n",
    "# Write to gold table (overwrite mode for full refresh)\n",
    "df_gold.write.mode(\"overwrite\").saveAsTable(\"main.default.daily_sales_gold\")\n",
    "\n",
    "print(\"‚úÖ Gold table populated\")\n",
    "```\n",
    "\n",
    "**Verify:**\n",
    "```python\n",
    "df_gold_check = spark.table(\"main.default.daily_sales_gold\")\n",
    "print(f\"Total dates: {df_gold_check.count()}\")\n",
    "display(df_gold_check.orderBy(\"order_date\"))\n",
    "```\n",
    "\n",
    "**What this does:**\n",
    "1. Groups by order_date\n",
    "2. Uses .agg() with multiple aggregation functions\n",
    "3. Uses when() for conditional counts\n",
    "4. round() for clean numbers\n",
    "5. mode(\"overwrite\") replaces all data (full refresh)\n",
    "\n",
    "**Expected results:**\n",
    "* ~30 rows (one per unique date)\n",
    "* Each row has aggregated metrics for that day\n",
    "* Ready for dashboard visualization\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528b67a5-4745-4ea1-8c6d-e3580e72be31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 9 (Data Quality Validation)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import lit, count, countDistinct, when, col\n",
    "\n",
    "# Bronze metrics\n",
    "df_bronze = spark.table(\"main.default.orders_bronze\")\n",
    "bronze_metrics = df_bronze.select(\n",
    "    lit(\"Bronze\").alias(\"layer\"),\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "    (count(\"*\") - countDistinct(\"order_id\")).alias(\"duplicates\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") < 0, 1)).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "# Silver metrics\n",
    "df_silver = spark.table(\"main.default.orders_silver\")\n",
    "silver_metrics = df_silver.select(\n",
    "    lit(\"Silver\").alias(\"layer\"),\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "    (count(\"*\") - countDistinct(\"order_id\")).alias(\"duplicates\"),\n",
    "    count(when(col(\"product_name\").isNull(), 1)).alias(\"null_products\"),\n",
    "    count(when(col(\"quantity\") < 0, 1)).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "# Gold metrics\n",
    "df_gold = spark.table(\"main.default.daily_sales_gold\")\n",
    "gold_metrics = df_gold.select(\n",
    "    lit(\"Gold\").alias(\"layer\"),\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    lit(0).alias(\"unique_orders\"),\n",
    "    lit(0).alias(\"duplicates\"),\n",
    "    lit(0).alias(\"null_products\"),\n",
    "    lit(0).alias(\"negative_quantities\")\n",
    ")\n",
    "\n",
    "# Combine all metrics\n",
    "all_metrics = bronze_metrics.union(silver_metrics).union(gold_metrics)\n",
    "\n",
    "print(\"Data quality comparison across layers:\")\n",
    "display(all_metrics)\n",
    "```\n",
    "\n",
    "**Expected results:**\n",
    "\n",
    "| layer | total_rows | unique_orders | duplicates | null_products | negative_quantities |\n",
    "|-------|------------|---------------|------------|---------------|--------------------|\n",
    "| Bronze | 155 | 150 | 5 | ~10 | ~10 |\n",
    "| Silver | ~145 | ~145 | 0 | 0 | 0 |\n",
    "| Gold | ~30 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "**Key insights:**\n",
    "* Bronze has all issues (duplicates, nulls, negatives)\n",
    "* Silver is clean (all issues removed)\n",
    "* Gold is aggregated (much fewer rows)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f3ba32-a9d6-4a95-9b43-81d396f746bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 10 (Test Incremental Updates)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "**Generate Batch 3:**\n",
    "```python\n",
    "# Generate 50 more orders\n",
    "data_batch3 = [\n",
    "    (i, \n",
    "     random.randint(1, 50),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 30))).strftime(\"%Y-%m-%d\"),\n",
    "     random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']),\n",
    "     random.randint(1, 10),\n",
    "     round(random.uniform(10, 500), 2),\n",
    "     random.choice(['completed', 'pending', 'cancelled']))\n",
    "    for i in range(151, 201)\n",
    "]\n",
    "\n",
    "df_batch3 = spark.createDataFrame(data_batch3, schema)\n",
    "df_batch3.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/main/default/ecommerce_raw_data/batch3\")\n",
    "print(\"‚úÖ Batch 3 created: 50 orders\")\n",
    "```\n",
    "\n",
    "**Load to Bronze (rerun Auto Loader):**\n",
    "```python\n",
    "# Same Auto Loader code from Task 4\n",
    "df_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/main/default/ecommerce_raw_data/_schema\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/Volumes/main/default/ecommerce_raw_data/\") \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "query = df_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/main/default/ecommerce_raw_data/_checkpoint\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"main.default.orders_bronze\")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(\"‚úÖ Batch 3 loaded to bronze\")\n",
    "```\n",
    "\n",
    "**Update Silver (rerun Delta merge):**\n",
    "```python\n",
    "# Same cleaning and merge code from Task 6\n",
    "df_bronze = spark.table(\"main.default.orders_bronze\")\n",
    "\n",
    "df_cleaned = df_bronze.filter(\n",
    "    col(\"product_name\").isNotNull() &\n",
    "    (col(\"quantity\") > 0) &\n",
    "    (col(\"unit_price\") > 0)\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "df_deduped = df_cleaned.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "df_transformed = df_deduped \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\"))) \\\n",
    "    .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n",
    "    .drop(\"ingestion_timestamp\", \"_rescued_data\") \\\n",
    "    .select(\n",
    "        \"order_id\", \"customer_id\", \"order_date\", \"product_name\",\n",
    "        \"quantity\", \"unit_price\", \"total_amount\", \"status\"\n",
    "    )\n",
    "\n",
    "delta_silver = DeltaTable.forName(spark, \"main.default.orders_silver\")\n",
    "\n",
    "delta_silver.alias(\"target\").merge(\n",
    "    df_transformed.alias(\"source\"),\n",
    "    \"target.order_id = source.order_id\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"order_date\": \"source.order_date\",\n",
    "    \"product_name\": \"source.product_name\",\n",
    "    \"quantity\": \"source.quantity\",\n",
    "    \"unit_price\": \"source.unit_price\",\n",
    "    \"total_amount\": \"source.total_amount\",\n",
    "    \"status\": \"source.status\",\n",
    "    \"updated_at\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"order_id\": \"source.order_id\",\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"order_date\": \"source.order_date\",\n",
    "    \"product_name\": \"source.product_name\",\n",
    "    \"quantity\": \"source.quantity\",\n",
    "    \"unit_price\": \"source.unit_price\",\n",
    "    \"total_amount\": \"source.total_amount\",\n",
    "    \"status\": \"source.status\",\n",
    "    \"created_at\": \"current_timestamp()\",\n",
    "    \"updated_at\": \"current_timestamp()\"\n",
    "}).execute()\n",
    "\n",
    "print(\"‚úÖ Silver updated\")\n",
    "```\n",
    "\n",
    "**Update Gold (rerun aggregation):**\n",
    "```python\n",
    "# Same aggregation code from Task 8\n",
    "df_silver = spark.table(\"main.default.orders_silver\")\n",
    "\n",
    "df_gold = df_silver.groupBy(\"order_date\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    round(sum(\"total_amount\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"total_amount\"), 2).alias(\"avg_order_value\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(when(col(\"status\") == \"completed\", 1).otherwise(0)).alias(\"completed_orders\"),\n",
    "    sum(when(col(\"status\") == \"cancelled\", 1).otherwise(0)).alias(\"cancelled_orders\")\n",
    ").withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "df_gold.write.mode(\"overwrite\").saveAsTable(\"main.default.daily_sales_gold\")\n",
    "print(\"‚úÖ Gold updated\")\n",
    "```\n",
    "\n",
    "**Verify:**\n",
    "```python\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "bronze_count = spark.table(\"main.default.orders_bronze\").select(lit(\"Bronze\").alias(\"layer\"), count(\"*\").alias(\"row_count\"))\n",
    "silver_count = spark.table(\"main.default.orders_silver\").select(lit(\"Silver\").alias(\"layer\"), count(\"*\").alias(\"row_count\"))\n",
    "gold_count = spark.table(\"main.default.daily_sales_gold\").select(lit(\"Gold\").alias(\"layer\"), count(\"*\").alias(\"row_count\"))\n",
    "\n",
    "all_counts = bronze_count.union(silver_count).union(gold_count)\n",
    "display(all_counts)\n",
    "```\n",
    "\n",
    "**Expected results:**\n",
    "* Bronze: ~205 rows (155 + 50)\n",
    "* Silver: ~195 rows (cleaned)\n",
    "* Gold: ~30 rows (dates)\n",
    "\n",
    "**Key insight:** The pipeline is reusable - just rerun the same code for new data!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37649b69-b196-41d6-8e36-956e6558561e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ Common Pipeline Patterns\n",
    "\n",
    "### **Pattern 1: Batch Pipeline (This Lab)**\n",
    "\n",
    "```\n",
    "Raw Files ‚Üí Bronze (Auto Loader) ‚Üí Silver (Delta merge) ‚Üí Gold (PySpark agg)\n",
    "```\n",
    "\n",
    "**Schedule:**\n",
    "* Bronze: Every hour (ingest new files)\n",
    "* Silver: Every hour (clean new data)\n",
    "* Gold: Daily (aggregate for reports)\n",
    "\n",
    "**Code pattern:**\n",
    "```python\n",
    "# Bronze\n",
    "spark.readStream.format(\"cloudFiles\").load(...).writeStream.toTable(...)\n",
    "\n",
    "# Silver\n",
    "DeltaTable.forName(...).merge(...).whenMatchedUpdate(...).execute()\n",
    "\n",
    "# Gold\n",
    "df.groupBy(...).agg(...).write.mode(\"overwrite\").saveAsTable(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern 2: Streaming Pipeline**\n",
    "\n",
    "```\n",
    "Raw Files ‚Üí Bronze (Auto Loader) ‚Üí Silver (Stream) ‚Üí Gold (Stream)\n",
    "```\n",
    "\n",
    "**Use when:**\n",
    "* Need real-time data\n",
    "* Continuous file arrival\n",
    "* Low latency requirements\n",
    "\n",
    "**Code pattern:**\n",
    "```python\n",
    "# All layers use streaming\n",
    "spark.readStream...writeStream.trigger(processingTime=\"1 minute\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern 3: CDC Pipeline**\n",
    "\n",
    "```\n",
    "Change Data ‚Üí Bronze (Auto Loader) ‚Üí Silver (MERGE with SCD) ‚Üí Gold (Agg)\n",
    "```\n",
    "\n",
    "**Use when:**\n",
    "* Capturing database changes\n",
    "* Need historical tracking\n",
    "* SCD Type 2 requirements\n",
    "\n",
    "---\n",
    "\n",
    "### **This Lab's Pipeline**\n",
    "\n",
    "```\n",
    "CSV Files in Volume\n",
    "    ‚Üì\n",
    "    Auto Loader (streaming)\n",
    "    ‚Üì\n",
    "ü•â Bronze: orders_bronze (155 rows, raw data)\n",
    "    ‚Üì\n",
    "    Delta merge API (clean, dedupe)\n",
    "    ‚Üì\n",
    "ü•à Silver: orders_silver (~145 rows, validated)\n",
    "    ‚Üì\n",
    "    PySpark aggregations\n",
    "    ‚Üì\n",
    "ü•á Gold: daily_sales_gold (~30 rows, metrics)\n",
    "    ‚Üì\n",
    "    Dashboards & Reports\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad86072-9968-4566-bf15-f409c483c1da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üí° Key Concepts Summary\n",
    "\n",
    "### **Medallion Architecture**\n",
    "\n",
    "**Bronze ‚Üí Silver ‚Üí Gold** = **Raw ‚Üí Cleaned ‚Üí Aggregated**\n",
    "\n",
    "**Why use it?**\n",
    "* Clear separation of concerns\n",
    "* Incremental processing\n",
    "* Data quality improvement\n",
    "* Reprocessing capability\n",
    "* Audit trail\n",
    "\n",
    "---\n",
    "\n",
    "### **Auto Loader (Bronze)**\n",
    "\n",
    "**Purpose:** Incremental file ingestion\n",
    "\n",
    "**Key features:**\n",
    "* Automatic file discovery\n",
    "* Schema inference and evolution\n",
    "* Checkpoint-based tracking\n",
    "* Scalable (millions of files)\n",
    "* Streaming API\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .load(path) \\\n",
    "    .writeStream.toTable(table)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "* Production file ingestion\n",
    "* Continuous data arrival\n",
    "* Need schema evolution\n",
    "* Bronze layer ingestion\n",
    "\n",
    "---\n",
    "\n",
    "### **Delta Merge API (Silver)**\n",
    "\n",
    "**Purpose:** Programmatic upsert and deduplication\n",
    "\n",
    "**Key features:**\n",
    "* Python-based control\n",
    "* Chainable methods\n",
    "* Integrates with DataFrame ops\n",
    "* Atomic operation\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "DeltaTable.forName(spark, table).alias(\"t\").merge(\n",
    "    source.alias(\"s\"),\n",
    "    \"t.id = s.id\"\n",
    ").whenMatchedUpdate(set={...}) \\\n",
    " .whenNotMatchedInsert(values={...}) \\\n",
    " .execute()\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "* Complex transformations before merge\n",
    "* Programmatic control needed\n",
    "* Silver layer updates\n",
    "* SCD patterns\n",
    "\n",
    "---\n",
    "\n",
    "### **PySpark Aggregations (Gold)**\n",
    "\n",
    "**Purpose:** Business metrics\n",
    "\n",
    "**Key features:**\n",
    "* DataFrame API\n",
    "* Multiple aggregation functions\n",
    "* Conditional logic with when()\n",
    "* Chainable operations\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "df.groupBy(\"date\").agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    sum(\"amount\").alias(\"revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_value\")\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "* Creating KPIs\n",
    "* Dashboard data\n",
    "* Report tables\n",
    "* Gold layer metrics\n",
    "\n",
    "---\n",
    "\n",
    "### **Data Quality**\n",
    "\n",
    "**Bronze:** Accept all data (good and bad)  \n",
    "**Silver:** Filter and validate with DataFrame operations  \n",
    "**Gold:** Aggregate clean data  \n",
    "\n",
    "**Quality improves at each layer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef589b3e-0cf7-419b-b5e6-c642ca86884d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2750c5c9-6d2b-456d-ad40-4401443972d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öñÔ∏è PySpark vs SQL: When to Use Each\n",
    "\n",
    "### **Bronze Layer Ingestion**\n",
    "\n",
    "| Feature | SQL (COPY INTO) | PySpark (Auto Loader) |\n",
    "|---------|-----------------|----------------------|\n",
    "| **Syntax** | Simple SQL | Streaming API |\n",
    "| **File discovery** | Manual path | Automatic |\n",
    "| **Schema evolution** | Manual | Automatic |\n",
    "| **Scalability** | Good | Excellent |\n",
    "| **Checkpoint** | Built-in | Manual setup |\n",
    "| **Best for** | Simple batch | Production pipelines |\n",
    "\n",
    "**Recommendation:** Use **Auto Loader** for production - better scalability and features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Silver Layer Transformation**\n",
    "\n",
    "| Feature | SQL (MERGE INTO) | PySpark (Delta merge API) |\n",
    "|---------|------------------|---------------------------|\n",
    "| **Syntax** | Declarative SQL | Programmatic Python |\n",
    "| **Complexity** | Simple | More verbose |\n",
    "| **Flexibility** | Limited | High |\n",
    "| **Integration** | Standalone | With DataFrame ops |\n",
    "| **Best for** | Simple upserts | Complex transformations |\n",
    "\n",
    "**Recommendation:** \n",
    "* Use **SQL MERGE** for simple upserts\n",
    "* Use **Delta merge API** when you need complex DataFrame transformations before merge\n",
    "\n",
    "---\n",
    "\n",
    "### **Gold Layer Aggregation**\n",
    "\n",
    "| Feature | SQL | PySpark |\n",
    "|---------|-----|----------|\n",
    "| **Syntax** | Familiar SQL | DataFrame API |\n",
    "| **Performance** | Same | Same |\n",
    "| **Flexibility** | Good | Excellent |\n",
    "| **ML integration** | No | Yes |\n",
    "| **Best for** | Analysts | Engineers |\n",
    "\n",
    "**Recommendation:** Use what your team prefers - both are equally performant.\n",
    "\n",
    "---\n",
    "\n",
    "### **This Lab's Approach**\n",
    "\n",
    "**Why PySpark?**\n",
    "* ‚úÖ Auto Loader is more powerful than COPY INTO\n",
    "* ‚úÖ Better for production pipelines\n",
    "* ‚úÖ Programmatic control\n",
    "* ‚úÖ Integrates with ML workflows\n",
    "* ‚úÖ More flexible transformations\n",
    "\n",
    "**When to use SQL instead:**\n",
    "* Simpler syntax for analysts\n",
    "* Standalone queries\n",
    "* Ad-hoc analysis\n",
    "* Dashboard queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42248055-7ede-4274-83a7-67b837ec1a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Medallion Architecture Best Practices (PySpark)\n",
    "\n",
    "### **ü•â Bronze Layer**\n",
    "\n",
    "‚úÖ **Keep raw data as-is** - No transformations  \n",
    "‚úÖ **Append-only** - Never delete from bronze  \n",
    "‚úÖ **Add metadata columns** - ingestion_timestamp, _rescued_data  \n",
    "‚úÖ **Use Auto Loader** - Incremental, scalable file discovery  \n",
    "‚úÖ **Set checkpoint location** - Ensures idempotency  \n",
    "‚úÖ **Use trigger(availableNow=True)** - For batch processing  \n",
    "\n",
    "**Purpose:** Audit trail, reprocessing capability, data lineage\n",
    "\n",
    "---\n",
    "\n",
    "### **ü•à Silver Layer**\n",
    "\n",
    "‚úÖ **Apply data quality rules** - Filter nulls, validate ranges  \n",
    "‚úÖ **Deduplicate** - Use window functions with row_number()  \n",
    "‚úÖ **Convert data types** - Use to_date(), cast()  \n",
    "‚úÖ **Add calculated columns** - Use withColumn()  \n",
    "‚úÖ **Use Delta merge API** - Programmatic upserts  \n",
    "‚úÖ **Track lineage** - created_at, updated_at  \n",
    "\n",
    "**Purpose:** Clean, validated data for analytics\n",
    "\n",
    "---\n",
    "\n",
    "### **ü•á Gold Layer**\n",
    "\n",
    "‚úÖ **Pre-aggregate** - Use groupBy().agg()  \n",
    "‚úÖ **Business-friendly** - Clear column names with .alias()  \n",
    "‚úÖ **Optimize for queries** - Denormalize if needed  \n",
    "‚úÖ **Use mode(\"overwrite\")** - For full refresh  \n",
    "‚úÖ **Use Delta merge** - For incremental updates  \n",
    "‚úÖ **Document metrics** - What each column means  \n",
    "\n",
    "**Purpose:** Fast dashboards, reports, analytics\n",
    "\n",
    "---\n",
    "\n",
    "### **General Best Practices**\n",
    "\n",
    "‚úÖ **Separate concerns** - Each layer has clear purpose  \n",
    "‚úÖ **Idempotent pipelines** - Safe to rerun  \n",
    "‚úÖ **Incremental processing** - Process only new data  \n",
    "‚úÖ **Monitor data quality** - Track metrics at each layer  \n",
    "‚úÖ **Use Delta Lake** - ACID, time travel, performance  \n",
    "‚úÖ **Schedule appropriately** - Bronze (frequent), Silver (hourly), Gold (daily)  \n",
    "‚úÖ **Test with small data** - Validate logic before production  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a286ceac-fb23-4a86-9b18-77488f73b68a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Medallion Architecture: PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
