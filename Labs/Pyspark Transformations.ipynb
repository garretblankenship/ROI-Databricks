{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c45fdbe-877b-4301-b3b1-4d10408c08b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PySpark Transformations Lab\n",
    "\n",
    "## üéØ Lab Scenario\n",
    "\n",
    "You're a data engineer at a taxi analytics company. Your manager has asked you to analyze NYC taxi trip data and create a summary table that will be used by the business intelligence team for reporting.\n",
    "\n",
    "**Your Task:** Transform raw taxi trip data into a business-ready summary table.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "\n",
    "By completing this lab, you will:\n",
    "\n",
    "* Read data from Unity Catalog tables\n",
    "* Apply PySpark transformations: `select`, `filter`, `withColumn`, `groupBy`, `orderBy`\n",
    "* Create calculated columns for business metrics\n",
    "* Write transformed data to a new table in Unity Catalog\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Business Requirements\n",
    "\n",
    "Create a summary table that shows:\n",
    "* Trip statistics by hour of day\n",
    "* Only include trips with valid data (positive fares and distances)\n",
    "* Calculate average fare, total revenue, and trip counts\n",
    "* Add a revenue category column (Low/Medium/High)\n",
    "* Sort results for easy analysis\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Lab Format\n",
    "\n",
    "This is a **challenge lab** - you'll need to figure out the solutions yourself!\n",
    "\n",
    "* Each section has **requirements** but not step-by-step instructions\n",
    "* **Hints** are available if you get stuck\n",
    "* **Solutions** are provided at the end for verification\n",
    "\n",
    "**Ready? Let's begin!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "883f7831-3e6d-48e9-9514-0051c6dd3d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Load the Source Data üìö\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Load the NYC taxi trips data from the Unity Catalog samples.\n",
    "\n",
    "**Requirements:**\n",
    "* Use the table: `samples.nyctaxi.trips`\n",
    "* Store it in a variable called `taxi_df`\n",
    "* Display the first few rows to verify\n",
    "\n",
    "**Questions to answer:**\n",
    "1. How many columns does the dataset have?\n",
    "2. What are the data types?\n",
    "3. What columns will you need for the analysis?\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517a3480-12e1-4a23-b13d-a1385778ac9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Load Data"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Load the taxi trips data from samples.nyctaxi.trips\n",
    "# Store it in a variable called taxi_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e17ddef7-ae83-49ad-bd11-020093eaddae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 1\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> How to read from Unity Catalog (click to expand)</summary>\n",
    "\n",
    "Use `spark.table()` to read from Unity Catalog tables:\n",
    "```python\n",
    "df = spark.table(\"catalog.schema.table_name\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Exploring the data (click to expand)</summary>\n",
    "\n",
    "Useful methods:\n",
    "* `df.printSchema()` - See column names and types\n",
    "* `display(df)` - View the data interactively\n",
    "* `df.count()` - Count total rows\n",
    "* `df.columns` - List all column names\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087cd298-062d-410b-85fd-0ff9b2aecaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Explore the Data Schema üîç\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Examine the structure of the data to understand what you're working with.\n",
    "\n",
    "**Requirements:**\n",
    "* Print the schema to see all columns and data types\n",
    "* Identify which columns you'll need:\n",
    "  * Pickup datetime (for extracting hour)\n",
    "  * Fare amount (for revenue calculations)\n",
    "  * Trip distance (for filtering valid trips)\n",
    "  * Any other relevant fields\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0cf33d8-3b23-43cd-82b2-eef9d470fc0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Explore Schema"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Print the schema of taxi_df\n",
    "# TODO: Examine the columns and identify the ones you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "084280a4-8236-48e3-9644-4541eeaf82b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Select Relevant Columns üéØ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Select only the columns you need for the analysis to improve performance.\n",
    "\n",
    "**Requirements:**\n",
    "* Use the `select()` transformation\n",
    "* Include these columns:\n",
    "  * `tpep_pickup_datetime` - for extracting hour\n",
    "  * `fare_amount` - for revenue calculations\n",
    "  * `trip_distance` - for filtering\n",
    "  * `passenger_count` - for additional analysis (optional)\n",
    "* Store the result in a new variable called `selected_df`\n",
    "* Verify you have the correct columns\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431586d3-5733-496f-b35a-c7e6bd6763fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Select"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Select only the columns you need\n",
    "# Store in selected_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b542d75c-e74a-4768-b4d0-b55c32d4efe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 3\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Using select() (click to expand)</summary>\n",
    "\n",
    "The `select()` method takes column names as arguments:\n",
    "```python\n",
    "df.select(\"column1\", \"column2\", \"column3\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Verifying your selection (click to expand)</summary>\n",
    "\n",
    "Check your work:\n",
    "```python\n",
    "print(selected_df.columns)  # See column names\n",
    "display(selected_df)        # View the data\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e043c21b-0314-42d2-a73a-43523fe18b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Filter for Valid Trips üìä\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Filter out invalid or suspicious trip records.\n",
    "\n",
    "**Requirements:**\n",
    "* Use the `filter()` transformation\n",
    "* Keep only trips where:\n",
    "  * `fare_amount` > 0 (positive fares)\n",
    "  * `trip_distance` > 0 (positive distances)\n",
    "  * `fare_amount` < 500 (remove outliers)\n",
    "* Store the result in a variable called `filtered_df`\n",
    "* Check how many rows remain after filtering\n",
    "\n",
    "**Bonus Challenge:** Can you write the filter in a single statement?\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26c6bf5-cbc4-48fa-91c0-dd68806fd1ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Filter"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Filter for valid trips\n",
    "# Store in filtered_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3875ed0b-1dd9-4617-a5d8-4c69be3bcf63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 4\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Using filter() (click to expand)</summary>\n",
    "\n",
    "You can use column references and comparison operators:\n",
    "```python\n",
    "df.filter(df.column_name > value)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Multiple conditions (click to expand)</summary>\n",
    "\n",
    "Combine conditions with `&` (and) or `|` (or):\n",
    "```python\n",
    "df.filter((df.col1 > 0) & (df.col2 < 100))\n",
    "```\n",
    "Note: Wrap each condition in parentheses!\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Alternative syntax (click to expand)</summary>\n",
    "\n",
    "You can also use SQL-style strings:\n",
    "```python\n",
    "df.filter(\"column1 > 0 AND column2 < 100\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d7d4845-f22b-49a0-a01f-45d03a45f0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 5: Add Calculated Columns üßÆ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Create new columns with calculated values.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "Use `withColumn()` to add these new columns:\n",
    "\n",
    "1. **`pickup_hour`** - Extract the hour from `tpep_pickup_datetime`\n",
    "   * Hint: Use the `hour()` function from `pyspark.sql.functions`\n",
    "\n",
    "2. **`revenue_category`** - Categorize fares as:\n",
    "   * \"Low\" if fare_amount < 10\n",
    "   * \"Medium\" if fare_amount between 10 and 30\n",
    "   * \"High\" if fare_amount > 30\n",
    "   * Hint: Use the `when()` function for conditional logic\n",
    "\n",
    "* Store the result in a variable called `enriched_df`\n",
    "* Verify the new columns were added correctly\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89b41c33-7545-44b6-b974-780a87e64519",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: WithColumn"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Import necessary functions\n",
    "# from pyspark.sql.functions import ...\n",
    "\n",
    "# TODO: Add pickup_hour column\n",
    "# TODO: Add revenue_category column\n",
    "# Store in enriched_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "639d6f31-4669-4621-9f8d-866590aeeec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 5\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Importing functions (click to expand)</summary>\n",
    "\n",
    "You'll need these functions:\n",
    "```python\n",
    "from pyspark.sql.functions import hour, when, col\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Extracting hour (click to expand)</summary>\n",
    "\n",
    "Use the `hour()` function:\n",
    "```python\n",
    "df.withColumn(\"hour_column\", hour(\"datetime_column\"))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Conditional logic with when() (click to expand)</summary>\n",
    "\n",
    "Chain `when()` statements for multiple conditions:\n",
    "```python\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.withColumn(\"category\",\n",
    "    when(col(\"amount\") < 10, \"Low\")\n",
    "    .when(col(\"amount\") <= 30, \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 4:</b> Chaining withColumn() (click to expand)</summary>\n",
    "\n",
    "You can chain multiple `withColumn()` calls:\n",
    "```python\n",
    "df.withColumn(\"col1\", ...).withColumn(\"col2\", ...)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e8ce11d-13c0-49e4-ae8f-755c72d5f53a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 6: Aggregate Data by Hour üìà\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Create summary statistics grouped by hour of day.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "Use `groupBy()` and aggregation functions to calculate:\n",
    "\n",
    "* Group by: `pickup_hour`\n",
    "* Calculate:\n",
    "  * `trip_count` - Total number of trips (use `count()`)\n",
    "  * `total_revenue` - Sum of all fares (use `sum()`)\n",
    "  * `avg_fare` - Average fare amount (use `avg()`)\n",
    "  * `avg_distance` - Average trip distance (use `avg()`)\n",
    "\n",
    "* Store the result in a variable called `hourly_summary`\n",
    "* Round the averages to 2 decimal places\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54105214-8349-42d1-945c-34448977f9c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: GroupBy"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Import aggregation functions\n",
    "# from pyspark.sql.functions import ...\n",
    "\n",
    "# TODO: Group by pickup_hour and calculate aggregations\n",
    "# Store in hourly_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b3f4de-23da-4d0f-ac03-87b9651f8b0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 6\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Importing aggregation functions (click to expand)</summary>\n",
    "\n",
    "You'll need:\n",
    "```python\n",
    "from pyspark.sql.functions import count, sum, avg, round\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Using groupBy() with agg() (click to expand)</summary>\n",
    "\n",
    "Basic syntax:\n",
    "```python\n",
    "df.groupBy(\"column\").agg(\n",
    "    count(\"*\").alias(\"count_name\"),\n",
    "    sum(\"column2\").alias(\"sum_name\"),\n",
    "    avg(\"column3\").alias(\"avg_name\")\n",
    ")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Rounding values (click to expand)</summary>\n",
    "\n",
    "Wrap aggregations with `round()`:\n",
    "```python\n",
    "round(avg(\"column_name\"), 2).alias(\"avg_column\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97cac496-17d4-4e45-9322-331972c639f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 7: Sort the Results üîΩ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Sort the summary data for easy analysis.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Use `orderBy()` to sort by `pickup_hour` in ascending order\n",
    "* This will show the data chronologically from hour 0 (midnight) to hour 23 (11 PM)\n",
    "* Store the result in a variable called `final_df`\n",
    "* Display the results to verify\n",
    "\n",
    "**Bonus Challenge:** Can you also sort by total_revenue descending to see which hours generate the most revenue?\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0abdb79-cb51-42ab-ac32-fbc110b37336",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: OrderBy"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Sort hourly_summary by pickup_hour\n",
    "# Store in final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90fa077-9f07-48a6-99ad-445f6f00b4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 7\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Using orderBy() (click to expand)</summary>\n",
    "\n",
    "Basic syntax:\n",
    "```python\n",
    "df.orderBy(\"column_name\")  # Ascending by default\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Descending order (click to expand)</summary>\n",
    "\n",
    "For descending order:\n",
    "```python\n",
    "df.orderBy(col(\"column_name\").desc())\n",
    "# or\n",
    "df.orderBy(\"column_name\", ascending=False)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Multiple sort columns (click to expand)</summary>\n",
    "\n",
    "Sort by multiple columns:\n",
    "```python\n",
    "df.orderBy(\"column1\", col(\"column2\").desc())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7422e5a-9f97-4152-a746-31e633d443e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 8: Save Results to Unity Catalog üíæ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Write your transformed data to a new table in Unity Catalog.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Create a table in the `main` catalog\n",
    "* Use your default schema (usually your username)\n",
    "* Table name: `taxi_hourly_summary`\n",
    "* Full table name format: `main.<your_schema>.taxi_hourly_summary`\n",
    "* Use the `saveAsTable()` method\n",
    "* Mode: `overwrite` (so you can re-run the lab)\n",
    "\n",
    "**Important Notes:**\n",
    "* The table will be created in your personal schema\n",
    "* You can verify it was created by querying it\n",
    "* This table can now be used by other notebooks and queries!\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4873222e-dae0-425e-b1d5-521cfc07e45f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Write Table"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write final_df to Unity Catalog\n",
    "# Table: main.default.taxi_hourly_summary (or use your schema name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8355fbc1-fc87-416d-9576-76bd0d3df814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 8\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Using saveAsTable() (click to expand)</summary>\n",
    "\n",
    "Basic syntax:\n",
    "```python\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"catalog.schema.table_name\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Finding your schema name (click to expand)</summary>\n",
    "\n",
    "You can use:\n",
    "```python\n",
    "# Get current database/schema\n",
    "spark.sql(\"SELECT current_database()\").show()\n",
    "\n",
    "# Or just use 'default' schema\n",
    "\"main.default.taxi_hourly_summary\"\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Write modes (click to expand)</summary>\n",
    "\n",
    "Common modes:\n",
    "* `\"overwrite\"` - Replace table if it exists\n",
    "* `\"append\"` - Add to existing table\n",
    "* `\"error\"` - Fail if table exists (default)\n",
    "* `\"ignore\"` - Do nothing if table exists\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a6182a3-89af-40eb-821d-2a740b921bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 9: Verify Your Table ‚úÖ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Confirm that your table was created successfully.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Query your new table using SQL or PySpark\n",
    "* Display the results\n",
    "* Verify:\n",
    "  * All 24 hours are present (0-23)\n",
    "  * The columns are correct\n",
    "  * The data looks reasonable\n",
    "\n",
    "**Bonus:** Try querying it with SQL using `%sql` magic command!\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2456a4ef-a2b7-4a7f-bd25-b8e873c5990f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Verify Table"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Read your table back and display it\n",
    "# Verify it was created correctly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3d20daa-5787-4ae2-a578-95f03e4216f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 9\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Reading a table (click to expand)</summary>\n",
    "\n",
    "Read it back:\n",
    "```python\n",
    "verify_df = spark.table(\"main.default.taxi_hourly_summary\")\n",
    "display(verify_df)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Using SQL (click to expand)</summary>\n",
    "\n",
    "You can also use SQL:\n",
    "```sql\n",
    "%sql\n",
    "SELECT * FROM main.default.taxi_hourly_summary\n",
    "ORDER BY pickup_hour\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Checking row count (click to expand)</summary>\n",
    "\n",
    "Verify you have 24 rows (one per hour):\n",
    "```python\n",
    "print(f\"Row count: {verify_df.count()}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0710379d-0623-408d-8862-953e93b526f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# üìù Complete Solutions\n",
    "\n",
    "**Only look at these if you're stuck or want to verify your work!**\n",
    "\n",
    "Try to solve the challenges yourself first. Learning happens through struggle and problem-solving!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b2d5b7-bf4e-44dd-ae11-18b46d92da05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Tasks 1-2 (Load and Explore Data)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Task 1: Load the data\n",
    "taxi_df = spark.table(\"samples.nyctaxi.trips\")\n",
    "\n",
    "# Verify it loaded\n",
    "print(f\"Total rows: {taxi_df.count():,}\")\n",
    "print(f\"Total columns: {len(taxi_df.columns)}\")\n",
    "\n",
    "# Task 2: Explore the schema\n",
    "taxi_df.printSchema()\n",
    "\n",
    "# Display sample data\n",
    "display(taxi_df.limit(10))\n",
    "```\n",
    "\n",
    "**Key columns we need:**\n",
    "* `tpep_pickup_datetime` - Timestamp for pickup\n",
    "* `fare_amount` - Fare charged\n",
    "* `trip_distance` - Distance traveled\n",
    "* `passenger_count` - Number of passengers\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28cd44b2-2d1e-4bf0-a39e-213c9d979fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 3 (Select Columns)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Select only the columns we need\n",
    "selected_df = taxi_df.select(\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"fare_amount\",\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\"\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(\"Selected columns:\", selected_df.columns)\n",
    "display(selected_df)\n",
    "```\n",
    "\n",
    "**Why select?**\n",
    "* Improves performance by reducing data size\n",
    "* Makes transformations clearer\n",
    "* Only keeps relevant columns\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25056d15-05c9-4f71-a6b3-dee40f08ec55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 4 (Filter Valid Trips)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Filter for valid trips\n",
    "filtered_df = selected_df.filter(\n",
    "    (selected_df.fare_amount > 0) & \n",
    "    (selected_df.trip_distance > 0) & \n",
    "    (selected_df.fare_amount < 500)\n",
    ")\n",
    "\n",
    "# Check how many rows remain\n",
    "print(f\"Rows after filtering: {filtered_df.count():,}\")\n",
    "display(filtered_df)\n",
    "```\n",
    "\n",
    "**Alternative SQL-style syntax:**\n",
    "```python\n",
    "filtered_df = selected_df.filter(\n",
    "    \"fare_amount > 0 AND trip_distance > 0 AND fare_amount < 500\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Why filter?**\n",
    "* Removes invalid/suspicious data\n",
    "* Improves data quality\n",
    "* Reduces processing time\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e0475b9-9b59-4be5-b243-54c446de4a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 5 (Add Calculated Columns)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Import necessary functions\n",
    "from pyspark.sql.functions import hour, when, col\n",
    "\n",
    "# Add calculated columns\n",
    "enriched_df = filtered_df \\\n",
    "    .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "    .withColumn(\"revenue_category\",\n",
    "        when(col(\"fare_amount\") < 10, \"Low\")\n",
    "        .when(col(\"fare_amount\") <= 30, \"Medium\")\n",
    "        .otherwise(\"High\")\n",
    "    )\n",
    "\n",
    "# Verify new columns\n",
    "print(\"Columns after enrichment:\", enriched_df.columns)\n",
    "display(enriched_df)\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "* `hour()` extracts hour from timestamp\n",
    "* `when()` provides conditional logic (like IF-THEN-ELSE)\n",
    "* `otherwise()` is the final ELSE clause\n",
    "* Chain multiple `withColumn()` calls with `\\`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "192a0f98-13b5-4e56-b0c0-a1a3b6d28ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 6 (Group and Aggregate)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Import aggregation functions\n",
    "from pyspark.sql.functions import count, sum as spark_sum, avg, round as spark_round\n",
    "\n",
    "# Group by hour and calculate aggregations\n",
    "hourly_summary = enriched_df.groupBy(\"pickup_hour\").agg(\n",
    "    count(\"*\").alias(\"trip_count\"),\n",
    "    spark_round(spark_sum(\"fare_amount\"), 2).alias(\"total_revenue\"),\n",
    "    spark_round(avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "    spark_round(avg(\"trip_distance\"), 2).alias(\"avg_distance\")\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display(hourly_summary)\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "* `groupBy()` groups rows by column values\n",
    "* `agg()` applies aggregation functions\n",
    "* `alias()` renames the result columns\n",
    "* Use `spark_sum` and `spark_round` to avoid conflicts with Python built-ins\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "248994d4-49c4-483c-b79f-852f5a894a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 7 (Sort Results)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Sort by pickup_hour\n",
    "final_df = hourly_summary.orderBy(\"pickup_hour\")\n",
    "\n",
    "# Display sorted results\n",
    "display(final_df)\n",
    "\n",
    "# Bonus: Sort by revenue (descending)\n",
    "revenue_sorted = hourly_summary.orderBy(col(\"total_revenue\").desc())\n",
    "print(\"\\nTop revenue hours:\")\n",
    "display(revenue_sorted)\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "* `orderBy()` sorts the DataFrame\n",
    "* Default is ascending order\n",
    "* Use `.desc()` for descending\n",
    "* Can sort by multiple columns\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9191db22-7de8-4da0-9a29-d97fd857b9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Tasks 8-9 (Write and Verify Table)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Task 8: Write to Unity Catalog\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(\"main.default.taxi_hourly_summary\")\n",
    "\n",
    "print(\"‚úÖ Table created successfully!\")\n",
    "\n",
    "# Task 9: Verify the table\n",
    "verify_df = spark.table(\"main.default.taxi_hourly_summary\")\n",
    "\n",
    "print(f\"\\nTable has {verify_df.count()} rows (should be 24)\")\n",
    "print(\"\\nTable contents:\")\n",
    "display(verify_df)\n",
    "```\n",
    "\n",
    "**Using SQL to verify:**\n",
    "```sql\n",
    "%sql\n",
    "SELECT * \n",
    "FROM main.default.taxi_hourly_summary\n",
    "ORDER BY pickup_hour\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "* `saveAsTable()` writes to Unity Catalog\n",
    "* `mode(\"overwrite\")` replaces existing table\n",
    "* Table is now available to all users with permissions\n",
    "* Can query with SQL or PySpark\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4843f666-46ff-4ded-b917-41b8f7eafc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìú Complete Solution - All Steps Combined\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal complete solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Import all necessary functions\n",
    "from pyspark.sql.functions import (\n",
    "    hour, when, col, count, sum as spark_sum, \n",
    "    avg, round as spark_round\n",
    ")\n",
    "\n",
    "# Step 1: Load data\n",
    "taxi_df = spark.table(\"samples.nyctaxi.trips\")\n",
    "\n",
    "# Step 2: Select columns\n",
    "selected_df = taxi_df.select(\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"fare_amount\",\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\"\n",
    ")\n",
    "\n",
    "# Step 3: Filter valid trips\n",
    "filtered_df = selected_df.filter(\n",
    "    (col(\"fare_amount\") > 0) & \n",
    "    (col(\"trip_distance\") > 0) & \n",
    "    (col(\"fare_amount\") < 500)\n",
    ")\n",
    "\n",
    "# Step 4: Add calculated columns\n",
    "enriched_df = filtered_df \\\n",
    "    .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "    .withColumn(\"revenue_category\",\n",
    "        when(col(\"fare_amount\") < 10, \"Low\")\n",
    "        .when(col(\"fare_amount\") <= 30, \"Medium\")\n",
    "        .otherwise(\"High\")\n",
    "    )\n",
    "\n",
    "# Step 5: Group and aggregate\n",
    "hourly_summary = enriched_df.groupBy(\"pickup_hour\").agg(\n",
    "    count(\"*\").alias(\"trip_count\"),\n",
    "    spark_round(spark_sum(\"fare_amount\"), 2).alias(\"total_revenue\"),\n",
    "    spark_round(avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "    spark_round(avg(\"trip_distance\"), 2).alias(\"avg_distance\")\n",
    ")\n",
    "\n",
    "# Step 6: Sort results\n",
    "final_df = hourly_summary.orderBy(\"pickup_hour\")\n",
    "\n",
    "# Step 7: Write to Unity Catalog\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(\"main.default.taxi_hourly_summary\")\n",
    "\n",
    "# Step 8: Verify\n",
    "print(\"‚úÖ Lab completed successfully!\")\n",
    "print(f\"\\nCreated table with {final_df.count()} rows\")\n",
    "display(final_df)\n",
    "```\n",
    "\n",
    "**Optimized version (chained transformations):**\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# All transformations in one chain\n",
    "final_df = (\n",
    "    spark.table(\"samples.nyctaxi.trips\")\n",
    "    .select(\"tpep_pickup_datetime\", \"fare_amount\", \"trip_distance\", \"passenger_count\")\n",
    "    .filter((col(\"fare_amount\") > 0) & (col(\"trip_distance\") > 0) & (col(\"fare_amount\") < 500))\n",
    "    .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"revenue_category\",\n",
    "        when(col(\"fare_amount\") < 10, \"Low\")\n",
    "        .when(col(\"fare_amount\") <= 30, \"Medium\")\n",
    "        .otherwise(\"High\")\n",
    "    )\n",
    "    .groupBy(\"pickup_hour\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"trip_count\"),\n",
    "        round(sum(\"fare_amount\"), 2).alias(\"total_revenue\"),\n",
    "        round(avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "        round(avg(\"trip_distance\"), 2).alias(\"avg_distance\")\n",
    "    )\n",
    "    .orderBy(\"pickup_hour\")\n",
    ")\n",
    "\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(\"main.default.taxi_hourly_summary\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d2e8d6-04d5-4081-be11-69379a9ad7ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the PySpark Transformations Lab!\n",
    "\n",
    "### üéØ What You Accomplished:\n",
    "\n",
    "‚úÖ Read data from Unity Catalog  \n",
    "‚úÖ Applied `select()` to choose relevant columns  \n",
    "‚úÖ Applied `filter()` to clean invalid data  \n",
    "‚úÖ Applied `withColumn()` to create calculated fields  \n",
    "‚úÖ Applied `groupBy()` to aggregate data  \n",
    "‚úÖ Applied `orderBy()` to sort results  \n",
    "‚úÖ Wrote results to a new Unity Catalog table  \n",
    "\n",
    "### üìä Key Takeaways:\n",
    "\n",
    "* **Transformations are lazy** - They don't execute until an action is called\n",
    "* **Chain transformations** - Build complex logic step by step\n",
    "* **Unity Catalog** - Modern way to manage data in Databricks\n",
    "* **Business value** - Raw data ‚Üí Actionable insights\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "* Experiment with different aggregations\n",
    "* Try joining multiple tables\n",
    "* Explore window functions\n",
    "* Learn about partitioning and optimization\n",
    "* Build more complex data pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**Great work!** You're now ready to tackle real-world data engineering challenges! üí™"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
