{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc851fc8-f393-4c4d-b5bd-50d505be6261",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lab Title"
    }
   },
   "source": [
    "# OPTIMIZE and VACUUM Lab üß™\n",
    "\n",
    "Welcome to the OPTIMIZE and VACUUM lab! In this hands-on lab, you'll learn how to maintain and optimize Delta Lake tables for better performance and storage efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. ‚úÖ Understand why small files are a performance problem\n",
    "2. ‚úÖ Use **OPTIMIZE** to compact small files\n",
    "3. ‚úÖ Use **ZORDER** to co-locate related data\n",
    "4. ‚úÖ Use **VACUUM** to remove old files and save storage\n",
    "5. ‚úÖ Monitor table health with DESCRIBE DETAIL\n",
    "6. ‚úÖ Check table history with DESCRIBE HISTORY\n",
    "7. ‚úÖ Understand retention periods and data lifecycle\n",
    "\n",
    "---\n",
    "\n",
    "## üìä What are OPTIMIZE and VACUUM?\n",
    "\n",
    "### **OPTIMIZE**\n",
    "* **Problem:** Many small files slow down queries\n",
    "* **Solution:** Combine small files into larger, optimized files\n",
    "* **Benefit:** Faster queries, better compression\n",
    "* **Optional:** ZORDER for data clustering\n",
    "\n",
    "### **VACUUM**\n",
    "* **Problem:** Old files accumulate and waste storage\n",
    "* **Solution:** Delete files no longer needed\n",
    "* **Benefit:** Reduced storage costs\n",
    "* **Caution:** Affects time travel capability\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Lab Structure\n",
    "\n",
    "This lab has **9 tasks** to complete:\n",
    "\n",
    "1. Create a table with small files (simulate the problem)\n",
    "2. Check table details (see the small files issue)\n",
    "3. Run OPTIMIZE to compact files\n",
    "4. Verify optimization results\n",
    "5. Add more data and use ZORDER\n",
    "6. Run VACUUM to clean up old files\n",
    "7. Understand retention periods\n",
    "8. Monitor table health\n",
    "9. Best practices review\n",
    "\n",
    "**Each task includes:**\n",
    "* üìù Clear instructions\n",
    "* üí° Hints to guide you\n",
    "* ‚úÖ Solutions at the end (try first!)\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Dataset\n",
    "\n",
    "We'll create our own table with booking data to simulate real-world scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14dd15b7-8adf-4c52-addf-3172645239f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Create a Table with Small Files üìÅ\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "When data is written in many small batches, Delta tables end up with many small files. This hurts query performance!\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Create a Delta table with booking data by writing multiple small batches.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Create a table called `main.default.bookings_lab`\n",
    "2. Write data in **5 separate batches** (simulating incremental writes)\n",
    "3. Each batch should have 1000 rows\n",
    "4. Use `.coalesce(1)` to ensure each batch creates a small file\n",
    "5. Use mode `append` for batches 2-5\n",
    "\n",
    "**Data structure:**\n",
    "* `booking_id` - INT (sequential)\n",
    "* `customer_id` - INT (random 1-500)\n",
    "* `booking_date` - DATE (random dates in 2024)\n",
    "* `amount` - DOUBLE (random 50-1000)\n",
    "* `region` - STRING (random: 'North', 'South', 'East', 'West')\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17c9cb30-f237-4298-ae26-b71728a89baa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Create Table"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Generate and write 5 batches of booking data\n",
    "# Each batch: 1000 rows, coalesce(1) to create small files\n",
    "# Batch 1: mode(\"overwrite\"), Batches 2-5: mode(\"append\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ce4779-fbca-4673-9c45-2bb1f19cd9ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 1\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Generating sample data (click to expand)</summary>\n",
    "\n",
    "Use Python to generate data:\n",
    "```python\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data = [\n",
    "    (i, random.randint(1, 500), \n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "     round(random.uniform(50, 1000), 2),\n",
    "     random.choice(['North', 'South', 'East', 'West']))\n",
    "    for i in range(start_id, end_id)\n",
    "]\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Creating DataFrame (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"booking_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"booking_date\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"region\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Writing batches (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Batch 1 (overwrite)\n",
    "df1.coalesce(1).write.mode(\"overwrite\").saveAsTable(\"main.default.bookings_lab\")\n",
    "\n",
    "# Batch 2-5 (append)\n",
    "df2.coalesce(1).write.mode(\"append\").saveAsTable(\"main.default.bookings_lab\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c36e57a-39a7-4199-963d-42fd6ce37a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Check Table Details üîç\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Inspect the table to see the small files problem.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use `DESCRIBE DETAIL` to view table metadata\n",
    "2. Look for:\n",
    "   * `numFiles` - Should be around 5 (one per batch)\n",
    "   * `sizeInBytes` - Total table size\n",
    "   * `location` - Where the table is stored\n",
    "3. Calculate the average file size\n",
    "\n",
    "**Questions to answer:**\n",
    "* How many files does the table have?\n",
    "* What's the average file size?\n",
    "* Is this optimal? (Hint: Ideal file size is 128MB-1GB)\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc6297e2-04d3-4c88-a3f4-248c26f5f904",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Check Details"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Use DESCRIBE DETAIL to inspect the table\n",
    "-- Look at numFiles and sizeInBytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c758ca56-0ac6-4565-9501-a43fe6dfab29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 2\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> DESCRIBE DETAIL syntax (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "DESCRIBE DETAIL catalog.schema.table_name\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Key columns to look at (click to expand)</summary>\n",
    "\n",
    "Important columns:\n",
    "* `numFiles` - Number of data files\n",
    "* `sizeInBytes` - Total size in bytes\n",
    "* `location` - Table location\n",
    "* `format` - Should be 'delta'\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Calculate average file size (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  numFiles,\n",
    "  sizeInBytes,\n",
    "  ROUND(sizeInBytes / numFiles / 1024 / 1024, 2) AS avg_file_size_mb\n",
    "FROM (\n",
    "  DESCRIBE DETAIL main.default.bookings_lab\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cda38d0e-a628-4111-a759-194b4aa491fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Run OPTIMIZE ‚ö°\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Your table has 5 small files. Reading many small files is slow because:\n",
    "* More file open/close operations\n",
    "* Less efficient compression\n",
    "* More metadata to track\n",
    "* Slower query performance\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Run OPTIMIZE to compact the small files into larger, optimized files.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use the `OPTIMIZE` command on your table\n",
    "2. Run it using SQL (`%sql`)\n",
    "3. Observe the output metrics:\n",
    "   * `numFilesAdded` - New optimized files created\n",
    "   * `numFilesRemoved` - Old small files marked for removal\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "OPTIMIZE catalog.schema.table_name\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e8aa5a-94c2-4004-a714-6b2bc4152b05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: OPTIMIZE"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Run OPTIMIZE on main.default.bookings_lab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90c7e62-703c-4206-a4c5-3715cde04dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 3\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> OPTIMIZE syntax (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "OPTIMIZE main.default.bookings_lab\n",
    "```\n",
    "\n",
    "That's it! Just one line.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Understanding the output (click to expand)</summary>\n",
    "\n",
    "OPTIMIZE returns metrics:\n",
    "* `numFilesAdded` - New optimized files (usually 1)\n",
    "* `numFilesRemoved` - Old files marked for deletion (should be 5)\n",
    "* `totalFilesSkipped` - Files already optimal\n",
    "* `totalTimeMs` - Time taken\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> What happens (click to expand)</summary>\n",
    "\n",
    "OPTIMIZE:\n",
    "1. Reads all small files\n",
    "2. Combines them into larger files\n",
    "3. Writes new optimized files\n",
    "4. Marks old files for deletion (but doesn't delete yet!)\n",
    "5. Updates transaction log\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c3c949-a357-40bc-966a-8cbc09737ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Verify Optimization Results ‚úÖ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Check if OPTIMIZE worked by inspecting the table again.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Run `DESCRIBE DETAIL` again on your table\n",
    "2. Compare with Task 2 results:\n",
    "   * `numFiles` - Should be fewer (ideally 1)\n",
    "   * `sizeInBytes` - Should be similar (same data)\n",
    "3. Calculate the new average file size\n",
    "\n",
    "**Expected results:**\n",
    "* Before OPTIMIZE: ~5 small files\n",
    "* After OPTIMIZE: 1 larger file\n",
    "* File size: Much larger per file\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d672a1-6181-40ab-8e29-6054da9a6c68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Verify Optimization"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Run DESCRIBE DETAIL again to see the changes\n",
    "-- Compare numFiles before and after\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eeea767-88f5-44b4-b2e3-45e4a0ee90ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 4\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Same command as Task 2 (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "DESCRIBE DETAIL main.default.bookings_lab\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> What to look for (click to expand)</summary>\n",
    "\n",
    "Compare:\n",
    "* **Before:** numFiles = 5, small avg file size\n",
    "* **After:** numFiles = 1, larger avg file size\n",
    "* **Data:** sizeInBytes should be similar (same data, better compressed)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Why files are better (click to expand)</summary>\n",
    "\n",
    "Larger files mean:\n",
    "* Fewer file operations\n",
    "* Better compression\n",
    "* Faster queries\n",
    "* More efficient storage\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b2e6f11-e20b-4743-a0d7-83ed11fecfa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 5: Add More Data and Use ZORDER üéØ\n",
    "\n",
    "**What is ZORDER?**\n",
    "\n",
    "ZORDER BY co-locates related data in the same files, making queries on those columns much faster.\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Add more data and optimize with ZORDER.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "**Part A: Add more data**\n",
    "1. Generate 2000 more rows (booking_id 5001-7000)\n",
    "2. Append to the table\n",
    "3. Use `.coalesce(2)` to create 2 more small files\n",
    "\n",
    "**Part B: OPTIMIZE with ZORDER**\n",
    "1. Run OPTIMIZE with ZORDER BY on the `region` column\n",
    "2. This will cluster data by region for faster region-based queries\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "OPTIMIZE table_name ZORDER BY (column_name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cells below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ea364f9-6d2f-4755-bd9e-c18fcd01e683",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Add More Data"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Generate 2000 more rows (booking_id 5001-7000)\n",
    "# Append to the table with coalesce(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40989508-372b-4852-9264-cabb4bd1e879",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: OPTIMIZE with ZORDER"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Run OPTIMIZE with ZORDER BY (region)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06682370-ff18-43b4-b12a-f5f9d3a1f6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 5\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Generating more data (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Similar to Task 1, but different ID range\n",
    "data_batch6 = [\n",
    "    (i, random.randint(1, 500), \n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "     round(random.uniform(50, 1000), 2),\n",
    "     random.choice(['North', 'South', 'East', 'West']))\n",
    "    for i in range(5001, 7001)\n",
    "]\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> ZORDER syntax (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "OPTIMIZE main.default.bookings_lab\n",
    "ZORDER BY (region)\n",
    "```\n",
    "\n",
    "You can ZORDER by multiple columns:\n",
    "```sql\n",
    "ZORDER BY (region, booking_date)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> When to use ZORDER (click to expand)</summary>\n",
    "\n",
    "Use ZORDER on columns that are:\n",
    "* Frequently used in WHERE clauses\n",
    "* Used for joins\n",
    "* High cardinality (many distinct values)\n",
    "* Not partition columns (use partitioning instead)\n",
    "\n",
    "In our case: `region` is frequently filtered, so ZORDER helps!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0fd2d7-c264-4384-8376-8c2bcc48761f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 6: Understand VACUUM üß™\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "After OPTIMIZE, the old small files are still on disk! They're marked for deletion in the transaction log, but physically still exist.\n",
    "\n",
    "**Why?**\n",
    "* Time travel needs old files\n",
    "* Concurrent readers might be using them\n",
    "* Safety buffer before permanent deletion\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Run VACUUM DRY RUN to see what would be deleted.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use `VACUUM` with `DRY RUN` option\n",
    "2. Set retention to 0 hours (for demo purposes only!)\n",
    "3. Observe what files would be deleted\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "VACUUM table_name RETAIN 0 HOURS DRY RUN\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è Important:** \n",
    "* DRY RUN shows what WOULD be deleted (doesn't actually delete)\n",
    "* RETAIN 0 HOURS is only for demos - never use in production!\n",
    "* Default retention is 7 days (168 hours)\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ec24af7-718b-402b-bc02-c662cba804ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: VACUUM DRY RUN"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Run VACUUM with DRY RUN to preview deletions\n",
    "-- Use RETAIN 0 HOURS for this demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0df1ccca-e484-4b59-99d9-370dfd4ac67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 6\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> VACUUM DRY RUN syntax (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "VACUUM main.default.bookings_lab RETAIN 0 HOURS DRY RUN\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Disable retention check (click to expand)</summary>\n",
    "\n",
    "For RETAIN 0 HOURS, you need to disable the safety check first:\n",
    "```sql\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "VACUUM main.default.bookings_lab RETAIN 0 HOURS DRY RUN\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Understanding output (click to expand)</summary>\n",
    "\n",
    "DRY RUN shows:\n",
    "* List of files that would be deleted\n",
    "* These are the old small files from before OPTIMIZE\n",
    "* No files are actually deleted (safe to run)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c11ca8e1-fe18-4d12-8f0e-6722d5e8d7ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 7: Run VACUUM (Actually Delete Files) üóëÔ∏è\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Now run VACUUM for real to delete the old files.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Disable the retention check (required for 0 hours)\n",
    "2. Run VACUUM with RETAIN 0 HOURS (without DRY RUN)\n",
    "3. Observe the output showing deleted files\n",
    "\n",
    "**Commands needed:**\n",
    "```sql\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "VACUUM table_name RETAIN 0 HOURS\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è Warning:** \n",
    "* This permanently deletes files!\n",
    "* Time travel to old versions will fail after VACUUM\n",
    "* In production, use 7+ days retention\n",
    "* We use 0 hours only for demo purposes\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224dec60-304d-4b16-8f88-fef62605c37c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Run VACUUM"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Disable retention check and run VACUUM\n",
    "-- This will actually delete the old files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1172c95-3c91-4b33-a1d9-95f32ea6a544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 7\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Two commands needed (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "-- Command 1: Disable safety check\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "\n",
    "-- Command 2: Run VACUUM\n",
    "VACUUM main.default.bookings_lab RETAIN 0 HOURS\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> What happens (click to expand)</summary>\n",
    "\n",
    "VACUUM:\n",
    "1. Finds files older than retention period\n",
    "2. Checks they're not in current table version\n",
    "3. Permanently deletes them from storage\n",
    "4. Frees up disk space\n",
    "5. Returns list of deleted files\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Production retention (click to expand)</summary>\n",
    "\n",
    "In production, use:\n",
    "```sql\n",
    "VACUUM table_name RETAIN 168 HOURS  -- 7 days (default)\n",
    "VACUUM table_name RETAIN 720 HOURS  -- 30 days\n",
    "```\n",
    "\n",
    "Balance:\n",
    "* Longer retention = More time travel, more storage cost\n",
    "* Shorter retention = Less time travel, less storage cost\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b95afc-4d83-49d5-9065-dd62fa26c985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 8: Check Table History üìú\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "View the complete history of operations on your table.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use `DESCRIBE HISTORY` to view all transactions\n",
    "2. Look for these operations:\n",
    "   * `CREATE OR REPLACE TABLE` (initial creation)\n",
    "   * `WRITE` (your append operations)\n",
    "   * `OPTIMIZE` (file compaction)\n",
    "   * `VACUUM` (file deletion)\n",
    "3. Examine the `operationMetrics` to see:\n",
    "   * How many files were added/removed\n",
    "   * How much data was processed\n",
    "\n",
    "**Questions to answer:**\n",
    "* How many versions does your table have?\n",
    "* Which operations created the most files?\n",
    "* What did OPTIMIZE do (check numFilesAdded/numFilesRemoved)?\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08cdeb7d-9456-49e7-b977-26a8e481ef82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Check History"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Use DESCRIBE HISTORY to view all operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c01435-0225-4f2b-b063-4f7f771f13d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 8\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> DESCRIBE HISTORY syntax (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "DESCRIBE HISTORY main.default.bookings_lab\n",
    "```\n",
    "\n",
    "Limit to recent operations:\n",
    "```sql\n",
    "DESCRIBE HISTORY main.default.bookings_lab LIMIT 10\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Key columns to examine (click to expand)</summary>\n",
    "\n",
    "Important columns:\n",
    "* `version` - Transaction number\n",
    "* `timestamp` - When it happened\n",
    "* `operation` - Type of operation\n",
    "* `operationMetrics` - Detailed metrics (files, rows, bytes)\n",
    "* `userName` - Who did it\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Analyzing OPTIMIZE metrics (click to expand)</summary>\n",
    "\n",
    "For OPTIMIZE operations, look at:\n",
    "```\n",
    "operationMetrics:\n",
    "  numFilesAdded: 1 (new optimized file)\n",
    "  numFilesRemoved: 5 (old small files)\n",
    "  minFileSize: ...\n",
    "  maxFileSize: ...\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f6167e8-ea2c-4f60-acb3-57b740bc8612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 9: Final Table Health Check üéØ\n",
    "\n",
    "**Your Challenge:**\n",
    "\n",
    "Perform a final health check on your optimized table.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Run `DESCRIBE DETAIL` one more time\n",
    "2. Verify the table is in good shape:\n",
    "   * `numFiles` - Should be 1 or very few\n",
    "   * `sizeInBytes` - Total data size\n",
    "   * Calculate average file size (should be larger now)\n",
    "3. Query the table to ensure data is intact\n",
    "4. Count total rows (should be 7000)\n",
    "\n",
    "**Success criteria:**\n",
    "* ‚úÖ Fewer files than before\n",
    "* ‚úÖ Larger average file size\n",
    "* ‚úÖ All data still accessible\n",
    "* ‚úÖ No data loss\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code in the cells below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dabed70-3d04-4204-ae1a-6cfbafa69781",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Final Health Check"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Run DESCRIBE DETAIL to check final state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "761fa5cf-fa5d-440d-8d62-5dc3a284de5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "YOUR CODE: Verify Data Integrity"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Query the table and count rows\n",
    "-- Should have 7000 rows total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11639294-394e-48a4-8825-0c14236f178e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Hints for Task 9\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1:</b> Check table details (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "DESCRIBE DETAIL main.default.bookings_lab\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2:</b> Verify row count (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(*) AS total_rows\n",
    "FROM main.default.bookings_lab\n",
    "```\n",
    "\n",
    "Should be 7000 (5 batches of 1000 + 1 batch of 2000)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3:</b> Sample the data (click to expand)</summary>\n",
    "\n",
    "```sql\n",
    "SELECT * \n",
    "FROM main.default.bookings_lab\n",
    "ORDER BY booking_id\n",
    "LIMIT 20\n",
    "```\n",
    "\n",
    "Verify:\n",
    "* booking_id ranges from 1 to 7000\n",
    "* All columns present\n",
    "* Data looks correct\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ff76f2-1804-4749-b38c-2c59286645c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# üìù Complete Solutions\n",
    "\n",
    "**‚ö†Ô∏è Only look at these if you're stuck or want to verify your work!**\n",
    "\n",
    "Try to solve the challenges yourself first. Learning happens through struggle and problem-solving!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "622337a5-7c1c-402b-adf1-94f0c95f3eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 1 (Create Table with Small Files)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```python\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"booking_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"booking_date\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"region\", StringType())\n",
    "])\n",
    "\n",
    "# Generate and write 5 batches\n",
    "for batch_num in range(1, 6):\n",
    "    start_id = (batch_num - 1) * 1000 + 1\n",
    "    end_id = batch_num * 1000 + 1\n",
    "    \n",
    "    # Generate data\n",
    "    data = [\n",
    "        (i, \n",
    "         random.randint(1, 500),\n",
    "         (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "         round(random.uniform(50, 1000), 2),\n",
    "         random.choice(['North', 'South', 'East', 'West']))\n",
    "        for i in range(start_id, end_id)\n",
    "    ]\n",
    "    \n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    \n",
    "    # Write batch\n",
    "    if batch_num == 1:\n",
    "        df.coalesce(1).write.mode(\"overwrite\").saveAsTable(\"main.default.bookings_lab\")\n",
    "    else:\n",
    "        df.coalesce(1).write.mode(\"append\").saveAsTable(\"main.default.bookings_lab\")\n",
    "    \n",
    "    print(f\"‚úÖ Batch {batch_num} written: {len(data)} rows\")\n",
    "\n",
    "print(f\"\\n‚úÖ Table created with 5000 rows in 5 small files\")\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "* Loop to create multiple batches\n",
    "* `coalesce(1)` forces one file per batch\n",
    "* First batch uses `overwrite`, rest use `append`\n",
    "* This simulates real-world incremental writes\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f0b91a-1b68-4fff-b524-fc3a26e79883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 2 (Check Table Details)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```sql\n",
    "DESCRIBE DETAIL main.default.bookings_lab\n",
    "```\n",
    "\n",
    "**Calculate average file size:**\n",
    "```sql\n",
    "SELECT \n",
    "  numFiles,\n",
    "  sizeInBytes,\n",
    "  ROUND(sizeInBytes / numFiles / 1024 / 1024, 2) AS avg_file_size_mb\n",
    "FROM (\n",
    "  DESCRIBE DETAIL main.default.bookings_lab\n",
    ")\n",
    "```\n",
    "\n",
    "**What you should see:**\n",
    "* `numFiles`: 5\n",
    "* `sizeInBytes`: ~500KB-1MB total\n",
    "* `avg_file_size_mb`: Very small (< 1 MB)\n",
    "\n",
    "**Why this is bad:**\n",
    "* Small files = many file operations\n",
    "* Inefficient for Spark to process\n",
    "* Slower queries\n",
    "* More metadata overhead\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40c4425f-54e3-48d7-b78a-91da9008f836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 3 (Run OPTIMIZE)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```sql\n",
    "OPTIMIZE main.default.bookings_lab\n",
    "```\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "metrics:\n",
    "  numFilesAdded: 1\n",
    "  numFilesRemoved: 5\n",
    "  totalFilesSkipped: 0\n",
    "  totalTimeMs: ~1000-5000\n",
    "```\n",
    "\n",
    "**What happened:**\n",
    "1. Delta Lake read all 5 small files\n",
    "2. Combined them into 1 larger file\n",
    "3. Wrote the new optimized file\n",
    "4. Marked old files for deletion (in transaction log)\n",
    "5. Old files still exist on disk (until VACUUM)\n",
    "\n",
    "**Benefits:**\n",
    "* Queries now read 1 file instead of 5\n",
    "* Better compression\n",
    "* Faster performance\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f11e0bf-22d5-492f-978e-503e778be954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 4 (Verify Optimization)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```sql\n",
    "DESCRIBE DETAIL main.default.bookings_lab\n",
    "```\n",
    "\n",
    "**What changed:**\n",
    "\n",
    "**Before OPTIMIZE:**\n",
    "* numFiles: 5\n",
    "* avg file size: ~100-200 KB\n",
    "\n",
    "**After OPTIMIZE:**\n",
    "* numFiles: 1\n",
    "* avg file size: ~500KB-1MB\n",
    "\n",
    "**Verification:**\n",
    "```sql\n",
    "SELECT COUNT(*) AS total_rows\n",
    "FROM main.default.bookings_lab\n",
    "```\n",
    "Should still be 5000 rows (no data lost!)\n",
    "\n",
    "**Key insight:** OPTIMIZE doesn't change data, just reorganizes files for better performance.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b011f6b4-497a-429c-95e3-b70c9efe6a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 5 (Add Data and Use ZORDER)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "**Part A: Add more data**\n",
    "```python\n",
    "# Generate 2000 more rows\n",
    "data_batch6 = [\n",
    "    (i, \n",
    "     random.randint(1, 500),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "     round(random.uniform(50, 1000), 2),\n",
    "     random.choice(['North', 'South', 'East', 'West']))\n",
    "    for i in range(5001, 7001)\n",
    "]\n",
    "\n",
    "df_batch6 = spark.createDataFrame(data_batch6, schema)\n",
    "df_batch6.coalesce(2).write.mode(\"append\").saveAsTable(\"main.default.bookings_lab\")\n",
    "\n",
    "print(\"‚úÖ Added 2000 more rows in 2 files\")\n",
    "```\n",
    "\n",
    "**Part B: OPTIMIZE with ZORDER**\n",
    "```sql\n",
    "OPTIMIZE main.default.bookings_lab\n",
    "ZORDER BY (region)\n",
    "```\n",
    "\n",
    "**What ZORDER does:**\n",
    "* Co-locates data with same region values\n",
    "* Makes queries filtering by region much faster\n",
    "* Example: `WHERE region = 'North'` only reads relevant files\n",
    "\n",
    "**When to use ZORDER:**\n",
    "* Columns frequently used in WHERE clauses\n",
    "* High cardinality columns\n",
    "* Columns used for joins\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2690abd-7439-46d6-9c09-4ccab7bc6864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 6 (VACUUM DRY RUN)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```sql\n",
    "-- Disable retention check (required for 0 hours)\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "\n",
    "-- Run VACUUM DRY RUN\n",
    "VACUUM main.default.bookings_lab RETAIN 0 HOURS DRY RUN\n",
    "```\n",
    "\n",
    "**Expected output:**\n",
    "* List of file paths that would be deleted\n",
    "* These are the old files from before OPTIMIZE\n",
    "* No files are actually deleted (DRY RUN is safe)\n",
    "\n",
    "**Why DRY RUN is useful:**\n",
    "* Preview what will be deleted\n",
    "* Verify retention period is correct\n",
    "* Check if important files would be removed\n",
    "* Safe to run in production\n",
    "\n",
    "**üí° Best practice:** Always run DRY RUN first before actual VACUUM!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a9089d-a44b-47a5-8a10-db00d1bd6504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 7 (Run VACUUM)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```sql\n",
    "-- Disable retention check\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "\n",
    "-- Run VACUUM (actually delete files)\n",
    "VACUUM main.default.bookings_lab RETAIN 0 HOURS\n",
    "```\n",
    "\n",
    "**Expected output:**\n",
    "* List of deleted file paths\n",
    "* These files are permanently removed from storage\n",
    "* Storage space is freed\n",
    "\n",
    "**What happened:**\n",
    "1. VACUUM found files older than 0 hours\n",
    "2. Verified they're not in current table version\n",
    "3. Permanently deleted them from cloud storage\n",
    "4. Freed up disk space\n",
    "\n",
    "**‚ö†Ô∏è Important:**\n",
    "* Time travel to old versions will now fail\n",
    "* This is permanent - files cannot be recovered\n",
    "* In production, use 7+ days retention\n",
    "\n",
    "**Production example:**\n",
    "```sql\n",
    "-- Safe production VACUUM (7 days retention)\n",
    "VACUUM main.default.bookings_lab RETAIN 168 HOURS\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e5c5de-72ce-4218-813c-8b8bf9e2e348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 8 (Check Table History)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "```sql\n",
    "DESCRIBE HISTORY main.default.bookings_lab\n",
    "```\n",
    "\n",
    "**What you should see:**\n",
    "\n",
    "Multiple versions showing:\n",
    "1. Version 0: `CREATE OR REPLACE TABLE`\n",
    "2. Versions 1-4: `WRITE` (append operations)\n",
    "3. Version 5: `OPTIMIZE`\n",
    "4. Version 6: `WRITE` (batch 6)\n",
    "5. Version 7: `OPTIMIZE` (with ZORDER)\n",
    "6. Version 8: `VACUUM`\n",
    "\n",
    "**Analyzing OPTIMIZE metrics:**\n",
    "```sql\n",
    "SELECT \n",
    "  version,\n",
    "  operation,\n",
    "  operationMetrics.numFilesAdded AS files_added,\n",
    "  operationMetrics.numFilesRemoved AS files_removed\n",
    "FROM (\n",
    "  DESCRIBE HISTORY main.default.bookings_lab\n",
    ")\n",
    "WHERE operation = 'OPTIMIZE'\n",
    "```\n",
    "\n",
    "**Key insights:**\n",
    "* Each operation creates a new version\n",
    "* OPTIMIZE shows files added/removed\n",
    "* Complete audit trail of all changes\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2e64670-dfec-4b3e-a380-fd3e92611de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Solution: Task 9 (Final Table Health Check)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solution</b></summary>\n",
    "\n",
    "**Check table details:**\n",
    "```sql\n",
    "DESCRIBE DETAIL main.default.bookings_lab\n",
    "```\n",
    "\n",
    "**Expected results:**\n",
    "* `numFiles`: 1 (or very few)\n",
    "* `sizeInBytes`: ~1-2 MB\n",
    "* Much better than 5+ small files!\n",
    "\n",
    "**Verify data integrity:**\n",
    "```sql\n",
    "SELECT COUNT(*) AS total_rows\n",
    "FROM main.default.bookings_lab\n",
    "```\n",
    "Should be 7000 rows.\n",
    "\n",
    "**Sample the data:**\n",
    "```sql\n",
    "SELECT * \n",
    "FROM main.default.bookings_lab\n",
    "ORDER BY booking_id\n",
    "LIMIT 20\n",
    "```\n",
    "\n",
    "**Success criteria:**\n",
    "* ‚úÖ Fewer files (1 vs 5+)\n",
    "* ‚úÖ All 7000 rows present\n",
    "* ‚úÖ Data intact and queryable\n",
    "* ‚úÖ Better performance for queries\n",
    "\n",
    "**What we accomplished:**\n",
    "1. Created table with small files (the problem)\n",
    "2. Used OPTIMIZE to compact files (the solution)\n",
    "3. Used ZORDER to cluster data (performance boost)\n",
    "4. Used VACUUM to clean up old files (save storage)\n",
    "5. Verified everything works (data integrity)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1df5ceef-c0d7-4417-badd-56d939f6ce14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Best Practices Summary\n",
    "\n",
    "### **OPTIMIZE Best Practices**\n",
    "\n",
    "‚úÖ **Run regularly** - After many small writes (daily/weekly)  \n",
    "‚úÖ **Use ZORDER** - On frequently filtered columns  \n",
    "‚úÖ **Monitor file count** - Keep numFiles reasonable  \n",
    "‚úÖ **Schedule during off-peak** - Can be resource-intensive  \n",
    "‚úÖ **Check metrics** - Use DESCRIBE HISTORY to verify  \n",
    "\n",
    "**When to OPTIMIZE:**\n",
    "* After many incremental writes\n",
    "* When queries are slow\n",
    "* When numFiles is high (> 100)\n",
    "* Before important queries/reports\n",
    "\n",
    "**ZORDER columns:**\n",
    "* High cardinality (many distinct values)\n",
    "* Frequently in WHERE clauses\n",
    "* Used for joins\n",
    "* Not partition columns\n",
    "\n",
    "---\n",
    "\n",
    "### **VACUUM Best Practices**\n",
    "\n",
    "‚úÖ **Use appropriate retention** - 7 days minimum (168 hours)  \n",
    "‚úÖ **Always DRY RUN first** - Preview before deleting  \n",
    "‚úÖ **Consider time travel needs** - Longer retention for auditing  \n",
    "‚úÖ **Schedule regularly** - Weekly or monthly  \n",
    "‚úÖ **Monitor storage** - Track space savings  \n",
    "\n",
    "**Retention guidelines:**\n",
    "* **7 days (168 hours)** - Minimum, default\n",
    "* **30 days (720 hours)** - Good for most use cases\n",
    "* **90 days (2160 hours)** - Compliance/audit requirements\n",
    "* **Never use 0 hours** - Only for demos!\n",
    "\n",
    "**When to VACUUM:**\n",
    "* After multiple OPTIMIZE operations\n",
    "* When storage costs are high\n",
    "* Regularly scheduled maintenance\n",
    "* After large DELETE/UPDATE operations\n",
    "\n",
    "---\n",
    "\n",
    "### **Monitoring**\n",
    "\n",
    "‚úÖ **DESCRIBE DETAIL** - Check file count and sizes  \n",
    "‚úÖ **DESCRIBE HISTORY** - Track operations  \n",
    "‚úÖ **Set up alerts** - Monitor table health  \n",
    "‚úÖ **Track metrics** - Files, size, query performance  \n",
    "\n",
    "---\n",
    "\n",
    "### **Common Workflow**\n",
    "\n",
    "```sql\n",
    "-- 1. Check table health\n",
    "DESCRIBE DETAIL table_name\n",
    "\n",
    "-- 2. Optimize if needed (many small files)\n",
    "OPTIMIZE table_name ZORDER BY (frequently_filtered_column)\n",
    "\n",
    "-- 3. Preview vacuum\n",
    "VACUUM table_name RETAIN 168 HOURS DRY RUN\n",
    "\n",
    "-- 4. Run vacuum\n",
    "VACUUM table_name RETAIN 168 HOURS\n",
    "\n",
    "-- 5. Verify results\n",
    "DESCRIBE DETAIL table_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac3c186-6375-4313-be11-a82ce534ef75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üí° Key Concepts Review\n",
    "\n",
    "### **Small Files Problem**\n",
    "\n",
    "**Causes:**\n",
    "* Incremental writes (streaming, frequent inserts)\n",
    "* Many small transactions\n",
    "* Unoptimized data ingestion\n",
    "\n",
    "**Impact:**\n",
    "* Slow query performance\n",
    "* More metadata overhead\n",
    "* Inefficient resource usage\n",
    "\n",
    "**Solution:** OPTIMIZE\n",
    "\n",
    "---\n",
    "\n",
    "### **OPTIMIZE**\n",
    "\n",
    "**What it does:**\n",
    "* Combines small files into larger files\n",
    "* Improves compression\n",
    "* Updates transaction log\n",
    "* Marks old files for deletion\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "OPTIMIZE table_name\n",
    "OPTIMIZE table_name ZORDER BY (column)\n",
    "```\n",
    "\n",
    "**When to run:**\n",
    "* After many small writes\n",
    "* When numFiles is high\n",
    "* Before important queries\n",
    "* Scheduled maintenance\n",
    "\n",
    "---\n",
    "\n",
    "### **ZORDER**\n",
    "\n",
    "**What it does:**\n",
    "* Co-locates related data\n",
    "* Clusters data by column values\n",
    "* Enables data skipping\n",
    "* Faster filtered queries\n",
    "\n",
    "**Best columns for ZORDER:**\n",
    "* High cardinality\n",
    "* Frequently filtered\n",
    "* Used in joins\n",
    "* Not partition columns\n",
    "\n",
    "---\n",
    "\n",
    "### **VACUUM**\n",
    "\n",
    "**What it does:**\n",
    "* Permanently deletes old files\n",
    "* Frees storage space\n",
    "* Removes files older than retention\n",
    "* Cannot be undone\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "VACUUM table_name RETAIN n HOURS\n",
    "VACUUM table_name RETAIN n HOURS DRY RUN\n",
    "```\n",
    "\n",
    "**Retention trade-offs:**\n",
    "* **Longer retention:**\n",
    "  - ‚úÖ More time travel capability\n",
    "  - ‚ùå Higher storage costs\n",
    "* **Shorter retention:**\n",
    "  - ‚úÖ Lower storage costs\n",
    "  - ‚ùå Less time travel capability\n",
    "\n",
    "---\n",
    "\n",
    "### **Transaction Log**\n",
    "\n",
    "**Role in OPTIMIZE/VACUUM:**\n",
    "* Tracks which files are current\n",
    "* Marks old files for deletion\n",
    "* Enables time travel\n",
    "* VACUUM uses it to find deletable files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca11367a-e7a8-4d79-91a9-097de7aacb9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéâ Lab Complete!\n",
    "\n",
    "Congratulations! You've successfully completed the OPTIMIZE and VACUUM lab.\n",
    "\n",
    "### **What You Accomplished:**\n",
    "\n",
    "‚úÖ Created a table with small files (simulated the problem)  \n",
    "‚úÖ Used DESCRIBE DETAIL to diagnose issues  \n",
    "‚úÖ Ran OPTIMIZE to compact files  \n",
    "‚úÖ Used ZORDER to cluster data  \n",
    "‚úÖ Ran VACUUM DRY RUN to preview deletions  \n",
    "‚úÖ Ran VACUUM to free storage  \n",
    "‚úÖ Monitored table health with DESCRIBE HISTORY  \n",
    "‚úÖ Verified data integrity  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Small files hurt performance** - Many small files slow down queries\n",
    "2. **OPTIMIZE is your friend** - Compact files regularly\n",
    "3. **ZORDER boosts queries** - Cluster by frequently filtered columns\n",
    "4. **VACUUM saves money** - Delete old files to reduce storage costs\n",
    "5. **Balance retention** - Time travel vs storage costs\n",
    "6. **Monitor regularly** - Use DESCRIBE DETAIL and DESCRIBE HISTORY\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Checklist:**\n",
    "\n",
    "‚òê Schedule OPTIMIZE (daily/weekly)  \n",
    "‚òê Use ZORDER on key columns  \n",
    "‚òê Schedule VACUUM (weekly/monthly)  \n",
    "‚òê Set appropriate retention (7-30 days)  \n",
    "‚òê Monitor table health  \n",
    "‚òê Track storage costs  \n",
    "‚òê Document maintenance procedures  \n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "* Apply OPTIMIZE/VACUUM to your production tables\n",
    "* Set up scheduled jobs for maintenance\n",
    "* Monitor query performance improvements\n",
    "* Explore Delta Lake table properties\n",
    "* Learn about liquid clustering (alternative to ZORDER)\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources:**\n",
    "\n",
    "* [OPTIMIZE Documentation](https://docs.databricks.com/sql/language-manual/delta-optimize.html)\n",
    "* [VACUUM Documentation](https://docs.databricks.com/sql/language-manual/delta-vacuum.html)\n",
    "* [Delta Lake Best Practices](https://docs.databricks.com/delta/best-practices.html)\n",
    "* [File Management Guide](https://docs.databricks.com/delta/file-mgmt.html)\n",
    "\n",
    "---\n",
    "\n",
    "**You're now ready to maintain production Delta tables!** üöÄ\n",
    "\n",
    "*Happy optimizing!*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "OPTIMISE and VACUUM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
