{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8456849-17a6-4c19-8e68-6287f5bb461c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "# Lakeflow Jobs: Orchestrate Your Pipelines ğŸš€\n",
    "\n",
    "Welcome to the **Lakeflow Jobs** demo! This notebook teaches you how to create, configure, and manage jobs to orchestrate your Lakeflow pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## What you'll learn:\n",
    "\n",
    "* ğŸ“š **Part 1:** Understanding Lakeflow Jobs concepts\n",
    "* ğŸ”§ **Part 2:** Creating jobs programmatically with the Jobs API\n",
    "* âš™ï¸ **Part 3:** Job configuration and parameters\n",
    "* â° **Part 4:** Scheduling and triggers\n",
    "* ğŸ“Š **Part 5:** Monitoring and notifications\n",
    "* âœ… **Part 6:** Best practices and patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "* Complete [Lakeflow Pipeline Fundamentals](#notebook/2846436383063456)\n",
    "* Complete [Lakeflow Expectations](#notebook/2846436383063443)\n",
    "* Complete [Lakeflow Auto CDC](#notebook/2846436383063462)\n",
    "* Basic understanding of Delta Live Tables\n",
    "* Familiarity with Python\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** ğŸ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06436c6a-30aa-48f7-b692-0930173080d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 1: Understanding Lakeflow Jobs ğŸ“š\n",
    "\n",
    "Before creating jobs, let's understand what they are and why they matter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34852dfc-178a-4285-a5d3-b0ac6c9c67a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¯ What are Lakeflow Jobs?\n",
    "\n",
    "**Definition:**\n",
    "* Jobs are **orchestration units** that run your Lakeflow pipelines\n",
    "* Automate pipeline execution on a schedule or trigger\n",
    "* Manage compute resources and configurations\n",
    "* Monitor execution and send notifications\n",
    "* Part of Databricks Workflows\n",
    "\n",
    "---\n",
    "\n",
    "### **The Problem Without Jobs:**\n",
    "\n",
    "**Manual pipeline execution:**\n",
    "* âŒ Must manually click \"Start\" in the UI\n",
    "* âŒ No automation or scheduling\n",
    "* âŒ Hard to coordinate multiple pipelines\n",
    "* âŒ No centralized monitoring\n",
    "* âŒ Difficult to manage in production\n",
    "\n",
    "---\n",
    "\n",
    "### **The Solution With Jobs:**\n",
    "\n",
    "**Automated orchestration:**\n",
    "* âœ… Schedule pipelines to run automatically\n",
    "* âœ… Trigger on events (file arrival, table updates)\n",
    "* âœ… Chain multiple tasks together\n",
    "* âœ… Centralized monitoring and alerting\n",
    "* âœ… Production-ready automation\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Benefits:**\n",
    "\n",
    "* ğŸ”„ **Automation** - Set it and forget it\n",
    "* â° **Scheduling** - Run on cron schedules\n",
    "* ğŸ”— **Orchestration** - Chain multiple tasks\n",
    "* ğŸ“Š **Monitoring** - Track runs and failures\n",
    "* ğŸ”” **Notifications** - Alert on success/failure\n",
    "* ğŸ›ï¸ **Control** - Manage compute and configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0cbb8c7-ebbb-4471-9386-fc4974602731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”„ Jobs vs Pipelines: What's the Difference?\n",
    "\n",
    "**Understanding the relationship:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Lakeflow Pipeline (DLT Pipeline):**\n",
    "\n",
    "**What it is:**\n",
    "* The **data transformation logic**\n",
    "* Defines tables, views, and CDC flows\n",
    "* Written in notebooks using `@dp.table`, `@dp.view`, etc.\n",
    "* Processes data from source to target\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "@dp.table\n",
    "def bronze_customers():\n",
    "    return spark.readStream.table(\"raw.customers\")\n",
    "\n",
    "@dp.table\n",
    "def silver_customers():\n",
    "    return spark.readStream.table(\"bronze_customers\").filter(...)\n",
    "```\n",
    "\n",
    "**Think of it as:** The \"what\" - what data transformations to perform\n",
    "\n",
    "---\n",
    "\n",
    "### **Lakeflow Job:**\n",
    "\n",
    "**What it is:**\n",
    "* The **orchestration wrapper** around pipelines\n",
    "* Defines **when** and **how** to run pipelines\n",
    "* Manages compute, scheduling, and monitoring\n",
    "* Can run multiple tasks (pipelines, notebooks, etc.)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "job = {\n",
    "    \"name\": \"customer_pipeline_job\",\n",
    "    \"tasks\": [{\n",
    "        \"task_key\": \"run_customer_pipeline\",\n",
    "        \"pipeline_task\": {\n",
    "            \"pipeline_id\": \"abc-123-def\"\n",
    "        }\n",
    "    }],\n",
    "    \"schedule\": {\"quartz_cron_expression\": \"0 0 * * * ?\"}\n",
    "}\n",
    "```\n",
    "\n",
    "**Think of it as:** The \"when\" and \"how\" - when to run and how to configure\n",
    "\n",
    "---\n",
    "\n",
    "### **Relationship:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Lakeflow Job                â”‚\n",
    "â”‚  (Orchestration & Scheduling)       â”‚\n",
    "â”‚                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚   Task 1: DLT Pipeline        â”‚ â”‚\n",
    "â”‚  â”‚   (Data Transformations)      â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚   Task 2: Notebook            â”‚ â”‚\n",
    "â”‚  â”‚   (Post-processing)           â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key point:** Jobs **run** pipelines, pipelines **transform** data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ffc500-af6c-4711-88dd-5af002630933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ§© Job Components\n",
    "\n",
    "A Lakeflow Job consists of several key components:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Tasks**\n",
    "\n",
    "**What they are:**\n",
    "* Individual units of work within a job\n",
    "* Can be DLT pipelines, notebooks, Python scripts, etc.\n",
    "* Execute in sequence or parallel\n",
    "\n",
    "**Types of tasks:**\n",
    "* **Pipeline Task** - Run a DLT pipeline\n",
    "* **Notebook Task** - Run a notebook\n",
    "* **Python Task** - Run Python code\n",
    "* **SQL Task** - Run SQL queries\n",
    "* **dbt Task** - Run dbt models\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "\"tasks\": [\n",
    "    {\n",
    "        \"task_key\": \"ingest_data\",\n",
    "        \"pipeline_task\": {\"pipeline_id\": \"pipeline-123\"}\n",
    "    },\n",
    "    {\n",
    "        \"task_key\": \"send_report\",\n",
    "        \"notebook_task\": {\"notebook_path\": \"/Reports/daily_summary\"}\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Schedule**\n",
    "\n",
    "**What it is:**\n",
    "* Defines when the job runs automatically\n",
    "* Uses cron expressions or simple intervals\n",
    "\n",
    "**Examples:**\n",
    "```python\n",
    "# Run daily at midnight\n",
    "\"schedule\": {\"quartz_cron_expression\": \"0 0 0 * * ?\"}\n",
    "\n",
    "# Run every hour\n",
    "\"schedule\": {\"quartz_cron_expression\": \"0 0 * * * ?\"}\n",
    "\n",
    "# Run every Monday at 9 AM\n",
    "\"schedule\": {\"quartz_cron_expression\": \"0 0 9 ? * MON\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Compute Configuration**\n",
    "\n",
    "**What it is:**\n",
    "* Defines the cluster/compute for task execution\n",
    "* Can use existing clusters or create new ones\n",
    "\n",
    "**Options:**\n",
    "* **Existing cluster** - Use a running cluster\n",
    "* **New cluster** - Create cluster per run\n",
    "* **Job cluster** - Shared cluster for all tasks\n",
    "* **Serverless** - Use serverless compute (recommended)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Parameters**\n",
    "\n",
    "**What they are:**\n",
    "* Dynamic values passed to tasks at runtime\n",
    "* Allow reusable jobs with different inputs\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "\"parameters\": {\n",
    "    \"start_date\": \"2026-01-01\",\n",
    "    \"environment\": \"production\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Notifications**\n",
    "\n",
    "**What they are:**\n",
    "* Alerts sent on job success, failure, or start\n",
    "* Can notify via email, Slack, PagerDuty, etc.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "\"email_notifications\": {\n",
    "    \"on_failure\": [\"team@company.com\"],\n",
    "    \"on_success\": [\"manager@company.com\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1995012-76f3-4791-95aa-6d606eddc96f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 2: Creating Jobs Programmatically ğŸ”§\n",
    "\n",
    "Let's learn how to create and manage jobs using the Databricks Jobs API.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fdad095-eae9-45ab-be1b-ed58b1fcd4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”Œ Databricks Jobs API\n",
    "\n",
    "**What is it?**\n",
    "* REST API for managing Databricks jobs\n",
    "* Create, update, delete, and run jobs programmatically\n",
    "* Accessible via Python SDK or direct HTTP calls\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use the API?**\n",
    "\n",
    "**Benefits:**\n",
    "* âœ… **Infrastructure as Code** - Version control your jobs\n",
    "* âœ… **Automation** - Create jobs from CI/CD pipelines\n",
    "* âœ… **Consistency** - Standardize job configurations\n",
    "* âœ… **Scalability** - Manage hundreds of jobs programmatically\n",
    "* âœ… **Flexibility** - Dynamic job creation based on logic\n",
    "\n",
    "---\n",
    "\n",
    "### **Two Ways to Use the API:**\n",
    "\n",
    "**1. Databricks SDK (Recommended):**\n",
    "```python\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "job = w.jobs.create(\n",
    "    name=\"my_pipeline_job\",\n",
    "    tasks=[...]\n",
    ")\n",
    "```\n",
    "\n",
    "**2. Direct REST API:**\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{workspace_url}/api/2.1/jobs/create\",\n",
    "    headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "    json={\"name\": \"my_pipeline_job\", \"tasks\": [...]}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**In this demo, we'll use the Databricks SDK for simplicity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd3a9d04-38ac-4cfd-a835-1d67e4adccac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup - Install SDK"
    }
   },
   "outputs": [],
   "source": [
    "# Install Databricks SDK\n",
    "%pip install databricks-sdk --quiet\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026730ab-af2a-4e4a-a91d-f8b74b4fdbc1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup - Initialize Client"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import jobs\n",
    "import json\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "# This automatically uses the notebook's authentication\n",
    "w = WorkspaceClient()\n",
    "\n",
    "print(\"âœ… Databricks SDK initialized successfully!\")\n",
    "print(f\"ğŸ“ Workspace: {w.config.host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae1e9d08-c648-427f-a279-5c9a1e703a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¯ Creating Your First Job\n",
    "\n",
    "Let's create a simple job that runs a DLT pipeline.\n",
    "\n",
    "**Steps:**\n",
    "1. Define the job configuration\n",
    "2. Create the job using the API\n",
    "3. Verify the job was created\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ebf51ae-7b48-414d-b477-d0a147906fc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example - Simple Pipeline Job"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 1: Simple Pipeline Job\n",
    "# ============================================\n",
    "\n",
    "# First, let's find an existing pipeline to use\n",
    "# (In practice, you'd use your own pipeline ID)\n",
    "\n",
    "try:\n",
    "    # List existing pipelines\n",
    "    pipelines = w.pipelines.list_pipelines()\n",
    "    pipeline_list = list(pipelines)\n",
    "    \n",
    "    if pipeline_list:\n",
    "        # Use the first pipeline as an example\n",
    "        example_pipeline = pipeline_list[0]\n",
    "        pipeline_id = example_pipeline.pipeline_id\n",
    "        pipeline_name = example_pipeline.name\n",
    "        \n",
    "        print(f\"ğŸ“‹ Found pipeline: {pipeline_name}\")\n",
    "        print(f\"ğŸ†” Pipeline ID: {pipeline_id}\")\n",
    "        \n",
    "        # Create a simple job to run this pipeline\n",
    "        job = w.jobs.create(\n",
    "            name=f\"Demo: {pipeline_name} - Daily Run\",\n",
    "            tasks=[\n",
    "                jobs.Task(\n",
    "                    task_key=\"run_pipeline\",\n",
    "                    description=\"Run the DLT pipeline\",\n",
    "                    pipeline_task=jobs.PipelineTask(\n",
    "                        pipeline_id=pipeline_id,\n",
    "                        full_refresh=False  # Incremental run\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            # Optional: Add tags for organization\n",
    "            tags={\n",
    "                \"environment\": \"demo\",\n",
    "                \"created_by\": \"lakeflow_jobs_demo\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Job created successfully!\")\n",
    "        print(f\"ğŸ†” Job ID: {job.job_id}\")\n",
    "        print(f\"ğŸ”— View job: {w.config.host}/#job/{job.job_id}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No pipelines found. Please create a pipeline first.\")\n",
    "        print(\"ğŸ’¡ Tip: Complete the Lakeflow Pipeline Fundamentals demo first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure you have permissions to create jobs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a67074a-e5b6-4e76-b350-17ffe006d543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ’¡ Understanding the Job Creation Code\n",
    "\n",
    "**What we did:**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Initialized the Workspace Client:**\n",
    "```python\n",
    "w = WorkspaceClient()\n",
    "```\n",
    "* Connects to your Databricks workspace\n",
    "* Uses notebook authentication automatically\n",
    "* Provides access to all Databricks APIs\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Created a Job:**\n",
    "```python\n",
    "job = w.jobs.create(\n",
    "    name=\"Demo: Pipeline - Daily Run\",\n",
    "    tasks=[...]\n",
    ")\n",
    "```\n",
    "* `name` - Human-readable job name\n",
    "* `tasks` - List of tasks to execute\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Defined a Pipeline Task:**\n",
    "```python\n",
    "jobs.Task(\n",
    "    task_key=\"run_pipeline\",              # Unique identifier\n",
    "    description=\"Run the DLT pipeline\",   # Human-readable description\n",
    "    pipeline_task=jobs.PipelineTask(\n",
    "        pipeline_id=pipeline_id,          # Which pipeline to run\n",
    "        full_refresh=False                # Incremental vs full refresh\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**Key parameters:**\n",
    "* `task_key` - Unique identifier for this task (used for dependencies)\n",
    "* `pipeline_task` - Specifies this is a DLT pipeline task\n",
    "* `pipeline_id` - The ID of the pipeline to run\n",
    "* `full_refresh` - `False` for incremental, `True` to reprocess all data\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Added Tags:**\n",
    "```python\n",
    "tags={\n",
    "    \"environment\": \"demo\",\n",
    "    \"created_by\": \"lakeflow_jobs_demo\"\n",
    "}\n",
    "```\n",
    "* Helps organize and filter jobs\n",
    "* Useful for cost tracking and governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd3095db-b9ea-436d-9e7d-286683d5d0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 3: Job Configuration and Parameters âš™ï¸\n",
    "\n",
    "Let's explore advanced job configurations including compute, parameters, and task dependencies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7cf6963-c73d-43b8-8883-a2a25aeb9391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ’» Compute Configuration Options\n",
    "\n",
    "**Jobs can use different compute strategies:**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Serverless Compute (Recommended)**\n",
    "\n",
    "**What it is:**\n",
    "* Managed compute that scales automatically\n",
    "* No cluster management required\n",
    "* Fast startup times\n",
    "* Pay only for what you use\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Most production workloads\n",
    "* âœ… When you want simplicity\n",
    "* âœ… Variable workload sizes\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "# For pipeline tasks, serverless is automatic\n",
    "# No additional configuration needed!\n",
    "pipeline_task=jobs.PipelineTask(\n",
    "    pipeline_id=pipeline_id\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Job Cluster**\n",
    "\n",
    "**What it is:**\n",
    "* Dedicated cluster created for the job\n",
    "* Terminated after job completes\n",
    "* Full control over cluster configuration\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Need specific cluster configurations\n",
    "* âœ… Custom libraries or init scripts\n",
    "* âœ… Specific instance types\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "job_cluster_key=\"my_cluster\",\n",
    "job_clusters=[\n",
    "    jobs.JobCluster(\n",
    "        job_cluster_key=\"my_cluster\",\n",
    "        new_cluster=jobs.ClusterSpec(\n",
    "            spark_version=\"14.3.x-scala2.12\",\n",
    "            node_type_id=\"i3.xlarge\",\n",
    "            num_workers=2,\n",
    "            spark_conf={\n",
    "                \"spark.databricks.delta.preview.enabled\": \"true\"\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Existing Cluster**\n",
    "\n",
    "**What it is:**\n",
    "* Use an already-running cluster\n",
    "* Cluster must be running when job starts\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Development and testing\n",
    "* âœ… Interactive debugging\n",
    "* âŒ Not recommended for production\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "existing_cluster_id=\"0123-456789-abcdef\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9832f86d-ea0c-4104-871a-245287048caa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Job with Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 2: Job with Parameters\n",
    "# ============================================\n",
    "\n",
    "# Create a job that accepts parameters\n",
    "# Parameters allow dynamic configuration at runtime\n",
    "\n",
    "try:\n",
    "    # Get a pipeline to use\n",
    "    pipelines = w.pipelines.list_pipelines()\n",
    "    pipeline_list = list(pipelines)\n",
    "    \n",
    "    if pipeline_list:\n",
    "        pipeline_id = pipeline_list[0].pipeline_id\n",
    "        \n",
    "        # Create job with parameters\n",
    "        job_with_params = w.jobs.create(\n",
    "            name=\"Demo: Parameterized Pipeline Job\",\n",
    "            tasks=[\n",
    "                jobs.Task(\n",
    "                    task_key=\"run_pipeline_with_params\",\n",
    "                    description=\"Run pipeline with custom parameters\",\n",
    "                    pipeline_task=jobs.PipelineTask(\n",
    "                        pipeline_id=pipeline_id,\n",
    "                        full_refresh=False\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            # Define default parameters\n",
    "            parameters=[\n",
    "                jobs.JobParameterDefinition(\n",
    "                    name=\"environment\",\n",
    "                    default=\"development\"\n",
    "                ),\n",
    "                jobs.JobParameterDefinition(\n",
    "                    name=\"start_date\",\n",
    "                    default=\"2026-01-01\"\n",
    "                ),\n",
    "                jobs.JobParameterDefinition(\n",
    "                    name=\"end_date\",\n",
    "                    default=\"2026-01-31\"\n",
    "                )\n",
    "            ],\n",
    "            tags={\n",
    "                \"environment\": \"demo\",\n",
    "                \"type\": \"parameterized\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Parameterized job created!\")\n",
    "        print(f\"ğŸ†” Job ID: {job_with_params.job_id}\")\n",
    "        print(f\"\\nğŸ“‹ Parameters:\")\n",
    "        for param in job_with_params.settings.parameters:\n",
    "            print(f\"   â€¢ {param.name}: {param.default}\")\n",
    "        print(f\"\\nğŸ”— View job: {w.config.host}/#job/{job_with_params.job_id}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No pipelines found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32fcd2c8-3820-49cd-97fa-fb2308a82bdf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multi-Task Job"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 3: Multi-Task Job with Dependencies\n",
    "# ============================================\n",
    "\n",
    "# Create a job with multiple tasks that run in sequence\n",
    "\n",
    "try:\n",
    "    pipelines = w.pipelines.list_pipelines()\n",
    "    pipeline_list = list(pipelines)\n",
    "    \n",
    "    if pipeline_list:\n",
    "        pipeline_id = pipeline_list[0].pipeline_id\n",
    "        \n",
    "        # Create a multi-task job\n",
    "        multi_task_job = w.jobs.create(\n",
    "            name=\"Demo: Multi-Task Pipeline Job\",\n",
    "            tasks=[\n",
    "                # Task 1: Run the pipeline\n",
    "                jobs.Task(\n",
    "                    task_key=\"ingest_and_transform\",\n",
    "                    description=\"Run DLT pipeline to ingest and transform data\",\n",
    "                    pipeline_task=jobs.PipelineTask(\n",
    "                        pipeline_id=pipeline_id,\n",
    "                        full_refresh=False\n",
    "                    )\n",
    "                ),\n",
    "                # Task 2: Run a notebook (depends on Task 1)\n",
    "                jobs.Task(\n",
    "                    task_key=\"generate_report\",\n",
    "                    description=\"Generate summary report after pipeline completes\",\n",
    "                    depends_on=[\n",
    "                        jobs.TaskDependency(task_key=\"ingest_and_transform\")\n",
    "                    ],\n",
    "                    notebook_task=jobs.NotebookTask(\n",
    "                        notebook_path=f\"{dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()}\",\n",
    "                        base_parameters={\n",
    "                            \"report_type\": \"summary\",\n",
    "                            \"output_format\": \"html\"\n",
    "                        }\n",
    "                    ),\n",
    "                    # For notebook tasks, we need to specify compute\n",
    "                    new_cluster=jobs.ClusterSpec(\n",
    "                        spark_version=w.clusters.select_spark_version(latest=True),\n",
    "                        node_type_id=w.clusters.select_node_type(local_disk=True, min_memory_gb=8),\n",
    "                        num_workers=1\n",
    "                    )\n",
    "                ),\n",
    "                # Task 3: Send notification (depends on Task 2)\n",
    "                jobs.Task(\n",
    "                    task_key=\"send_notification\",\n",
    "                    description=\"Send completion notification\",\n",
    "                    depends_on=[\n",
    "                        jobs.TaskDependency(task_key=\"generate_report\")\n",
    "                    ],\n",
    "                    notebook_task=jobs.NotebookTask(\n",
    "                        notebook_path=f\"{dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()}\",\n",
    "                        base_parameters={\n",
    "                            \"action\": \"notify\",\n",
    "                            \"message\": \"Pipeline completed successfully\"\n",
    "                        }\n",
    "                    ),\n",
    "                    new_cluster=jobs.ClusterSpec(\n",
    "                        spark_version=w.clusters.select_spark_version(latest=True),\n",
    "                        node_type_id=w.clusters.select_node_type(local_disk=True, min_memory_gb=8),\n",
    "                        num_workers=1\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            tags={\n",
    "                \"environment\": \"demo\",\n",
    "                \"type\": \"multi_task\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Multi-task job created!\")\n",
    "        print(f\"ğŸ†” Job ID: {multi_task_job.job_id}\")\n",
    "        print(f\"\\nğŸ“‹ Task Flow:\")\n",
    "        print(f\"   1ï¸âƒ£ ingest_and_transform (DLT Pipeline)\")\n",
    "        print(f\"   2ï¸âƒ£ generate_report (Notebook) - depends on task 1\")\n",
    "        print(f\"   3ï¸âƒ£ send_notification (Notebook) - depends on task 2\")\n",
    "        print(f\"\\nğŸ”— View job: {w.config.host}/#job/{multi_task_job.job_id}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No pipelines found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dcafbc4-2564-475f-9fab-0b49c40fbc18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”— Task Dependencies\n",
    "\n",
    "**How task dependencies work:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Sequential Execution:**\n",
    "\n",
    "```python\n",
    "tasks=[\n",
    "    jobs.Task(\n",
    "        task_key=\"task_1\",\n",
    "        pipeline_task=...\n",
    "    ),\n",
    "    jobs.Task(\n",
    "        task_key=\"task_2\",\n",
    "        depends_on=[jobs.TaskDependency(task_key=\"task_1\")],\n",
    "        notebook_task=...\n",
    "    ),\n",
    "    jobs.Task(\n",
    "        task_key=\"task_3\",\n",
    "        depends_on=[jobs.TaskDependency(task_key=\"task_2\")],\n",
    "        notebook_task=...\n",
    "    )\n",
    "]\n",
    "```\n",
    "\n",
    "**Execution order:**\n",
    "```\n",
    "task_1 â†’ task_2 â†’ task_3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Parallel Execution:**\n",
    "\n",
    "```python\n",
    "tasks=[\n",
    "    jobs.Task(\n",
    "        task_key=\"task_1\",\n",
    "        pipeline_task=...\n",
    "    ),\n",
    "    jobs.Task(\n",
    "        task_key=\"task_2a\",\n",
    "        depends_on=[jobs.TaskDependency(task_key=\"task_1\")],\n",
    "        notebook_task=...\n",
    "    ),\n",
    "    jobs.Task(\n",
    "        task_key=\"task_2b\",\n",
    "        depends_on=[jobs.TaskDependency(task_key=\"task_1\")],\n",
    "        notebook_task=...\n",
    "    ),\n",
    "    jobs.Task(\n",
    "        task_key=\"task_3\",\n",
    "        depends_on=[\n",
    "            jobs.TaskDependency(task_key=\"task_2a\"),\n",
    "            jobs.TaskDependency(task_key=\"task_2b\")\n",
    "        ],\n",
    "        notebook_task=...\n",
    "    )\n",
    "]\n",
    "```\n",
    "\n",
    "**Execution order:**\n",
    "```\n",
    "        task_1\n",
    "       /      \\\n",
    "   task_2a  task_2b  (run in parallel)\n",
    "       \\      /\n",
    "        task_3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points:**\n",
    "\n",
    "* Tasks without dependencies run immediately\n",
    "* Tasks with dependencies wait for parent tasks to complete\n",
    "* Multiple tasks can depend on the same parent (parallel execution)\n",
    "* A task can depend on multiple parents (waits for all to complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a1db59-b040-4cca-9fde-45c1127811e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 4: Scheduling and Triggers â°\n",
    "\n",
    "Let's learn how to schedule jobs to run automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "417f0c19-0ffc-487c-acc3-86a4bca01e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ“… Job Scheduling Options\n",
    "\n",
    "**Databricks Jobs support multiple scheduling methods:**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Cron Schedule**\n",
    "\n",
    "**What it is:**\n",
    "* Time-based scheduling using cron expressions\n",
    "* Most common scheduling method\n",
    "* Supports complex schedules\n",
    "\n",
    "**Cron format (Quartz):**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ second (0-59)\n",
    "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0-59)\n",
    "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0-23)\n",
    "â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of month (1-31)\n",
    "â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ month (1-12 or JAN-DEC)\n",
    "â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of week (0-6 or SUN-SAT)\n",
    "â”‚ â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "* * * * * *\n",
    "```\n",
    "\n",
    "**Common examples:**\n",
    "```python\n",
    "# Every day at midnight\n",
    "\"0 0 0 * * ?\"\n",
    "\n",
    "# Every hour\n",
    "\"0 0 * * * ?\"\n",
    "\n",
    "# Every 15 minutes\n",
    "\"0 */15 * * * ?\"\n",
    "\n",
    "# Every Monday at 9 AM\n",
    "\"0 0 9 ? * MON\"\n",
    "\n",
    "# Every weekday at 6 AM\n",
    "\"0 0 6 ? * MON-FRI\"\n",
    "\n",
    "# First day of every month at midnight\n",
    "\"0 0 0 1 * ?\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Continuous (Always Running)**\n",
    "\n",
    "**What it is:**\n",
    "* Job runs continuously\n",
    "* Restarts automatically after completion\n",
    "* Useful for streaming pipelines\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Real-time data processing\n",
    "* âœ… Continuous streaming pipelines\n",
    "* âŒ Not for batch processing\n",
    "\n",
    "---\n",
    "\n",
    "### **3. File Arrival Trigger**\n",
    "\n",
    "**What it is:**\n",
    "* Job runs when new files arrive in a location\n",
    "* Monitors cloud storage paths\n",
    "* Event-driven processing\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Process files as they arrive\n",
    "* âœ… Event-driven workflows\n",
    "* âœ… Unpredictable data arrival\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Manual (On-Demand)**\n",
    "\n",
    "**What it is:**\n",
    "* No automatic schedule\n",
    "* Run manually via UI or API\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Ad-hoc processing\n",
    "* âœ… Testing and development\n",
    "* âœ… Triggered by external systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee5257f0-33a9-414d-964f-998787800a9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scheduled Job Example"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 4: Scheduled Job (Daily at Midnight)\n",
    "# ============================================\n",
    "\n",
    "try:\n",
    "    pipelines = w.pipelines.list_pipelines()\n",
    "    pipeline_list = list(pipelines)\n",
    "    \n",
    "    if pipeline_list:\n",
    "        pipeline_id = pipeline_list[0].pipeline_id\n",
    "        \n",
    "        # Create a scheduled job\n",
    "        scheduled_job = w.jobs.create(\n",
    "            name=\"Demo: Daily Pipeline Job (Midnight)\",\n",
    "            tasks=[\n",
    "                jobs.Task(\n",
    "                    task_key=\"daily_pipeline_run\",\n",
    "                    description=\"Run pipeline daily at midnight\",\n",
    "                    pipeline_task=jobs.PipelineTask(\n",
    "                        pipeline_id=pipeline_id,\n",
    "                        full_refresh=False\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            # Schedule: Every day at midnight (UTC)\n",
    "            schedule=jobs.CronSchedule(\n",
    "                quartz_cron_expression=\"0 0 0 * * ?\",\n",
    "                timezone_id=\"UTC\",\n",
    "                pause_status=jobs.PauseStatus.UNPAUSED\n",
    "            ),\n",
    "            tags={\n",
    "                \"environment\": \"demo\",\n",
    "                \"schedule\": \"daily\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Scheduled job created!\")\n",
    "        print(f\"ğŸ†” Job ID: {scheduled_job.job_id}\")\n",
    "        print(f\"â° Schedule: Daily at midnight (UTC)\")\n",
    "        print(f\"ğŸ“… Cron: {scheduled_job.settings.schedule.quartz_cron_expression}\")\n",
    "        print(f\"ğŸ”— View job: {w.config.host}/#job/{scheduled_job.job_id}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No pipelines found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70092209-ee3e-479c-bcf9-9c5b3ca2ff57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "File Arrival Trigger Example"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 5: File Arrival Trigger\n",
    "# ============================================\n",
    "\n",
    "# Note: File arrival triggers require specific permissions\n",
    "# and cloud storage configuration\n",
    "\n",
    "try:\n",
    "    pipelines = w.pipelines.list_pipelines()\n",
    "    pipeline_list = list(pipelines)\n",
    "    \n",
    "    if pipeline_list:\n",
    "        pipeline_id = pipeline_list[0].pipeline_id\n",
    "        \n",
    "        # Create a file-triggered job\n",
    "        file_trigger_job = w.jobs.create(\n",
    "            name=\"Demo: File Arrival Triggered Job\",\n",
    "            tasks=[\n",
    "                jobs.Task(\n",
    "                    task_key=\"process_new_files\",\n",
    "                    description=\"Process pipeline when new files arrive\",\n",
    "                    pipeline_task=jobs.PipelineTask(\n",
    "                        pipeline_id=pipeline_id,\n",
    "                        full_refresh=False\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            # Trigger on file arrival\n",
    "            trigger=jobs.TriggerSettings(\n",
    "                file_arrival=jobs.FileArrivalTriggerConfiguration(\n",
    "                    url=\"s3://my-bucket/incoming-data/\",  # Change to your path\n",
    "                    min_time_between_triggers_seconds=60,  # Wait 60s between triggers\n",
    "                    wait_after_last_change_seconds=30      # Wait 30s after last file\n",
    "                )\n",
    "            ),\n",
    "            tags={\n",
    "                \"environment\": \"demo\",\n",
    "                \"trigger\": \"file_arrival\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… File-triggered job created!\")\n",
    "        print(f\"ğŸ†” Job ID: {file_trigger_job.job_id}\")\n",
    "        print(f\"ğŸ“ Monitoring: s3://my-bucket/incoming-data/\")\n",
    "        print(f\"â±ï¸ Min time between triggers: 60 seconds\")\n",
    "        print(f\"ğŸ”— View job: {w.config.host}/#job/{file_trigger_job.job_id}\")\n",
    "        print(f\"\\nğŸ’¡ Note: Update the S3 path to your actual data location\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No pipelines found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(f\"ğŸ’¡ File arrival triggers require cloud storage permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350d5efe-2a12-4fc4-a32a-b63e8775ad26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¯ Challenge 1: Create a Scheduled Job\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Create a job that runs a pipeline every Monday at 9 AM.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Job name:** \"Challenge 1: Weekly Pipeline Job\"\n",
    "2. **Task:**\n",
    "   * Task key: \"weekly_pipeline_run\"\n",
    "   * Use any existing pipeline\n",
    "   * Incremental refresh (not full refresh)\n",
    "3. **Schedule:**\n",
    "   * Every Monday at 9:00 AM UTC\n",
    "   * Cron expression: `\"0 0 9 ? * MON\"`\n",
    "   * Timezone: \"UTC\"\n",
    "   * Status: UNPAUSED\n",
    "4. **Tags:**\n",
    "   * environment: \"challenge\"\n",
    "   * schedule: \"weekly\"\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code below:** ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9f96d73-15df-45fa-b802-00134dcada68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Challenge 1 Solution Space"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHALLENGE 1: Your solution here\n",
    "# ============================================\n",
    "\n",
    "# TODO: Create a job that runs every Monday at 9 AM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8df5f082-9eed-445e-b037-f0e331f29373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 5: Monitoring and Notifications ğŸ“Š\n",
    "\n",
    "Let's learn how to monitor job execution and set up notifications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b49fc7-e935-40cc-9ab2-78e69e1dfeb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ“Š Monitoring Job Execution\n",
    "\n",
    "**Ways to monitor jobs:**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Databricks UI**\n",
    "\n",
    "**Workflows Page:**\n",
    "* View all jobs and their status\n",
    "* See run history and duration\n",
    "* Access logs and error messages\n",
    "* Monitor active runs\n",
    "\n",
    "**Navigation:**\n",
    "```\n",
    "Workflows â†’ Jobs â†’ [Your Job] â†’ Runs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Jobs API**\n",
    "\n",
    "**List job runs:**\n",
    "```python\n",
    "# Get recent runs for a job\n",
    "runs = w.jobs.list_runs(job_id=job_id, limit=10)\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"Run {run.run_id}: {run.state.life_cycle_state}\")\n",
    "```\n",
    "\n",
    "**Get run details:**\n",
    "```python\n",
    "# Get details of a specific run\n",
    "run = w.jobs.get_run(run_id=run_id)\n",
    "\n",
    "print(f\"Status: {run.state.life_cycle_state}\")\n",
    "print(f\"Result: {run.state.result_state}\")\n",
    "print(f\"Start: {run.start_time}\")\n",
    "print(f\"End: {run.end_time}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Run States**\n",
    "\n",
    "**Life cycle states:**\n",
    "* `PENDING` - Waiting to start\n",
    "* `RUNNING` - Currently executing\n",
    "* `TERMINATING` - Shutting down\n",
    "* `TERMINATED` - Completed\n",
    "* `SKIPPED` - Skipped due to conditions\n",
    "* `INTERNAL_ERROR` - System error\n",
    "\n",
    "**Result states (for terminated runs):**\n",
    "* `SUCCESS` - Completed successfully\n",
    "* `FAILED` - Failed with errors\n",
    "* `TIMEDOUT` - Exceeded timeout\n",
    "* `CANCELED` - Manually canceled\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Metrics to Monitor**\n",
    "\n",
    "**Key metrics:**\n",
    "* âœ… Success rate\n",
    "* âœ… Run duration\n",
    "* âœ… Failure frequency\n",
    "* âœ… Queue time\n",
    "* âœ… Cost per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e4d6677-9ac8-4d81-92bb-4e3eec8f25e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List and Monitor Jobs"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 6: List and Monitor Jobs\n",
    "# ============================================\n",
    "\n",
    "try:\n",
    "    # List all jobs (limit to 10 for demo)\n",
    "    all_jobs = w.jobs.list(limit=10)\n",
    "    \n",
    "    print(\"ğŸ“‹ Your Jobs:\\n\")\n",
    "    \n",
    "    for job in all_jobs:\n",
    "        print(f\"ğŸ†” Job ID: {job.job_id}\")\n",
    "        print(f\"ğŸ“ Name: {job.settings.name}\")\n",
    "        \n",
    "        # Get recent runs for this job\n",
    "        runs = list(w.jobs.list_runs(job_id=job.job_id, limit=3))\n",
    "        \n",
    "        if runs:\n",
    "            print(f\"ğŸ“Š Recent Runs:\")\n",
    "            for run in runs:\n",
    "                status = run.state.life_cycle_state\n",
    "                result = run.state.result_state if run.state.result_state else \"N/A\"\n",
    "                print(f\"   â€¢ Run {run.run_id}: {status} - {result}\")\n",
    "        else:\n",
    "            print(f\"ğŸ“Š No runs yet\")\n",
    "        \n",
    "        print(f\"ğŸ”— View: {w.config.host}/#job/{job.job_id}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a9e774-57c0-4475-a231-a2db400dfb86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”” Notification Configuration\n",
    "\n",
    "**Set up alerts for job events:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Email Notifications**\n",
    "\n",
    "**Basic email alerts:**\n",
    "```python\n",
    "email_notifications=jobs.JobEmailNotifications(\n",
    "    on_start=[\"team@company.com\"],\n",
    "    on_success=[\"manager@company.com\"],\n",
    "    on_failure=[\"oncall@company.com\", \"team@company.com\"],\n",
    "    no_alert_for_skipped_runs=True\n",
    ")\n",
    "```\n",
    "\n",
    "**When to notify:**\n",
    "* `on_start` - Job starts running\n",
    "* `on_success` - Job completes successfully\n",
    "* `on_failure` - Job fails\n",
    "* `on_duration_warning_threshold_exceeded` - Job runs longer than expected\n",
    "\n",
    "---\n",
    "\n",
    "### **Webhook Notifications**\n",
    "\n",
    "**Integrate with external systems:**\n",
    "```python\n",
    "webhook_notifications=jobs.WebhookNotifications(\n",
    "    on_start=[jobs.Webhook(id=\"webhook-123\")],\n",
    "    on_success=[jobs.Webhook(id=\"webhook-456\")],\n",
    "    on_failure=[jobs.Webhook(id=\"webhook-789\")]\n",
    ")\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "* Slack notifications\n",
    "* PagerDuty alerts\n",
    "* Custom monitoring systems\n",
    "* Ticketing systems (Jira, ServiceNow)\n",
    "\n",
    "---\n",
    "\n",
    "### **Notification Best Practices**\n",
    "\n",
    "**Do:**\n",
    "* âœ… Always notify on failures\n",
    "* âœ… Use different recipients for different events\n",
    "* âœ… Set up escalation for critical jobs\n",
    "* âœ… Include job context in notifications\n",
    "\n",
    "**Don't:**\n",
    "* âŒ Spam with success notifications for every job\n",
    "* âŒ Use personal emails for production alerts\n",
    "* âŒ Ignore notification fatigue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f848aca-bded-432f-acc0-3762b7e1e75b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Job with Notifications"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 7: Job with Email Notifications\n",
    "# ============================================\n",
    "\n",
    "try:\n",
    "    pipelines = w.pipelines.list_pipelines()\n",
    "    pipeline_list = list(pipelines)\n",
    "    \n",
    "    if pipeline_list:\n",
    "        pipeline_id = pipeline_list[0].pipeline_id\n",
    "        \n",
    "        # Get current user email\n",
    "        current_user = w.current_user.me()\n",
    "        user_email = current_user.user_name\n",
    "        \n",
    "        # Create job with notifications\n",
    "        notified_job = w.jobs.create(\n",
    "            name=\"Demo: Job with Notifications\",\n",
    "            tasks=[\n",
    "                jobs.Task(\n",
    "                    task_key=\"pipeline_with_alerts\",\n",
    "                    description=\"Pipeline with email notifications\",\n",
    "                    pipeline_task=jobs.PipelineTask(\n",
    "                        pipeline_id=pipeline_id,\n",
    "                        full_refresh=False\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            # Email notifications\n",
    "            email_notifications=jobs.JobEmailNotifications(\n",
    "                on_failure=[user_email],  # Notify on failure\n",
    "                on_success=[user_email],  # Notify on success\n",
    "                no_alert_for_skipped_runs=True\n",
    "            ),\n",
    "            # Timeout settings\n",
    "            timeout_seconds=3600,  # 1 hour timeout\n",
    "            tags={\n",
    "                \"environment\": \"demo\",\n",
    "                \"notifications\": \"enabled\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Job with notifications created!\")\n",
    "        print(f\"ğŸ†” Job ID: {notified_job.job_id}\")\n",
    "        print(f\"ğŸ“§ Notifications sent to: {user_email}\")\n",
    "        print(f\"â±ï¸ Timeout: 1 hour\")\n",
    "        print(f\"ğŸ”— View job: {w.config.host}/#job/{notified_job.job_id}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No pipelines found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6a61fe7-0c2b-4633-b20c-021b13f37cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### â–¶ï¸ Running Jobs Programmatically\n",
    "\n",
    "**Trigger job runs via API:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Run Now (Manual Trigger)**\n",
    "\n",
    "**Start a job immediately:**\n",
    "```python\n",
    "# Run a job now\n",
    "run = w.jobs.run_now(job_id=job_id)\n",
    "\n",
    "print(f\"Started run: {run.run_id}\")\n",
    "```\n",
    "\n",
    "**Run with custom parameters:**\n",
    "```python\n",
    "# Run with different parameters\n",
    "run = w.jobs.run_now(\n",
    "    job_id=job_id,\n",
    "    job_parameters={\n",
    "        \"environment\": \"production\",\n",
    "        \"start_date\": \"2026-01-26\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Wait for Completion**\n",
    "\n",
    "**Block until run completes:**\n",
    "```python\n",
    "# Start run and wait for completion\n",
    "run = w.jobs.run_now(job_id=job_id)\n",
    "\n",
    "# Wait for run to complete (blocks)\n",
    "result = w.jobs.wait_get_run_job_terminated_or_skipped(run_id=run.run_id)\n",
    "\n",
    "if result.state.result_state == jobs.RunResultState.SUCCESS:\n",
    "    print(\"âœ… Job completed successfully!\")\n",
    "else:\n",
    "    print(f\"âŒ Job failed: {result.state.state_message}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cancel a Run**\n",
    "\n",
    "**Stop a running job:**\n",
    "```python\n",
    "# Cancel a specific run\n",
    "w.jobs.cancel_run(run_id=run_id)\n",
    "\n",
    "print(f\"Canceled run: {run_id}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d5d940-0462-4124-ae08-b7d308665711",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Job Example"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 8: Run a Job and Monitor\n",
    "# ============================================\n",
    "\n",
    "try:\n",
    "    # Get the first demo job we created\n",
    "    all_jobs = list(w.jobs.list(limit=5))\n",
    "    \n",
    "    if all_jobs:\n",
    "        # Find a job created by this demo\n",
    "        demo_job = None\n",
    "        for job in all_jobs:\n",
    "            if \"Demo:\" in job.settings.name:\n",
    "                demo_job = job\n",
    "                break\n",
    "        \n",
    "        if demo_job:\n",
    "            print(f\"ğŸ¯ Running job: {demo_job.settings.name}\")\n",
    "            print(f\"ğŸ†” Job ID: {demo_job.job_id}\")\n",
    "            \n",
    "            # Start the job\n",
    "            run = w.jobs.run_now(job_id=demo_job.job_id)\n",
    "            \n",
    "            print(f\"\\nâ–¶ï¸ Started run: {run.run_id}\")\n",
    "            print(f\"ğŸ”— View run: {w.config.host}/#job/{demo_job.job_id}/run/{run.run_id}\")\n",
    "            print(f\"\\nâ³ Job is running... (not waiting for completion in demo)\")\n",
    "            print(f\"ğŸ’¡ Check the link above to monitor progress\")\n",
    "            \n",
    "            # Note: We're not waiting for completion to avoid long delays\n",
    "            # In production, you might use:\n",
    "            # result = w.jobs.wait_get_run_job_terminated_or_skipped(run_id=run.run_id)\n",
    "            \n",
    "        else:\n",
    "            print(\"âš ï¸ No demo jobs found. Create a job first.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No jobs found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "241fbc25-8515-4f4a-8e48-0c0f914a819b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¯ Challenge 2: Create a Job with Notifications\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Create a job with email notifications and run it.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Job name:** \"Challenge 2: Monitored Pipeline Job\"\n",
    "2. **Task:**\n",
    "   * Task key: \"monitored_pipeline\"\n",
    "   * Use any existing pipeline\n",
    "   * Incremental refresh\n",
    "3. **Notifications:**\n",
    "   * Send email to your user email on failure\n",
    "   * Send email to your user email on success\n",
    "   * No alerts for skipped runs\n",
    "4. **Timeout:** 30 minutes (1800 seconds)\n",
    "5. **Tags:**\n",
    "   * environment: \"challenge\"\n",
    "   * monitoring: \"enabled\"\n",
    "6. **After creating the job, run it using `w.jobs.run_now()`**\n",
    "\n",
    "---\n",
    "\n",
    "**Hints:**\n",
    "* Use `w.current_user.me().user_name` to get your email\n",
    "* Use `jobs.JobEmailNotifications()` for notifications\n",
    "* Use `timeout_seconds` parameter\n",
    "\n",
    "---\n",
    "\n",
    "**Write your code below:** ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7186ad-d14f-476d-aa36-ca10feb0e54d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Challenge 2 Solution Space"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHALLENGE 2: Your solution here\n",
    "# ============================================\n",
    "\n",
    "# TODO: Create a job with notifications and run it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d082a861-6818-44b3-824b-cedaa238004d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 6: Best Practices and Patterns âœ…\n",
    "\n",
    "Let's cover best practices for production Lakeflow Jobs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3292394c-3b78-47db-a346-2a0a6be9b5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¯ Lakeflow Jobs Best Practices\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Naming Conventions**\n",
    "\n",
    "**Use descriptive, consistent names:**\n",
    "\n",
    "```python\n",
    "# Good\n",
    "\"Production: Customer Pipeline - Daily\"\n",
    "\"Dev: Sales Analytics - Hourly\"\n",
    "\"Staging: Inventory Sync - Real-time\"\n",
    "\n",
    "# Bad\n",
    "\"job1\"\n",
    "\"test\"\n",
    "\"my_pipeline\"\n",
    "```\n",
    "\n",
    "**Include:**\n",
    "* Environment (Production, Dev, Staging)\n",
    "* Purpose (what it does)\n",
    "* Frequency (Daily, Hourly, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Use Tags for Organization**\n",
    "\n",
    "**Tag everything:**\n",
    "\n",
    "```python\n",
    "tags={\n",
    "    \"environment\": \"production\",\n",
    "    \"team\": \"data-engineering\",\n",
    "    \"cost_center\": \"analytics\",\n",
    "    \"criticality\": \"high\",\n",
    "    \"owner\": \"data-team@company.com\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "* Easy filtering and searching\n",
    "* Cost allocation\n",
    "* Ownership tracking\n",
    "* Governance and compliance\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Compute Strategy**\n",
    "\n",
    "**Choose the right compute:**\n",
    "\n",
    "**For DLT Pipelines:**\n",
    "* âœ… Use serverless (default)\n",
    "* âœ… Let DLT manage compute\n",
    "* âŒ Don't override unless necessary\n",
    "\n",
    "**For Notebook Tasks:**\n",
    "* âœ… Use job clusters (created per run)\n",
    "* âœ… Right-size for workload\n",
    "* âŒ Avoid existing clusters in production\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Error Handling**\n",
    "\n",
    "**Plan for failures:**\n",
    "\n",
    "```python\n",
    "# Set retries\n",
    "max_retries=3,\n",
    "min_retry_interval_millis=60000,  # 1 minute\n",
    "\n",
    "# Set timeouts\n",
    "timeout_seconds=7200,  # 2 hours\n",
    "\n",
    "# Configure notifications\n",
    "email_notifications=jobs.JobEmailNotifications(\n",
    "    on_failure=[\"oncall@company.com\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**Best practices:**\n",
    "* âœ… Set appropriate timeouts\n",
    "* âœ… Configure retries for transient failures\n",
    "* âœ… Always notify on failures\n",
    "* âœ… Log errors for debugging\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Scheduling Strategy**\n",
    "\n",
    "**Choose the right schedule:**\n",
    "\n",
    "**Batch Processing:**\n",
    "* âœ… Use cron schedules\n",
    "* âœ… Run during off-peak hours\n",
    "* âœ… Consider data freshness requirements\n",
    "\n",
    "**Streaming/Real-time:**\n",
    "* âœ… Use continuous mode\n",
    "* âœ… Monitor for backlog\n",
    "* âœ… Set up alerting\n",
    "\n",
    "**Event-driven:**\n",
    "* âœ… Use file arrival triggers\n",
    "* âœ… Set minimum time between triggers\n",
    "* âœ… Handle duplicate events\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Monitoring and Alerting**\n",
    "\n",
    "**Monitor everything:**\n",
    "\n",
    "```python\n",
    "# Critical jobs\n",
    "email_notifications=jobs.JobEmailNotifications(\n",
    "    on_failure=[\"oncall@company.com\", \"team@company.com\"],\n",
    "    on_duration_warning_threshold_exceeded=[\"manager@company.com\"]\n",
    ")\n",
    "\n",
    "# Set SLAs\n",
    "timeout_seconds=3600,  # Fail if exceeds 1 hour\n",
    "```\n",
    "\n",
    "**What to monitor:**\n",
    "* âœ… Success/failure rate\n",
    "* âœ… Run duration trends\n",
    "* âœ… Cost per run\n",
    "* âœ… Data quality metrics\n",
    "* âœ… Pipeline backlog\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Parameters and Configuration**\n",
    "\n",
    "**Make jobs reusable:**\n",
    "\n",
    "```python\n",
    "# Define parameters\n",
    "parameters=[\n",
    "    jobs.JobParameterDefinition(\n",
    "        name=\"environment\",\n",
    "        default=\"production\"\n",
    "    ),\n",
    "    jobs.JobParameterDefinition(\n",
    "        name=\"date\",\n",
    "        default=\"{{job.start_time.iso_date}}\"\n",
    "    )\n",
    "]\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "* âœ… Reuse same job for different environments\n",
    "* âœ… Override values at runtime\n",
    "* âœ… Easier testing and debugging\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Documentation**\n",
    "\n",
    "**Document your jobs:**\n",
    "\n",
    "```python\n",
    "jobs.Task(\n",
    "    task_key=\"ingest_customers\",\n",
    "    description=\"Ingest customer data from S3, apply CDC, and update silver table. Runs daily at midnight. Owner: data-team@company.com\",\n",
    "    pipeline_task=...\n",
    ")\n",
    "```\n",
    "\n",
    "**Include:**\n",
    "* Purpose and business context\n",
    "* Data sources and targets\n",
    "* Schedule and frequency\n",
    "* Owner and contact info\n",
    "* Dependencies and prerequisites\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Testing Strategy**\n",
    "\n",
    "**Test before production:**\n",
    "\n",
    "1. **Development:**\n",
    "   * Create dev version of job\n",
    "   * Use dev pipelines and data\n",
    "   * Test manually\n",
    "\n",
    "2. **Staging:**\n",
    "   * Create staging version\n",
    "   * Use production-like data\n",
    "   * Test scheduling\n",
    "\n",
    "3. **Production:**\n",
    "   * Deploy with monitoring\n",
    "   * Start with manual runs\n",
    "   * Enable schedule after validation\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Version Control**\n",
    "\n",
    "**Treat jobs as code:**\n",
    "\n",
    "```python\n",
    "# Store job definitions in Git\n",
    "# Use CI/CD to deploy\n",
    "# Track changes over time\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "* âœ… Audit trail of changes\n",
    "* âœ… Easy rollback\n",
    "* âœ… Peer review\n",
    "* âœ… Automated deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a21b3d39-0368-40d5-9885-bdecbb1c5b72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Production-Ready Job Example"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE 9: Production-Ready Job\n",
    "# ============================================\n",
    "\n",
    "# This example shows a complete production-ready job\n",
    "# with all best practices applied\n",
    "\n",
    "try:\n",
    "    pipelines = w.pipelines.list_pipelines()\n",
    "    pipeline_list = list(pipelines)\n",
    "    \n",
    "    if pipeline_list:\n",
    "        pipeline_id = pipeline_list[0].pipeline_id\n",
    "        current_user = w.current_user.me()\n",
    "        user_email = current_user.user_name\n",
    "        \n",
    "        # Create production-ready job\n",
    "        prod_job = w.jobs.create(\n",
    "            name=\"Production: Customer Data Pipeline - Daily\",\n",
    "            \n",
    "            # Tasks with clear descriptions\n",
    "            tasks=[\n",
    "                jobs.Task(\n",
    "                    task_key=\"ingest_and_transform\",\n",
    "                    description=\"Ingest customer data from source systems, apply transformations and data quality checks. Outputs to silver.customers table.\",\n",
    "                    pipeline_task=jobs.PipelineTask(\n",
    "                        pipeline_id=pipeline_id,\n",
    "                        full_refresh=False\n",
    "                    ),\n",
    "                    timeout_seconds=3600,  # 1 hour timeout per task\n",
    "                )\n",
    "            ],\n",
    "            \n",
    "            # Schedule: Daily at 2 AM UTC (off-peak)\n",
    "            schedule=jobs.CronSchedule(\n",
    "                quartz_cron_expression=\"0 0 2 * * ?\",\n",
    "                timezone_id=\"UTC\",\n",
    "                pause_status=jobs.PauseStatus.UNPAUSED\n",
    "            ),\n",
    "            \n",
    "            # Email notifications\n",
    "            email_notifications=jobs.JobEmailNotifications(\n",
    "                on_failure=[user_email],  # Alert on failure\n",
    "                on_success=[],  # Don't spam on success\n",
    "                no_alert_for_skipped_runs=True\n",
    "            ),\n",
    "            \n",
    "            # Retry configuration\n",
    "            max_retries=2,\n",
    "            min_retry_interval_millis=300000,  # 5 minutes between retries\n",
    "            retry_on_timeout=True,\n",
    "            \n",
    "            # Overall job timeout\n",
    "            timeout_seconds=7200,  # 2 hours max\n",
    "            \n",
    "            # Parameters for flexibility\n",
    "            parameters=[\n",
    "                jobs.JobParameterDefinition(\n",
    "                    name=\"environment\",\n",
    "                    default=\"production\"\n",
    "                ),\n",
    "                jobs.JobParameterDefinition(\n",
    "                    name=\"run_date\",\n",
    "                    default=\"{{job.start_time.iso_date}}\"\n",
    "                )\n",
    "            ],\n",
    "            \n",
    "            # Tags for organization\n",
    "            tags={\n",
    "                \"environment\": \"production\",\n",
    "                \"team\": \"data-engineering\",\n",
    "                \"pipeline\": \"customer-data\",\n",
    "                \"criticality\": \"high\",\n",
    "                \"owner\": user_email,\n",
    "                \"cost_center\": \"analytics\",\n",
    "                \"schedule\": \"daily\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Production-ready job created!\")\n",
    "        print(f\"ğŸ†” Job ID: {prod_job.job_id}\")\n",
    "        print(f\"\\nğŸ“‹ Configuration:\")\n",
    "        print(f\"   â€¢ Schedule: Daily at 2 AM UTC\")\n",
    "        print(f\"   â€¢ Timeout: 2 hours\")\n",
    "        print(f\"   â€¢ Retries: 2 (with 5 min intervals)\")\n",
    "        print(f\"   â€¢ Notifications: On failure to {user_email}\")\n",
    "        print(f\"   â€¢ Tags: {len(prod_job.settings.tags)} tags for organization\")\n",
    "        print(f\"\\nğŸ”— View job: {w.config.host}/#job/{prod_job.job_id}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ No pipelines found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe868ca-c801-426d-a868-4502d4b04ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ› ï¸ Job Management Operations\n",
    "\n",
    "**Common operations for managing jobs:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Update a Job**\n",
    "\n",
    "```python\n",
    "# Update job configuration\n",
    "w.jobs.update(\n",
    "    job_id=job_id,\n",
    "    new_settings=jobs.JobSettings(\n",
    "        name=\"Updated Job Name\",\n",
    "        schedule=jobs.CronSchedule(\n",
    "            quartz_cron_expression=\"0 0 3 * * ?\",  # Change to 3 AM\n",
    "            timezone_id=\"UTC\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pause/Resume Schedule**\n",
    "\n",
    "```python\n",
    "# Pause a scheduled job\n",
    "w.jobs.update(\n",
    "    job_id=job_id,\n",
    "    new_settings=jobs.JobSettings(\n",
    "        schedule=jobs.CronSchedule(\n",
    "            quartz_cron_expression=\"0 0 2 * * ?\",\n",
    "            timezone_id=\"UTC\",\n",
    "            pause_status=jobs.PauseStatus.PAUSED  # Pause\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Resume\n",
    "w.jobs.update(\n",
    "    job_id=job_id,\n",
    "    new_settings=jobs.JobSettings(\n",
    "        schedule=jobs.CronSchedule(\n",
    "            quartz_cron_expression=\"0 0 2 * * ?\",\n",
    "            timezone_id=\"UTC\",\n",
    "            pause_status=jobs.PauseStatus.UNPAUSED  # Resume\n",
    "        )\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Delete a Job**\n",
    "\n",
    "```python\n",
    "# Delete a job (careful!)\n",
    "w.jobs.delete(job_id=job_id)\n",
    "\n",
    "print(f\"Deleted job {job_id}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Export Job Configuration**\n",
    "\n",
    "```python\n",
    "# Get job details\n",
    "job = w.jobs.get(job_id=job_id)\n",
    "\n",
    "# Export to JSON for version control\n",
    "import json\n",
    "\n",
    "job_config = {\n",
    "    \"name\": job.settings.name,\n",
    "    \"tasks\": [task.as_dict() for task in job.settings.tasks],\n",
    "    \"schedule\": job.settings.schedule.as_dict() if job.settings.schedule else None,\n",
    "    \"tags\": job.settings.tags\n",
    "}\n",
    "\n",
    "with open(f\"job_{job_id}.json\", \"w\") as f:\n",
    "    json.dump(job_config, f, indent=2)\n",
    "\n",
    "print(f\"Exported job configuration to job_{job_id}.json\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "201d09bd-f936-47ac-83ac-70071f7af79b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleanup Demo Jobs"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CLEANUP: Delete Demo Jobs (Optional)\n",
    "# ============================================\n",
    "\n",
    "# Uncomment and run this cell to clean up demo jobs\n",
    "\n",
    "# try:\n",
    "#     # List all jobs\n",
    "#     all_jobs = list(w.jobs.list())\n",
    "#     \n",
    "#     # Find and delete demo jobs\n",
    "#     deleted_count = 0\n",
    "#     for job in all_jobs:\n",
    "#         if \"Demo:\" in job.settings.name or \"Challenge\" in job.settings.name:\n",
    "#             print(f\"ğŸ—‘ï¸ Deleting: {job.settings.name} (ID: {job.job_id})\")\n",
    "#             w.jobs.delete(job_id=job.job_id)\n",
    "#             deleted_count += 1\n",
    "#     \n",
    "#     print(f\"\\nâœ… Deleted {deleted_count} demo jobs\")\n",
    "#     \n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Error: {e}\")\n",
    "\n",
    "print(\"ğŸ’¡ Uncomment the code above to delete demo jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bab7431-6d92-4291-b4db-39d9fdc39b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Summary: Lakeflow Jobs Mastery ğŸ“\n",
    "\n",
    "Congratulations! You've learned how to create and manage Lakeflow Jobs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts:**\n",
    "\n",
    "**1. Jobs vs Pipelines:**\n",
    "* âœ… Pipelines = data transformations (the \"what\")\n",
    "* âœ… Jobs = orchestration (the \"when\" and \"how\")\n",
    "* âœ… Jobs run pipelines on schedules or triggers\n",
    "\n",
    "**2. Job Components:**\n",
    "* âœ… **Tasks** - Units of work (pipelines, notebooks, etc.)\n",
    "* âœ… **Schedule** - When to run (cron, continuous, file arrival)\n",
    "* âœ… **Compute** - Resources for execution\n",
    "* âœ… **Parameters** - Dynamic configuration\n",
    "* âœ… **Notifications** - Alerts on events\n",
    "\n",
    "**3. Databricks SDK:**\n",
    "* âœ… `WorkspaceClient()` - Initialize client\n",
    "* âœ… `w.jobs.create()` - Create jobs\n",
    "* âœ… `w.jobs.run_now()` - Trigger runs\n",
    "* âœ… `w.jobs.list_runs()` - Monitor execution\n",
    "* âœ… `w.jobs.update()` - Modify jobs\n",
    "* âœ… `w.jobs.delete()` - Remove jobs\n",
    "\n",
    "**4. Task Types:**\n",
    "* âœ… **Pipeline Task** - Run DLT pipelines\n",
    "* âœ… **Notebook Task** - Run notebooks\n",
    "* âœ… **Python Task** - Run Python code\n",
    "* âœ… **SQL Task** - Run SQL queries\n",
    "\n",
    "**5. Scheduling:**\n",
    "* âœ… **Cron** - Time-based (daily, hourly, etc.)\n",
    "* âœ… **Continuous** - Always running\n",
    "* âœ… **File Arrival** - Event-driven\n",
    "* âœ… **Manual** - On-demand\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices:**\n",
    "\n",
    "* âœ… Use descriptive naming conventions\n",
    "* âœ… Tag everything for organization\n",
    "* âœ… Use serverless compute for DLT\n",
    "* âœ… Configure retries and timeouts\n",
    "* âœ… Set up notifications for failures\n",
    "* âœ… Monitor success rates and duration\n",
    "* âœ… Make jobs reusable with parameters\n",
    "* âœ… Document purpose and ownership\n",
    "* âœ… Test in dev/staging before production\n",
    "* âœ… Version control job configurations\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "1. **Create production jobs:**\n",
    "   * Identify pipelines to automate\n",
    "   * Define schedules and SLAs\n",
    "   * Configure monitoring and alerts\n",
    "\n",
    "2. **Implement CI/CD:**\n",
    "   * Store job configs in Git\n",
    "   * Automate deployment\n",
    "   * Use environments (dev/staging/prod)\n",
    "\n",
    "3. **Monitor and optimize:**\n",
    "   * Track job metrics\n",
    "   * Optimize schedules\n",
    "   * Reduce costs\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources:**\n",
    "\n",
    "* [Lakeflow Pipeline Fundamentals](#notebook/2846436383063456)\n",
    "* [Lakeflow Expectations](#notebook/2846436383063443)\n",
    "* [Lakeflow Auto CDC](#notebook/2846436383063462)\n",
    "* [Databricks Jobs API Documentation](https://docs.databricks.com/api/workspace/jobs)\n",
    "* [Databricks SDK Documentation](https://databricks-sdk-py.readthedocs.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy orchestrating!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cc9390b-467d-4880-845a-1114817669ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Next Steps: Putting It All Together ğŸ¯\n",
    "\n",
    "**You now have all the skills to build production data pipelines!**\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Workflow:**\n",
    "\n",
    "**1. Design Pipeline (Lakeflow Fundamentals)**\n",
    "* Define bronze, silver, gold layers\n",
    "* Create streaming tables and views\n",
    "* Implement transformations\n",
    "\n",
    "**2. Add Data Quality (Lakeflow Expectations)**\n",
    "* Add expectations for validation\n",
    "* Use expect, expect_or_drop, expect_or_fail\n",
    "* Monitor data quality metrics\n",
    "\n",
    "**3. Implement CDC (Lakeflow Auto CDC)**\n",
    "* Use Auto CDC for change tracking\n",
    "* Choose SCD Type 1 or Type 2\n",
    "* Handle inserts, updates, deletes\n",
    "\n",
    "**4. Orchestrate (Lakeflow Jobs)**\n",
    "* Create jobs to run pipelines\n",
    "* Schedule with cron or triggers\n",
    "* Configure monitoring and alerts\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Production Architecture:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  Lakeflow Job                       â”‚\n",
    "â”‚              (Scheduled Daily at 2 AM)              â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚         DLT Pipeline (Bronze â†’ Gold)          â”‚ â”‚\n",
    "â”‚  â”‚                                               â”‚ â”‚\n",
    "â”‚  â”‚  Bronze: Ingest with Auto Loader             â”‚ â”‚\n",
    "â”‚  â”‚     â†“                                         â”‚ â”‚\n",
    "â”‚  â”‚  Silver: Auto CDC + Expectations             â”‚ â”‚\n",
    "â”‚  â”‚     â†“                                         â”‚ â”‚\n",
    "â”‚  â”‚  Gold: Aggregations + Business Logic         â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚      Post-Processing Notebook                 â”‚ â”‚\n",
    "â”‚  â”‚  (Generate reports, update dashboards)        â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  ğŸ“§ Email notifications on failure                 â”‚\n",
    "â”‚  ğŸ”„ Retry 2x on transient errors                   â”‚\n",
    "â”‚  â±ï¸ 2 hour timeout                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ready to Build?**\n",
    "\n",
    "**Start with:**\n",
    "1. Identify a use case\n",
    "2. Design your pipeline\n",
    "3. Implement with best practices\n",
    "4. Test thoroughly\n",
    "5. Deploy to production\n",
    "6. Monitor and iterate\n",
    "\n",
    "---\n",
    "\n",
    "**You've got this!** ğŸ’ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99bdbcc9-d826-47b3-946b-d7dca4e0fb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lakeflow Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
