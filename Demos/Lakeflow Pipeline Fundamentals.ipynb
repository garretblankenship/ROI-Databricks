{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9e48164-1b39-41cf-aa16-be7dea7a1add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Pipelines: Bronze â†’ Silver â†’ Gold ðŸ¥‰ðŸ¥ˆðŸ¥‡\n",
    "\n",
    "Welcome! This demo will teach you how to build declarative data pipelines using **Lakeflow** (Delta Live Tables).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "By the end of this demo, you will be able to:\n",
    "\n",
    "1. âœ… Understand Lakeflow (Delta Live Tables) concepts\n",
    "2. âœ… Differentiate between streaming tables and materialized views\n",
    "3. âœ… Create bronze layer with streaming tables\n",
    "4. âœ… Build silver layer with data quality transformations\n",
    "5. âœ… Create gold layer with materialized views\n",
    "6. âœ… Configure and run a Lakeflow pipeline\n",
    "7. âœ… View pipeline results and lineage\n",
    "8. âœ… Understand declarative vs imperative pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŠ What is Lakeflow?\n",
    "\n",
    "**Lakeflow** (formerly Delta Live Tables / DLT) is a declarative framework for building reliable data pipelines.\n",
    "\n",
    "**Key concepts:**\n",
    "* **Declarative** - Define WHAT you want, not HOW to do it\n",
    "* **Automatic** - Handles dependencies, retries, scaling\n",
    "* **Reliable** - Built-in data quality and monitoring\n",
    "* **Incremental** - Processes only new data\n",
    "* **Managed** - No infrastructure management\n",
    "\n",
    "**Traditional pipeline vs Lakeflow:**\n",
    "\n",
    "| Traditional | Lakeflow |\n",
    "|-------------|----------|\n",
    "| Write orchestration code | Declare tables |\n",
    "| Handle dependencies manually | Automatic dependency resolution |\n",
    "| Manage incremental logic | Built-in incremental processing |\n",
    "| Write error handling | Automatic retries |\n",
    "| Monitor manually | Built-in monitoring |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Demo Scenario: Wanderbricks Analytics\n",
    "\n",
    "You'll build a pipeline for **Wanderbricks** booking data:\n",
    "\n",
    "**Pipeline flow:**\n",
    "```\n",
    "ðŸ“ Raw Data (samples.wanderbricks)\n",
    "    â†“\n",
    "ðŸ¥‰ Bronze: Streaming tables (raw ingestion)\n",
    "    â†“\n",
    "ðŸ¥ˆ Silver: Streaming tables (cleaned, validated)\n",
    "    â†“\n",
    "ðŸ¥‡ Gold: Materialized views (aggregated metrics)\n",
    "```\n",
    "\n",
    "**Data sources:**\n",
    "* Bookings data\n",
    "* Properties data\n",
    "* Reviews data\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d27bf2d5-b204-498d-869c-7662cd663d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Lakeflow Core Concepts ðŸ“š\n",
    "\n",
    "Before building, let's understand the key concepts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070a3211-e236-44b0-a9d0-114a01df090b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŒŠ Streaming Tables vs Materialized Views\n",
    "\n",
    "**Lakeflow has two table types:**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Streaming Tables**\n",
    "\n",
    "**What they are:**\n",
    "* Process data incrementally as it arrives\n",
    "* Append-only by default\n",
    "* Maintain state for incremental processing\n",
    "* Always up-to-date\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Bronze layer (raw data ingestion)\n",
    "* âœ… Silver layer (incremental transformations)\n",
    "* âœ… Real-time or near-real-time data\n",
    "* âœ… Append-only data (logs, events, transactions)\n",
    "* âœ… When you need low latency\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "@dlt.table\n",
    "def bronze_bookings():\n",
    "    return spark.readStream.table(\"samples.wanderbricks.bookings\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Materialized Views**\n",
    "\n",
    "**What they are:**\n",
    "* Recompute entire result on each update\n",
    "* Can handle updates and deletes\n",
    "* Batch processing\n",
    "* Optimized for aggregations\n",
    "\n",
    "**When to use:**\n",
    "* âœ… Gold layer (aggregations)\n",
    "* âœ… Complex joins\n",
    "* âœ… Aggregations (GROUP BY)\n",
    "* âœ… When you need complete refresh\n",
    "* âœ… Smaller result sets\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "@dlt.view\n",
    "def gold_daily_revenue():\n",
    "    return dlt.read(\"silver_bookings\").groupBy(\"date\").agg(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences:**\n",
    "\n",
    "| Feature | Streaming Table | Materialized View |\n",
    "|---------|----------------|-------------------|\n",
    "| **Processing** | Incremental | Full refresh |\n",
    "| **Updates** | Append-only | Supports updates/deletes |\n",
    "| **Latency** | Low (real-time) | Higher (batch) |\n",
    "| **Use case** | Bronze, Silver | Gold (aggregations) |\n",
    "| **State** | Maintains checkpoints | Stateless |\n",
    "| **Best for** | Large data volumes | Aggregated results |\n",
    "\n",
    "---\n",
    "\n",
    "**Rule of thumb:**\n",
    "* ðŸ¥‰ **Bronze:** Streaming tables (ingest raw data)\n",
    "* ðŸ¥ˆ **Silver:** Streaming tables (incremental cleaning)\n",
    "* ðŸ¥‡ **Gold:** Materialized views (aggregations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1b4ff74-8f1b-4729-a044-e248e491f2f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ“ Declarative Pipeline Syntax\n",
    "\n",
    "**Lakeflow uses Python decorators to declare tables:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Basic syntax:**\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Streaming table\n",
    "@dlt.table(\n",
    "    name=\"table_name\",\n",
    "    comment=\"Description of the table\"\n",
    ")\n",
    "def table_name():\n",
    "    return spark.readStream.table(\"source_table\")\n",
    "\n",
    "# Materialized view\n",
    "@dlt.view(\n",
    "    name=\"view_name\",\n",
    "    comment=\"Description of the view\"\n",
    ")\n",
    "def view_name():\n",
    "    return dlt.read(\"upstream_table\").groupBy(...).agg(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key components:**\n",
    "\n",
    "**1. Import dlt module:**\n",
    "```python\n",
    "import dlt\n",
    "```\n",
    "\n",
    "**2. Decorator:**\n",
    "* `@dlt.table` - Creates a streaming table\n",
    "* `@dlt.view` - Creates a materialized view\n",
    "\n",
    "**3. Function:**\n",
    "* Function name becomes table name (if not specified)\n",
    "* Returns a DataFrame\n",
    "* Can use PySpark transformations\n",
    "\n",
    "**4. Reading data:**\n",
    "* `spark.readStream.table()` - Read from source (streaming)\n",
    "* `dlt.read()` - Read from another DLT table\n",
    "* `spark.read.table()` - Read batch data\n",
    "\n",
    "---\n",
    "\n",
    "### **Dependencies are automatic:**\n",
    "\n",
    "```python\n",
    "# Bronze\n",
    "@dlt.table\n",
    "def bronze_bookings():\n",
    "    return spark.readStream.table(\"samples.wanderbricks.bookings\")\n",
    "\n",
    "# Silver (depends on bronze)\n",
    "@dlt.table\n",
    "def silver_bookings():\n",
    "    return dlt.read_stream(\"bronze_bookings\").filter(...)  # Automatic dependency!\n",
    "\n",
    "# Gold (depends on silver)\n",
    "@dlt.view\n",
    "def gold_revenue():\n",
    "    return dlt.read(\"silver_bookings\").groupBy(...).agg(...)  # Automatic dependency!\n",
    "```\n",
    "\n",
    "**Lakeflow automatically:**\n",
    "* âœ… Detects dependencies\n",
    "* âœ… Runs tables in correct order\n",
    "* âœ… Handles failures and retries\n",
    "* âœ… Manages incremental processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a445fdec-e2eb-4e72-8bc6-47f1e6b55853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Bronze Layer - Raw Data Ingestion ðŸ¥‰\n",
    "\n",
    "The bronze layer ingests raw data using **streaming tables**.\n",
    "\n",
    "**Bronze characteristics:**\n",
    "* Ingest data as-is (no transformations)\n",
    "* Streaming tables for incremental processing\n",
    "* Append-only\n",
    "* Full audit trail\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff1a7c43-d064-4ddf-93c2-41e3de2916c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze Layer Code"
    }
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ============================================\n",
    "# BRONZE LAYER: Raw Data Ingestion\n",
    "# ============================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_bookings\",\n",
    "    comment=\"Raw booking data from Wanderbricks - Bronze layer\"\n",
    ")\n",
    "def bronze_bookings():\n",
    "    \"\"\"\n",
    "    Ingest raw booking data as streaming table.\n",
    "    No transformations - data as-is from source.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .table(\"samples.wanderbricks.bookings\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_properties\",\n",
    "    comment=\"Raw property data from Wanderbricks - Bronze layer\"\n",
    ")\n",
    "def bronze_properties():\n",
    "    \"\"\"\n",
    "    Ingest raw property data as streaming table.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .table(\"samples.wanderbricks.properties\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_reviews\",\n",
    "    comment=\"Raw review data from Wanderbricks - Bronze layer\"\n",
    ")\n",
    "def bronze_reviews():\n",
    "    \"\"\"\n",
    "    Ingest raw review data as streaming table.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .table(\"samples.wanderbricks.reviews\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76abdc33-9954-428f-9c15-3ac3adeac86f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’¡ Bronze Layer - Key Points\n",
    "\n",
    "**What we did:**\n",
    "\n",
    "1. **Created 3 streaming tables:**\n",
    "   * `bronze_bookings` - Booking transactions\n",
    "   * `bronze_properties` - Property listings\n",
    "   * `bronze_reviews` - Customer reviews\n",
    "\n",
    "2. **Used `@dlt.table` decorator:**\n",
    "   * Declares a streaming table\n",
    "   * Automatically managed by Lakeflow\n",
    "   * Incremental processing\n",
    "\n",
    "3. **Used `spark.readStream.table()`:**\n",
    "   * Reads from source tables as streams\n",
    "   * Processes incrementally\n",
    "   * Tracks progress automatically\n",
    "\n",
    "**Why streaming tables for bronze?**\n",
    "* âœ… Incremental ingestion (only new data)\n",
    "* âœ… Low latency\n",
    "* âœ… Efficient for large data volumes\n",
    "* âœ… Automatic checkpoint management\n",
    "* âœ… Append-only semantics\n",
    "\n",
    "**What Lakeflow handles automatically:**\n",
    "* âœ… Checkpoint management\n",
    "* âœ… Incremental processing\n",
    "* âœ… Schema evolution\n",
    "* âœ… Failure recovery\n",
    "* âœ… Backpressure\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Bronze layer is always streaming tables - ingest raw data as-is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5934c81c-e016-4a03-a25b-5e2b78a5d3db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Silver Layer - Cleaned & Validated Data ðŸ¥ˆ\n",
    "\n",
    "The silver layer cleans and validates data using **streaming tables**.\n",
    "\n",
    "**Silver characteristics:**\n",
    "* Clean and validate data\n",
    "* Remove duplicates\n",
    "* Filter invalid records\n",
    "* Join related tables\n",
    "* Still streaming (incremental)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475974ca-3f1f-40cd-8ee3-b239d606d642",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Layer Code"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SILVER LAYER: Cleaned and Validated Data\n",
    "# ============================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_bookings\",\n",
    "    comment=\"Cleaned and validated booking data - Silver layer\"\n",
    ")\n",
    "def silver_bookings():\n",
    "    \"\"\"\n",
    "    Clean bronze bookings:\n",
    "    - Filter confirmed bookings only\n",
    "    - Remove invalid data (null amounts, invalid dates)\n",
    "    - Add calculated columns\n",
    "    - Deduplicate\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_bookings\")\n",
    "        # Data quality filters\n",
    "        .filter(col(\"status\") == \"confirmed\")\n",
    "        .filter(col(\"total_amount\").isNotNull())\n",
    "        .filter(col(\"total_amount\") > 0)\n",
    "        .filter(col(\"check_in\") < col(\"check_out\"))  # Valid date range\n",
    "        # Add calculated columns\n",
    "        .withColumn(\"nights\", datediff(col(\"check_out\"), col(\"check_in\")))\n",
    "        .withColumn(\"booking_date\", to_date(col(\"created_at\")))\n",
    "        .withColumn(\"booking_year\", year(col(\"created_at\")))\n",
    "        .withColumn(\"booking_month\", month(col(\"created_at\")))\n",
    "        # Select relevant columns\n",
    "        .select(\n",
    "            \"booking_id\",\n",
    "            \"user_id\",\n",
    "            \"property_id\",\n",
    "            \"check_in\",\n",
    "            \"check_out\",\n",
    "            \"nights\",\n",
    "            \"guests_count\",\n",
    "            \"total_amount\",\n",
    "            \"status\",\n",
    "            \"booking_date\",\n",
    "            \"booking_year\",\n",
    "            \"booking_month\",\n",
    "            \"created_at\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_properties\",\n",
    "    comment=\"Cleaned property data - Silver layer\"\n",
    ")\n",
    "def silver_properties():\n",
    "    \"\"\"\n",
    "    Clean bronze properties:\n",
    "    - Filter valid properties\n",
    "    - Standardize property types\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_properties\")\n",
    "        # Data quality filters\n",
    "        .filter(col(\"property_type\").isNotNull())\n",
    "        .filter(col(\"base_price\").isNotNull())\n",
    "        .filter(col(\"base_price\") > 0)\n",
    "        # Select relevant columns\n",
    "        .select(\n",
    "            \"property_id\",\n",
    "            \"host_id\",\n",
    "            \"destination_id\",\n",
    "            \"title\",\n",
    "            \"property_type\",\n",
    "            \"base_price\",\n",
    "            \"max_guests\",\n",
    "            \"bedrooms\",\n",
    "            \"bathrooms\",\n",
    "            \"created_at\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_reviews\",\n",
    "    comment=\"Cleaned review data - Silver layer\"\n",
    ")\n",
    "def silver_reviews():\n",
    "    \"\"\"\n",
    "    Clean bronze reviews:\n",
    "    - Filter non-deleted reviews\n",
    "    - Filter valid ratings\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_reviews\")\n",
    "        # Data quality filters\n",
    "        .filter(col(\"is_deleted\") == False)\n",
    "        .filter(col(\"rating\").isNotNull())\n",
    "        .filter((col(\"rating\") >= 1.0) & (col(\"rating\") <= 5.0))\n",
    "        # Add calculated columns\n",
    "        .withColumn(\"review_date\", to_date(col(\"created_at\")))\n",
    "        .withColumn(\"review_year\", year(col(\"created_at\")))\n",
    "        .withColumn(\"review_month\", month(col(\"created_at\")))\n",
    "        # Select relevant columns\n",
    "        .select(\n",
    "            \"review_id\",\n",
    "            \"booking_id\",\n",
    "            \"property_id\",\n",
    "            \"user_id\",\n",
    "            \"rating\",\n",
    "            \"comment\",\n",
    "            \"review_date\",\n",
    "            \"review_year\",\n",
    "            \"review_month\",\n",
    "            \"created_at\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f15d563a-d349-45ca-b098-ee3922221fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’¡ Silver Layer - Key Points\n",
    "\n",
    "**What we did:**\n",
    "\n",
    "1. **Created 3 streaming tables:**\n",
    "   * `silver_bookings` - Cleaned bookings\n",
    "   * `silver_properties` - Cleaned properties\n",
    "   * `silver_reviews` - Cleaned reviews\n",
    "\n",
    "2. **Used `dlt.read_stream()`:**\n",
    "   * Reads from bronze streaming tables\n",
    "   * Maintains streaming semantics\n",
    "   * Incremental processing continues\n",
    "\n",
    "3. **Applied data quality rules:**\n",
    "   * Filter invalid data (nulls, negatives, invalid ranges)\n",
    "   * Remove soft-deleted records\n",
    "   * Validate business rules\n",
    "\n",
    "4. **Added calculated columns:**\n",
    "   * `nights` - Duration of stay\n",
    "   * `booking_date` - Date portion of timestamp\n",
    "   * `booking_year`, `booking_month` - For aggregations\n",
    "\n",
    "**Why streaming tables for silver?**\n",
    "* âœ… Incremental processing (efficient)\n",
    "* âœ… Low latency\n",
    "* âœ… Handles large data volumes\n",
    "* âœ… Maintains data lineage\n",
    "\n",
    "**Data quality patterns:**\n",
    "```python\n",
    "# Filter nulls\n",
    ".filter(col(\"column\").isNotNull())\n",
    "\n",
    "# Filter invalid values\n",
    ".filter(col(\"amount\") > 0)\n",
    "\n",
    "# Filter by status\n",
    ".filter(col(\"status\") == \"confirmed\")\n",
    "\n",
    "# Validate ranges\n",
    ".filter((col(\"rating\") >= 1.0) & (col(\"rating\") <= 5.0))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Silver layer filters bad data and adds business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb640b07-2c2b-420a-be99-203b14f9d982",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Layer with JOIN"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SILVER LAYER: Enriched Data with JOINs\n",
    "# ============================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_bookings_enriched\",\n",
    "    comment=\"Bookings enriched with property details - Silver layer\"\n",
    ")\n",
    "def silver_bookings_enriched():\n",
    "    \"\"\"\n",
    "    Join bookings with properties to create enriched dataset.\n",
    "    This is still a streaming table for incremental processing.\n",
    "    \"\"\"\n",
    "    bookings = dlt.read_stream(\"silver_bookings\")\n",
    "    properties = dlt.read_stream(\"silver_properties\")\n",
    "    \n",
    "    return (\n",
    "        bookings\n",
    "        .join(\n",
    "            properties,\n",
    "            bookings.property_id == properties.property_id,\n",
    "            \"left\"\n",
    "        )\n",
    "        .select(\n",
    "            bookings[\"*\"],\n",
    "            properties[\"property_type\"],\n",
    "            properties[\"title\"].alias(\"property_name\"),\n",
    "            properties[\"base_price\"],\n",
    "            properties[\"bedrooms\"],\n",
    "            properties[\"bathrooms\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e0855f-8427-4823-8fa1-3bd1cda9b592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ”— Silver Layer - JOINs in Streaming\n",
    "\n",
    "**What we did:**\n",
    "\n",
    "1. **Created enriched streaming table:**\n",
    "   * `silver_bookings_enriched` - Bookings + property details\n",
    "\n",
    "2. **Joined two streaming tables:**\n",
    "   * Read both as streams with `dlt.read_stream()`\n",
    "   * Perform stream-stream JOIN\n",
    "   * Result is still a streaming table\n",
    "\n",
    "3. **Selected columns from both tables:**\n",
    "   * All booking columns\n",
    "   * Property type, name, price, bedrooms, bathrooms\n",
    "\n",
    "**Stream-stream JOINs:**\n",
    "* âœ… Both sides are streaming\n",
    "* âœ… Incremental processing\n",
    "* âœ… Lakeflow manages state\n",
    "* âœ… Automatic watermarking\n",
    "\n",
    "**JOIN patterns in DLT:**\n",
    "```python\n",
    "# Stream-stream JOIN\n",
    "left_stream = dlt.read_stream(\"table1\")\n",
    "right_stream = dlt.read_stream(\"table2\")\n",
    "result = left_stream.join(right_stream, \"key\")\n",
    "\n",
    "# Stream-batch JOIN (dimension lookup)\n",
    "stream = dlt.read_stream(\"fact_table\")\n",
    "dimension = dlt.read(\"dimension_table\")  # Batch read\n",
    "result = stream.join(dimension, \"key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Use stream-stream JOINs for fact tables, stream-batch for dimension lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42941550-8557-4841-b2e1-856b391c086a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: Gold Layer - Aggregated Metrics ðŸ¥‡\n",
    "\n",
    "The gold layer creates business metrics using **materialized views**.\n",
    "\n",
    "**Gold characteristics:**\n",
    "* Aggregated data (GROUP BY)\n",
    "* Business-ready metrics\n",
    "* Materialized views (full refresh)\n",
    "* Optimized for analytics\n",
    "* Smaller result sets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33706496-aa9f-4b4c-9ff0-a9a7ff4cfd37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold Layer Code"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GOLD LAYER: Aggregated Business Metrics\n",
    "# ============================================\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"gold_daily_bookings\",\n",
    "    comment=\"Daily booking metrics - Gold layer\"\n",
    ")\n",
    "def gold_daily_bookings():\n",
    "    \"\"\"\n",
    "    Daily aggregated booking metrics.\n",
    "    Uses materialized view for full refresh aggregations.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"silver_bookings_enriched\")\n",
    "        .groupBy(\"booking_date\", \"property_type\")\n",
    "        .agg(\n",
    "            count(\"booking_id\").alias(\"booking_count\"),\n",
    "            sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            avg(\"total_amount\").alias(\"avg_booking_value\"),\n",
    "            sum(\"nights\").alias(\"total_nights\"),\n",
    "            avg(\"nights\").alias(\"avg_nights\"),\n",
    "            countDistinct(\"user_id\").alias(\"unique_customers\"),\n",
    "            countDistinct(\"property_id\").alias(\"unique_properties\")\n",
    "        )\n",
    "        .orderBy(\"booking_date\", \"property_type\")\n",
    "    )\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"gold_property_performance\",\n",
    "    comment=\"Property performance metrics - Gold layer\"\n",
    ")\n",
    "def gold_property_performance():\n",
    "    \"\"\"\n",
    "    Aggregated metrics by property type.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"silver_bookings_enriched\")\n",
    "        .groupBy(\"property_type\")\n",
    "        .agg(\n",
    "            count(\"booking_id\").alias(\"total_bookings\"),\n",
    "            sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            avg(\"total_amount\").alias(\"avg_revenue_per_booking\"),\n",
    "            avg(\"nights\").alias(\"avg_stay_duration\"),\n",
    "            avg(\"guests_count\").alias(\"avg_guests\"),\n",
    "            countDistinct(\"property_id\").alias(\"property_count\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    )\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"gold_review_summary\",\n",
    "    comment=\"Review rating summary by property type - Gold layer\"\n",
    ")\n",
    "def gold_review_summary():\n",
    "    \"\"\"\n",
    "    Average ratings by property type and time period.\n",
    "    Joins reviews with properties.\n",
    "    \"\"\"\n",
    "    reviews = dlt.read(\"silver_reviews\")\n",
    "    properties = dlt.read(\"silver_properties\")\n",
    "    \n",
    "    return (\n",
    "        reviews\n",
    "        .join(properties, \"property_id\", \"left\")\n",
    "        .groupBy(\"property_type\", \"review_year\", \"review_month\")\n",
    "        .agg(\n",
    "            count(\"review_id\").alias(\"review_count\"),\n",
    "            avg(\"rating\").alias(\"avg_rating\"),\n",
    "            min(\"rating\").alias(\"min_rating\"),\n",
    "            max(\"rating\").alias(\"max_rating\")\n",
    "        )\n",
    "        .orderBy(\"review_year\", \"review_month\", \"property_type\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad8071b-f533-4666-8fae-7129a251713e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’¡ Gold Layer - Key Points\n",
    "\n",
    "**What we did:**\n",
    "\n",
    "1. **Created 3 materialized views:**\n",
    "   * `gold_daily_bookings` - Daily metrics by property type\n",
    "   * `gold_property_performance` - Overall property type performance\n",
    "   * `gold_review_summary` - Review ratings over time\n",
    "\n",
    "2. **Used `@dlt.view` decorator:**\n",
    "   * Creates materialized views\n",
    "   * Full refresh on each update\n",
    "   * Optimized for aggregations\n",
    "\n",
    "3. **Used `dlt.read()` (not read_stream):**\n",
    "   * Batch read from silver tables\n",
    "   * Appropriate for aggregations\n",
    "   * Full table scan\n",
    "\n",
    "4. **Applied aggregations:**\n",
    "   * `count()`, `sum()`, `avg()`, `min()`, `max()`\n",
    "   * `countDistinct()` for unique counts\n",
    "   * `groupBy()` for dimensions\n",
    "\n",
    "**Why materialized views for gold?**\n",
    "* âœ… Aggregations require full data\n",
    "* âœ… Smaller result sets (efficient full refresh)\n",
    "* âœ… Optimized for analytics\n",
    "* âœ… Simpler than streaming aggregations\n",
    "* âœ… Perfect for dashboards\n",
    "\n",
    "**Streaming table vs Materialized view for gold:**\n",
    "\n",
    "| Use Case | Recommendation |\n",
    "|----------|----------------|\n",
    "| Simple aggregations | Materialized view |\n",
    "| Large fact tables | Streaming table |\n",
    "| Real-time metrics | Streaming table |\n",
    "| Dashboard metrics | Materialized view |\n",
    "| Complex joins | Materialized view |\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Gold layer typically uses materialized views for aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5535d7dc-c0a6-4fd8-8ade-241209068ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Creating Your Lakeflow Pipeline ðŸ› ï¸\n",
    "\n",
    "Now that we've written the code, let's create the pipeline in the Databricks UI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7080b24e-e836-490d-af38-72e0446b1322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’¾ Step 1: Save This Notebook\n",
    "\n",
    "**Follow these steps:**\n",
    "\n",
    "1. This notebook contains all your pipeline code\n",
    "2. Click **\"Save\"** or press `Ctrl+S` / `Cmd+S`\n",
    "3. Note the notebook path (you'll need it)\n",
    "4. Example path: `/Users/your.email@company.com/Lakeflow Pipeline Fundamentals`\n",
    "\n",
    "**What's in this notebook:**\n",
    "* Bronze layer: 3 streaming tables\n",
    "* Silver layer: 4 streaming tables (including enriched)\n",
    "* Gold layer: 3 materialized views\n",
    "* Total: 10 tables/views\n",
    "\n",
    "âœ… **Checkpoint:** Notebook is saved\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** The notebook IS your pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070a9d84-9f54-4183-a97c-3ceaf0cd150b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ—ï¸ Step 2: Create a New Pipeline\n",
    "\n",
    "**Follow these steps:**\n",
    "\n",
    "1. Click **\"Workflows\"** in the left sidebar\n",
    "2. Click the **\"Delta Live Tables\"** tab\n",
    "3. Click **\"Create Pipeline\"** button\n",
    "4. A pipeline configuration form opens\n",
    "\n",
    "**Pipeline configuration form:**\n",
    "\n",
    "**General settings:**\n",
    "* **Pipeline name:** \"Wanderbricks Analytics Pipeline\"\n",
    "* **Product edition:** Advanced (or Core)\n",
    "* **Pipeline mode:** Triggered (or Continuous)\n",
    "\n",
    "**Source code:**\n",
    "* **Notebook libraries:** Click \"Add notebook\"\n",
    "* **Path:** Select this notebook (Lakeflow Pipeline Fundamentals)\n",
    "* The notebook path should appear\n",
    "\n",
    "**Destination:**\n",
    "* **Target schema:** `main.default` (or create new schema)\n",
    "* **Storage location:** Leave default (optional)\n",
    "\n",
    "**Compute:**\n",
    "* **Cluster mode:** Enhanced autoscaling (recommended)\n",
    "* **Min workers:** 1\n",
    "* **Max workers:** 5\n",
    "* **Serverless:** Enable if available (recommended)\n",
    "\n",
    "**Advanced settings (leave defaults for now):**\n",
    "* Channel: Current\n",
    "* Photon: Enabled\n",
    "* Configuration: Empty\n",
    "\n",
    "5. Click **\"Create\"** button\n",
    "\n",
    "âœ… **Checkpoint:** Pipeline created (not yet run)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Serverless DLT is faster and easier - no cluster management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bdd1ec4-f79e-4223-be57-af393c5a354c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### âš™ï¸ Pipeline Configuration - Key Settings\n",
    "\n",
    "**Pipeline mode:**\n",
    "\n",
    "**Triggered mode:**\n",
    "* Runs on-demand or on schedule\n",
    "* Processes all available data\n",
    "* Stops after completion\n",
    "* Use for: Batch pipelines, scheduled jobs\n",
    "\n",
    "**Continuous mode:**\n",
    "* Runs continuously\n",
    "* Processes data as it arrives\n",
    "* Always running\n",
    "* Use for: Real-time pipelines, streaming data\n",
    "\n",
    "**For this demo:** Use **Triggered** mode\n",
    "\n",
    "---\n",
    "\n",
    "**Product edition:**\n",
    "\n",
    "**Core:**\n",
    "* Basic DLT features\n",
    "* Streaming tables and views\n",
    "* Good for development\n",
    "\n",
    "**Pro:**\n",
    "* Adds expectations (data quality)\n",
    "* SLA monitoring\n",
    "* Advanced features\n",
    "\n",
    "**Advanced:**\n",
    "* All Pro features\n",
    "* Enhanced performance\n",
    "* Production workloads\n",
    "\n",
    "**For this demo:** **Advanced** (or Core if Advanced not available)\n",
    "\n",
    "---\n",
    "\n",
    "**Target schema:**\n",
    "* Where tables will be created\n",
    "* Example: `main.default`\n",
    "* All DLT tables appear here\n",
    "* Can query tables after pipeline runs\n",
    "\n",
    "---\n",
    "\n",
    "**Compute:**\n",
    "* **Serverless** - Recommended, no cluster management\n",
    "* **Enhanced autoscaling** - Automatically scales workers\n",
    "* **Fixed size** - Specific number of workers\n",
    "\n",
    "**For this demo:** **Serverless** (if available) or **Enhanced autoscaling**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8759fadb-f654-440f-955c-cdb493b486a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ” Step 3: Review Pipeline Configuration\n",
    "\n",
    "**Follow these steps:**\n",
    "\n",
    "After creating the pipeline:\n",
    "\n",
    "1. You're taken to the **pipeline details page**\n",
    "2. Review the configuration:\n",
    "\n",
    "**Pipeline details page shows:**\n",
    "* ðŸ“ **Pipeline name** - \"Wanderbricks Analytics Pipeline\"\n",
    "* ðŸ’¾ **Source notebook** - Path to this notebook\n",
    "* ðŸŽ¯ **Target schema** - Where tables are created\n",
    "* ðŸ’» **Compute** - Cluster configuration\n",
    "* ðŸ“… **Mode** - Triggered or Continuous\n",
    "\n",
    "**Tabs available:**\n",
    "* **Graph** - Visual pipeline lineage (we'll explore this)\n",
    "* **Event log** - Execution history and logs\n",
    "* **Settings** - Edit configuration\n",
    "\n",
    "3. Click the **\"Settings\"** tab to review\n",
    "4. You can edit any setting here\n",
    "5. Click **\"Save\"** if you make changes\n",
    "\n",
    "âœ… **Checkpoint:** Pipeline configuration reviewed\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** You can edit pipeline settings anytime from the Settings tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "851ae379-ad64-43b6-a0e7-fabed6e00f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 6: Running Your Pipeline â–¶ï¸\n",
    "\n",
    "Let's execute the pipeline and see it in action!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b892b8a5-ec8f-4709-89a2-167c41fd306d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### â–¶ï¸ Step 4: Start the Pipeline\n",
    "\n",
    "**Follow these steps:**\n",
    "\n",
    "1. On the pipeline details page, click the **\"Start\"** button (top-right)\n",
    "2. The pipeline begins execution\n",
    "3. You'll see:\n",
    "   * Status changes to \"Starting\"\n",
    "   * Then \"Running\"\n",
    "   * Progress indicators appear\n",
    "\n",
    "**What happens when you start:**\n",
    "* âœ… Lakeflow reads your notebook\n",
    "* âœ… Parses all `@dlt.table` and `@dlt.view` declarations\n",
    "* âœ… Builds dependency graph\n",
    "* âœ… Provisions compute resources\n",
    "* âœ… Executes tables in correct order\n",
    "* âœ… Creates tables in target schema\n",
    "\n",
    "**First run:**\n",
    "* Takes longer (initial data load)\n",
    "* Creates all tables from scratch\n",
    "* Processes all historical data\n",
    "\n",
    "**Subsequent runs:**\n",
    "* Faster (incremental processing)\n",
    "* Only processes new data\n",
    "* Updates existing tables\n",
    "\n",
    "âœ… **Checkpoint:** Pipeline is running\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** First run may take 5-10 minutes depending on data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccccce0f-aa21-4345-ab94-fde4ac8dc4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ“Š Step 5: Explore the Pipeline Graph\n",
    "\n",
    "**Follow these steps:**\n",
    "\n",
    "1. Click the **\"Graph\"** tab (should be default view)\n",
    "2. You'll see a **visual representation** of your pipeline\n",
    "\n",
    "**What the graph shows:**\n",
    "\n",
    "**Nodes (tables/views):**\n",
    "* ðŸ¥‰ **Bronze tables** - Raw data sources\n",
    "* ðŸ¥ˆ **Silver tables** - Cleaned data\n",
    "* ðŸ¥‡ **Gold views** - Aggregated metrics\n",
    "* Each node shows table name\n",
    "\n",
    "**Edges (dependencies):**\n",
    "* **Arrows** show data flow\n",
    "* Bronze â†’ Silver â†’ Gold\n",
    "* Automatic dependency detection\n",
    "\n",
    "**Node colors/status:**\n",
    "* ðŸŸ¢ **Green** - Completed successfully\n",
    "* ðŸ”µ **Blue** - Currently running\n",
    "* ðŸŸ¡ **Yellow** - Queued/waiting\n",
    "* ðŸ”´ **Red** - Failed\n",
    "* âšª **Gray** - Not started\n",
    "\n",
    "**Interactive graph:**\n",
    "* Click on a node to see details\n",
    "* Zoom in/out\n",
    "* Pan around\n",
    "* Auto-layout\n",
    "\n",
    "**Try this:**\n",
    "1. Watch nodes change color as pipeline runs\n",
    "2. Click on a node to see:\n",
    "   * Table schema\n",
    "   * Row count\n",
    "   * Execution time\n",
    "   * Data quality metrics\n",
    "\n",
    "âœ… **Checkpoint:** Can view and understand pipeline graph\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** The graph is your pipeline documentation - shows data lineage visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cc30e97-ca1f-472a-b9fd-cf8e4c600dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ“Š Step 6: Monitor Pipeline Execution\n",
    "\n",
    "**Follow these steps:**\n",
    "\n",
    "While the pipeline runs:\n",
    "\n",
    "1. **Watch the graph:**\n",
    "   * Nodes turn blue (running) then green (completed)\n",
    "   * Progress flows from bronze â†’ silver â†’ gold\n",
    "\n",
    "2. **Check execution details:**\n",
    "   * Click on a running/completed node\n",
    "   * Side panel shows:\n",
    "     - **Status** - Running, completed, failed\n",
    "     - **Duration** - How long it took\n",
    "     - **Rows processed** - Number of records\n",
    "     - **Data quality** - Pass/fail metrics (if expectations exist)\n",
    "\n",
    "3. **View event log:**\n",
    "   * Click **\"Event log\"** tab\n",
    "   * Shows detailed execution logs\n",
    "   * Timestamps for each step\n",
    "   * Error messages (if any)\n",
    "\n",
    "**Pipeline execution order:**\n",
    "```\n",
    "1. Bronze tables (parallel)\n",
    "   â†“\n",
    "2. Silver tables (after bronze completes)\n",
    "   â†“\n",
    "3. Silver enriched (after silver completes)\n",
    "   â†“\n",
    "4. Gold views (after silver completes)\n",
    "```\n",
    "\n",
    "**Execution states:**\n",
    "* ðŸŸ¡ **Queued** - Waiting for dependencies\n",
    "* ðŸ”µ **Running** - Currently processing\n",
    "* ðŸŸ¢ **Completed** - Successfully finished\n",
    "* ðŸ”´ **Failed** - Error occurred\n",
    "\n",
    "âœ… **Checkpoint:** Pipeline execution monitored\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Event log is crucial for debugging - shows detailed execution info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c2c0c7-0fe0-4cc6-8f14-f56e58f648f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ“Š Step 7: View Pipeline Results\n",
    "\n",
    "**Follow these steps:**\n",
    "\n",
    "After pipeline completes:\n",
    "\n",
    "1. **Check final status:**\n",
    "   * All nodes should be green\n",
    "   * Status shows \"Succeeded\"\n",
    "   * Total execution time displayed\n",
    "\n",
    "2. **View table details:**\n",
    "   * Click on any node in the graph\n",
    "   * Side panel shows:\n",
    "     - **Row count** - Number of records\n",
    "     - **Schema** - Column names and types\n",
    "     - **Sample data** - Preview of data\n",
    "     - **Lineage** - Upstream and downstream tables\n",
    "\n",
    "3. **Query the tables:**\n",
    "   * Tables are created in your target schema\n",
    "   * Open a SQL query or notebook\n",
    "   * Query the tables:\n",
    "   ```sql\n",
    "   -- Bronze\n",
    "   SELECT * FROM main.default.bronze_bookings LIMIT 10;\n",
    "   \n",
    "   -- Silver\n",
    "   SELECT * FROM main.default.silver_bookings LIMIT 10;\n",
    "   \n",
    "   -- Gold\n",
    "   SELECT * FROM main.default.gold_daily_bookings;\n",
    "   ```\n",
    "\n",
    "4. **Verify data flow:**\n",
    "   * Bronze has raw data\n",
    "   * Silver has cleaned data (fewer rows)\n",
    "   * Gold has aggregated data (much fewer rows)\n",
    "\n",
    "âœ… **Checkpoint:** Can query pipeline output tables\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** DLT tables are regular Delta tables - query them like any other table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f119538f-9143-43ed-a82a-e16cfb974462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸŽ¯ Practice Challenges\n",
    "\n",
    "Now it's your turn! Complete these challenges to reinforce your learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89a9de35-2448-4762-aa5f-0615a2b4c379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŽ¯ Challenge 1: Add a New Bronze Table\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Add a bronze table for the users data.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a streaming table named `bronze_users`\n",
    "2. Ingest from `samples.wanderbricks.users`\n",
    "3. Add appropriate comment\n",
    "4. Use `@dlt.table` decorator\n",
    "5. Use `spark.readStream.table()`\n",
    "\n",
    "**Expected code structure:**\n",
    "```python\n",
    "@dlt.table(\n",
    "    name=\"bronze_users\",\n",
    "    comment=\"Your comment here\"\n",
    ")\n",
    "def bronze_users():\n",
    "    return spark.readStream.table(\"samples.wanderbricks.users\")\n",
    "```\n",
    "\n",
    "**After adding:**\n",
    "1. Save the notebook\n",
    "2. Go to pipeline UI\n",
    "3. Click \"Start\" to run\n",
    "4. Verify new table appears in graph\n",
    "\n",
    "âœ… **Success criteria:** bronze_users table created and visible in pipeline graph\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbbc6257-2b65-4f51-b3b9-a68021c4d7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŽ¯ Challenge 2: Create Silver Users Table\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Create a cleaned silver table for users.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create streaming table named `silver_users`\n",
    "2. Read from `bronze_users` using `dlt.read_stream()`\n",
    "3. Apply data quality filters:\n",
    "   * Filter out null emails\n",
    "   * Filter out null names\n",
    "4. Add calculated column:\n",
    "   * `account_age_days` = DATEDIFF(CURRENT_DATE(), created_at)\n",
    "5. Select relevant columns\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "@dlt.table(\n",
    "    name=\"silver_users\",\n",
    "    comment=\"Cleaned user data - Silver layer\"\n",
    ")\n",
    "def silver_users():\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_users\")\n",
    "        .filter(col(\"email\").isNotNull())\n",
    "        .filter(col(\"name\").isNotNull())\n",
    "        .withColumn(\"account_age_days\", datediff(current_date(), col(\"created_at\")))\n",
    "        .select(\"user_id\", \"email\", \"name\", \"country\", \"user_type\", \n",
    "                \"created_at\", \"account_age_days\")\n",
    "    )\n",
    "```\n",
    "\n",
    "**After adding:**\n",
    "1. Save notebook\n",
    "2. Run pipeline\n",
    "3. Verify silver_users appears in graph\n",
    "4. Check it connects to bronze_users\n",
    "\n",
    "âœ… **Success criteria:** silver_users table with data quality filters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b296275-7dfa-4e81-acac-b3b94c16eefd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŽ¯ Challenge 3: Create Gold Metrics View\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Create a gold materialized view with user metrics.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create materialized view named `gold_user_metrics`\n",
    "2. Use `@dlt.view` decorator (not @dlt.table)\n",
    "3. Read from `silver_users` using `dlt.read()` (not read_stream)\n",
    "4. Group by `country` and `user_type`\n",
    "5. Calculate:\n",
    "   * Total user count\n",
    "   * Average account age\n",
    "   * Business user count\n",
    "6. Order by total users descending\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "@dlt.view(\n",
    "    name=\"gold_user_metrics\",\n",
    "    comment=\"User metrics by country and type - Gold layer\"\n",
    ")\n",
    "def gold_user_metrics():\n",
    "    return (\n",
    "        dlt.read(\"silver_users\")\n",
    "        .groupBy(\"country\", \"user_type\")\n",
    "        .agg(\n",
    "            count(\"user_id\").alias(\"total_users\"),\n",
    "            avg(\"account_age_days\").alias(\"avg_account_age_days\"),\n",
    "            sum(when(col(\"is_business\") == True, 1).otherwise(0)).alias(\"business_users\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_users\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "**After adding:**\n",
    "1. Save notebook\n",
    "2. Run pipeline\n",
    "3. Verify gold_user_metrics appears\n",
    "4. Query the view to see results\n",
    "\n",
    "âœ… **Success criteria:** gold_user_metrics view with aggregated data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f23f0b-b286-43ad-91e7-8f54ea1993a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŽ¯ Challenge 4: Build Complete End-to-End Flow\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Create a complete bronze â†’ silver â†’ gold flow for a new entity.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "Build a pipeline for **page views** data:\n",
    "\n",
    "**Bronze:**\n",
    "```python\n",
    "@dlt.table(name=\"bronze_page_views\", comment=\"Raw page view data\")\n",
    "def bronze_page_views():\n",
    "    return spark.readStream.table(\"samples.wanderbricks.page_views\")\n",
    "```\n",
    "\n",
    "**Silver:**\n",
    "```python\n",
    "@dlt.table(name=\"silver_page_views\", comment=\"Cleaned page views\")\n",
    "def silver_page_views():\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_page_views\")\n",
    "        .filter(col(\"user_id\").isNotNull())\n",
    "        .filter(col(\"property_id\").isNotNull())\n",
    "        .withColumn(\"view_date\", to_date(col(\"timestamp\")))\n",
    "        .withColumn(\"view_hour\", hour(col(\"timestamp\")))\n",
    "        .select(\"view_id\", \"user_id\", \"property_id\", \"page_url\", \n",
    "                \"device_type\", \"referrer\", \"view_date\", \"view_hour\", \"timestamp\")\n",
    "    )\n",
    "```\n",
    "\n",
    "**Gold:**\n",
    "```python\n",
    "@dlt.view(name=\"gold_page_view_metrics\", comment=\"Page view metrics\")\n",
    "def gold_page_view_metrics():\n",
    "    return (\n",
    "        dlt.read(\"silver_page_views\")\n",
    "        .groupBy(\"view_date\", \"device_type\")\n",
    "        .agg(\n",
    "            count(\"view_id\").alias(\"total_views\"),\n",
    "            countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "            countDistinct(\"property_id\").alias(\"properties_viewed\")\n",
    "        )\n",
    "        .orderBy(\"view_date\", \"device_type\")\n",
    "    )\n",
    "```\n",
    "\n",
    "**After adding:**\n",
    "1. Add all three tables to the notebook\n",
    "2. Save notebook\n",
    "3. Run pipeline\n",
    "4. Verify all 3 tables appear in graph\n",
    "5. Check dependencies are correct\n",
    "6. Query gold table to see metrics\n",
    "\n",
    "âœ… **Success criteria:** Complete bronze â†’ silver â†’ gold flow for page views\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb2f024-d244-496e-9faf-e5b5038f5de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“š Lakeflow Best Practices\n",
    "\n",
    "### **Table Type Selection**\n",
    "\n",
    "**Use Streaming Tables when:**\n",
    "* âœ… Bronze layer (always)\n",
    "* âœ… Silver layer (usually)\n",
    "* âœ… Append-only data\n",
    "* âœ… Large data volumes\n",
    "* âœ… Need low latency\n",
    "* âœ… Incremental processing\n",
    "\n",
    "**Use Materialized Views when:**\n",
    "* âœ… Gold layer aggregations\n",
    "* âœ… Complex joins\n",
    "* âœ… Small result sets\n",
    "* âœ… Full refresh is acceptable\n",
    "* âœ… Aggregations with GROUP BY\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Organization**\n",
    "\n",
    "**Structure your notebook:**\n",
    "```python\n",
    "# 1. Imports at top\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 2. Bronze layer\n",
    "@dlt.table\n",
    "def bronze_table1(): ...\n",
    "\n",
    "@dlt.table\n",
    "def bronze_table2(): ...\n",
    "\n",
    "# 3. Silver layer\n",
    "@dlt.table\n",
    "def silver_table1(): ...\n",
    "\n",
    "# 4. Gold layer\n",
    "@dlt.view\n",
    "def gold_view1(): ...\n",
    "```\n",
    "\n",
    "**Naming conventions:**\n",
    "* âœ… Prefix with layer: `bronze_`, `silver_`, `gold_`\n",
    "* âœ… Use descriptive names: `silver_bookings_enriched`\n",
    "* âœ… Be consistent across pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### **Data Quality**\n",
    "\n",
    "**Bronze layer:**\n",
    "* âœ… No filtering (keep all data)\n",
    "* âœ… Add metadata columns if needed\n",
    "* âœ… Preserve raw data\n",
    "\n",
    "**Silver layer:**\n",
    "* âœ… Filter invalid data\n",
    "* âœ… Remove nulls where appropriate\n",
    "* âœ… Validate business rules\n",
    "* âœ… Add calculated columns\n",
    "* âœ… Deduplicate if needed\n",
    "\n",
    "**Gold layer:**\n",
    "* âœ… Aggregate clean data\n",
    "* âœ… Create business metrics\n",
    "* âœ… Optimize for queries\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance**\n",
    "\n",
    "**Optimize streaming tables:**\n",
    "* âœ… Use appropriate partition columns\n",
    "* âœ… Limit data scanned with filters\n",
    "* âœ… Use broadcast joins for small dimensions\n",
    "* âœ… Enable Photon acceleration\n",
    "\n",
    "**Optimize materialized views:**\n",
    "* âœ… Aggregate early\n",
    "* âœ… Use efficient joins\n",
    "* âœ… Limit result set size\n",
    "* âœ… Use appropriate data types\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Configuration**\n",
    "\n",
    "**Development:**\n",
    "* âœ… Use Triggered mode\n",
    "* âœ… Smaller cluster (1-2 workers)\n",
    "* âœ… Development channel\n",
    "\n",
    "**Production:**\n",
    "* âœ… Use Continuous mode (if real-time needed)\n",
    "* âœ… Enhanced autoscaling\n",
    "* âœ… Serverless (if available)\n",
    "* âœ… Current channel\n",
    "* âœ… Enable monitoring\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3ff465-5249-476d-855f-4b6199664d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ’¡ Key Concepts Recap\n",
    "\n",
    "### **Declarative vs Imperative**\n",
    "\n",
    "**Imperative (Traditional):**\n",
    "```python\n",
    "# You manage everything\n",
    "df = spark.read.table(\"source\")\n",
    "df_cleaned = df.filter(...)\n",
    "df_cleaned.write.mode(\"append\").saveAsTable(\"target\")\n",
    "# Handle checkpoints, retries, dependencies manually\n",
    "```\n",
    "\n",
    "**Declarative (Lakeflow):**\n",
    "```python\n",
    "# You declare what you want\n",
    "@dlt.table\n",
    "def target():\n",
    "    return dlt.read_stream(\"source\").filter(...)\n",
    "# Lakeflow handles everything else automatically\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Automatic Features**\n",
    "\n",
    "**Lakeflow automatically handles:**\n",
    "* âœ… **Dependencies** - Runs tables in correct order\n",
    "* âœ… **Incremental processing** - Only processes new data\n",
    "* âœ… **Checkpoints** - Tracks progress\n",
    "* âœ… **Retries** - Handles failures\n",
    "* âœ… **Scaling** - Adjusts compute resources\n",
    "* âœ… **Monitoring** - Tracks metrics\n",
    "* âœ… **Lineage** - Shows data flow\n",
    "\n",
    "---\n",
    "\n",
    "### **Reading Data in DLT**\n",
    "\n",
    "**From source tables:**\n",
    "```python\n",
    "# Streaming\n",
    "spark.readStream.table(\"source\")\n",
    "\n",
    "# Batch\n",
    "spark.read.table(\"source\")\n",
    "```\n",
    "\n",
    "**From DLT tables:**\n",
    "```python\n",
    "# Streaming (for streaming tables)\n",
    "dlt.read_stream(\"upstream_table\")\n",
    "\n",
    "# Batch (for materialized views)\n",
    "dlt.read(\"upstream_table\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Medallion Architecture in DLT**\n",
    "\n",
    "```\n",
    "ðŸ¥‰ Bronze (Streaming Tables)\n",
    "   - Raw data ingestion\n",
    "   - spark.readStream.table()\n",
    "   - No transformations\n",
    "   - Append-only\n",
    "   â†“\n",
    "ðŸ¥ˆ Silver (Streaming Tables)\n",
    "   - Data cleaning\n",
    "   - dlt.read_stream()\n",
    "   - Filters and validations\n",
    "   - Calculated columns\n",
    "   â†“\n",
    "ðŸ¥‡ Gold (Materialized Views)\n",
    "   - Aggregations\n",
    "   - dlt.read()\n",
    "   - Business metrics\n",
    "   - Dashboard-ready\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05c6d221-6f1c-4520-a073-ca3346e10ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“Š Complete Pipeline Overview\n",
    "\n",
    "**Your pipeline now has:**\n",
    "\n",
    "### **Bronze Layer (3 streaming tables)**\n",
    "* `bronze_bookings` - Raw bookings\n",
    "* `bronze_properties` - Raw properties\n",
    "* `bronze_reviews` - Raw reviews\n",
    "\n",
    "### **Silver Layer (4 streaming tables)**\n",
    "* `silver_bookings` - Cleaned bookings\n",
    "* `silver_properties` - Cleaned properties\n",
    "* `silver_reviews` - Cleaned reviews\n",
    "* `silver_bookings_enriched` - Bookings + property details\n",
    "\n",
    "### **Gold Layer (3 materialized views)**\n",
    "* `gold_daily_bookings` - Daily metrics by property type\n",
    "* `gold_property_performance` - Property type performance\n",
    "* `gold_review_summary` - Review ratings over time\n",
    "\n",
    "**Plus challenges (if completed):**\n",
    "* `bronze_users`, `silver_users`, `gold_user_metrics`\n",
    "* `bronze_page_views`, `silver_page_views`, `gold_page_view_metrics`\n",
    "\n",
    "---\n",
    "\n",
    "### **Data Flow:**\n",
    "\n",
    "```\n",
    "samples.wanderbricks.bookings\n",
    "    â†“ (readStream)\n",
    "bronze_bookings\n",
    "    â†“ (read_stream + filter)\n",
    "silver_bookings\n",
    "    â†“ (read_stream + join)\n",
    "silver_bookings_enriched\n",
    "    â†“ (read + groupBy)\n",
    "gold_daily_bookings\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Characteristics:**\n",
    "\n",
    "* âœ… **10+ tables/views** (base + challenges)\n",
    "* âœ… **Automatic dependencies** - Lakeflow manages order\n",
    "* âœ… **Incremental processing** - Streaming tables\n",
    "* âœ… **Data quality** - Filters and validations\n",
    "* âœ… **Business metrics** - Gold aggregations\n",
    "* âœ… **Visual lineage** - Graph shows flow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d064c6b4-aa0a-4b79-9ec1-0c6d78ce7f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ” Querying Your Pipeline Results\n",
    "\n",
    "**After pipeline completes, query the tables:**\n",
    "\n",
    "### **Bronze tables:**\n",
    "```sql\n",
    "-- Raw data (all records)\n",
    "SELECT COUNT(*) FROM main.default.bronze_bookings;\n",
    "SELECT * FROM main.default.bronze_bookings LIMIT 10;\n",
    "```\n",
    "\n",
    "### **Silver tables:**\n",
    "```sql\n",
    "-- Cleaned data (fewer records)\n",
    "SELECT COUNT(*) FROM main.default.silver_bookings;\n",
    "SELECT * FROM main.default.silver_bookings_enriched LIMIT 10;\n",
    "```\n",
    "\n",
    "### **Gold views:**\n",
    "```sql\n",
    "-- Aggregated metrics\n",
    "SELECT * FROM main.default.gold_daily_bookings \n",
    "ORDER BY booking_date DESC \n",
    "LIMIT 30;\n",
    "\n",
    "SELECT * FROM main.default.gold_property_performance;\n",
    "\n",
    "SELECT * FROM main.default.gold_review_summary \n",
    "WHERE review_year = 2024;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Verify data quality:**\n",
    "```sql\n",
    "-- Compare row counts\n",
    "SELECT 'Bronze' AS layer, COUNT(*) AS row_count \n",
    "FROM main.default.bronze_bookings\n",
    "UNION ALL\n",
    "SELECT 'Silver' AS layer, COUNT(*) AS row_count \n",
    "FROM main.default.silver_bookings\n",
    "UNION ALL\n",
    "SELECT 'Gold' AS layer, COUNT(*) AS row_count \n",
    "FROM main.default.gold_daily_bookings;\n",
    "```\n",
    "\n",
    "**Expected results:**\n",
    "* Bronze: Most rows (all data)\n",
    "* Silver: Fewer rows (filtered)\n",
    "* Gold: Much fewer rows (aggregated)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Use these tables in dashboards, queries, and downstream applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa119fe8-6c37-44f1-b42e-c7d970267e80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ”„ Updating Your Pipeline\n",
    "\n",
    "**How to make changes:**\n",
    "\n",
    "### **Step 1: Edit the notebook**\n",
    "1. Open this notebook\n",
    "2. Add, modify, or remove `@dlt.table` or `@dlt.view` definitions\n",
    "3. Save the notebook\n",
    "\n",
    "### **Step 2: Run the pipeline**\n",
    "1. Go to the pipeline UI\n",
    "2. Click **\"Start\"**\n",
    "3. Lakeflow detects changes automatically\n",
    "4. Updates affected tables\n",
    "\n",
    "**What Lakeflow does:**\n",
    "* âœ… Detects new tables (adds them)\n",
    "* âœ… Detects modified tables (updates them)\n",
    "* âœ… Detects removed tables (keeps them, doesn't delete)\n",
    "* âœ… Updates dependencies automatically\n",
    "* âœ… Processes only affected tables\n",
    "\n",
    "---\n",
    "\n",
    "### **Common updates:**\n",
    "\n",
    "**Add a new table:**\n",
    "```python\n",
    "@dlt.table\n",
    "def new_table():\n",
    "    return dlt.read_stream(\"source\").filter(...)\n",
    "```\n",
    "* Save notebook\n",
    "* Run pipeline\n",
    "* New table appears in graph\n",
    "\n",
    "**Modify transformation:**\n",
    "```python\n",
    "@dlt.table\n",
    "def silver_bookings():\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_bookings\")\n",
    "        .filter(col(\"status\") == \"confirmed\")\n",
    "        .filter(col(\"total_amount\") > 100)  # NEW FILTER\n",
    "    )\n",
    "```\n",
    "* Save notebook\n",
    "* Run pipeline\n",
    "* Table is reprocessed with new logic\n",
    "\n",
    "**Add calculated column:**\n",
    "```python\n",
    ".withColumn(\"new_column\", expr(\"calculation\"))\n",
    "```\n",
    "* Save notebook\n",
    "* Run pipeline\n",
    "* Schema updates automatically\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Lakeflow handles schema evolution automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7829894-96b6-4d0b-a118-11c065942c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ”§ Troubleshooting Common Issues\n",
    "\n",
    "**Pipeline won't start:**\n",
    "* âœ… Check notebook is saved\n",
    "* âœ… Verify notebook path in pipeline settings\n",
    "* âœ… Ensure target schema exists\n",
    "* âœ… Check permissions on source tables\n",
    "* âœ… Verify SQL warehouse or cluster is available\n",
    "\n",
    "**Table shows as failed (red):**\n",
    "* âœ… Click the node to see error message\n",
    "* âœ… Check Event log for details\n",
    "* âœ… Common issues:\n",
    "   - Syntax errors in code\n",
    "   - Invalid column references\n",
    "   - Source table doesn't exist\n",
    "   - Permission issues\n",
    "* âœ… Fix code and rerun\n",
    "\n",
    "**Dependencies not detected:**\n",
    "* âœ… Use `dlt.read()` or `dlt.read_stream()` (not spark.read)\n",
    "* âœ… Check table names match exactly\n",
    "* âœ… Verify decorator syntax is correct\n",
    "\n",
    "**Streaming table not processing incrementally:**\n",
    "* âœ… Verify using `readStream` (not `read`)\n",
    "* âœ… Check source supports streaming\n",
    "* âœ… Ensure checkpoint location is accessible\n",
    "\n",
    "**Materialized view taking too long:**\n",
    "* âœ… Check if aggregating too much data\n",
    "* âœ… Add filters to reduce data volume\n",
    "* âœ… Consider using streaming table instead\n",
    "* âœ… Optimize source queries\n",
    "\n",
    "**Schema evolution issues:**\n",
    "* âœ… DLT handles schema evolution automatically\n",
    "* âœ… Check if column types are compatible\n",
    "* âœ… Review schema in table details\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tip:** Event log is your best friend for debugging - shows detailed error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba021a6-48cd-475c-a710-93fbcb2d1a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“Š Quick Reference Guide\n",
    "\n",
    "### **Decorators**\n",
    "```python\n",
    "# Streaming table\n",
    "@dlt.table(name=\"table_name\", comment=\"Description\")\n",
    "def table_name():\n",
    "    return spark.readStream.table(\"source\")\n",
    "\n",
    "# Materialized view\n",
    "@dlt.view(name=\"view_name\", comment=\"Description\")\n",
    "def view_name():\n",
    "    return dlt.read(\"source\").groupBy(...).agg(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Reading Data**\n",
    "```python\n",
    "# From source (streaming)\n",
    "spark.readStream.table(\"catalog.schema.table\")\n",
    "\n",
    "# From source (batch)\n",
    "spark.read.table(\"catalog.schema.table\")\n",
    "\n",
    "# From DLT table (streaming)\n",
    "dlt.read_stream(\"dlt_table_name\")\n",
    "\n",
    "# From DLT table (batch)\n",
    "dlt.read(\"dlt_table_name\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Patterns**\n",
    "```python\n",
    "# Bronze: Ingest raw\n",
    "@dlt.table\n",
    "def bronze_table():\n",
    "    return spark.readStream.table(\"source\")\n",
    "\n",
    "# Silver: Clean and transform\n",
    "@dlt.table\n",
    "def silver_table():\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_table\")\n",
    "        .filter(col(\"column\").isNotNull())\n",
    "        .withColumn(\"new_col\", expr(\"calculation\"))\n",
    "    )\n",
    "\n",
    "# Gold: Aggregate\n",
    "@dlt.view\n",
    "def gold_metrics():\n",
    "    return (\n",
    "        dlt.read(\"silver_table\")\n",
    "        .groupBy(\"dimension\")\n",
    "        .agg(count(\"*\").alias(\"total\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline UI**\n",
    "* **Create:** Workflows â†’ Delta Live Tables â†’ Create Pipeline\n",
    "* **Configure:** Set notebook path, target schema, compute\n",
    "* **Run:** Click Start button\n",
    "* **Monitor:** Graph tab shows visual lineage\n",
    "* **Debug:** Event log shows execution details\n",
    "* **Query:** Tables created in target schema\n",
    "\n",
    "---\n",
    "\n",
    "### **Layer Guidelines**\n",
    "\n",
    "| Layer | Table Type | Read Method | Purpose |\n",
    "|-------|-----------|-------------|----------|\n",
    "| Bronze | Streaming | readStream | Raw ingestion |\n",
    "| Silver | Streaming | read_stream | Cleaning |\n",
    "| Gold | Materialized | read | Aggregations |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8272e32c-4c1b-459a-9616-b2ea140da9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the Lakeflow Pipeline Fundamentals demo!\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "âœ… **Lakeflow concepts** - Declarative pipeline framework  \n",
    "âœ… **Streaming tables** - Incremental processing  \n",
    "âœ… **Materialized views** - Full refresh aggregations  \n",
    "âœ… **Bronze layer** - Raw data ingestion  \n",
    "âœ… **Silver layer** - Data cleaning and validation  \n",
    "âœ… **Gold layer** - Business metrics  \n",
    "âœ… **Pipeline creation** - UI configuration  \n",
    "âœ… **Pipeline execution** - Running and monitoring  \n",
    "âœ… **Data lineage** - Visual graph  \n",
    "âœ… **Best practices** - Production patterns  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Declarative is simpler** - Define what, not how\n",
    "2. **Streaming tables for incremental** - Bronze and silver\n",
    "3. **Materialized views for aggregations** - Gold layer\n",
    "4. **Dependencies are automatic** - No manual orchestration\n",
    "5. **Graph shows lineage** - Visual data flow\n",
    "6. **Incremental by default** - Efficient processing\n",
    "7. **Production-ready** - Built-in reliability\n",
    "\n",
    "---\n",
    "\n",
    "### **Your Pipeline:**\n",
    "\n",
    "```\n",
    "ðŸ“ samples.wanderbricks (source)\n",
    "         â†“\n",
    "ðŸ¥‰ Bronze Layer (3 streaming tables)\n",
    "   - bronze_bookings\n",
    "   - bronze_properties\n",
    "   - bronze_reviews\n",
    "         â†“\n",
    "ðŸ¥ˆ Silver Layer (4 streaming tables)\n",
    "   - silver_bookings\n",
    "   - silver_properties\n",
    "   - silver_reviews\n",
    "   - silver_bookings_enriched\n",
    "         â†“\n",
    "ðŸ¥‡ Gold Layer (3 materialized views)\n",
    "   - gold_daily_bookings\n",
    "   - gold_property_performance\n",
    "   - gold_review_summary\n",
    "         â†“\n",
    "    ðŸ“Š Dashboards & Analytics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What We Didn't Cover (Future Demos):**\n",
    "\n",
    "* âš ï¸ Expectations (data quality checks)\n",
    "* ðŸ“… Job scheduling\n",
    "* ðŸ”” Alerting and monitoring\n",
    "* ðŸ”„ Change Data Capture (CDC)\n",
    "* ðŸ›¡ï¸ Advanced error handling\n",
    "* ðŸ“Š Performance tuning\n",
    "* ðŸ”— External data sources\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "* Build pipelines for your own data\n",
    "* Explore expectations for data quality\n",
    "* Learn about CDC patterns\n",
    "* Set up production pipelines\n",
    "* Create dashboards from gold tables\n",
    "* Schedule pipeline runs\n",
    "* Monitor pipeline health\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources:**\n",
    "\n",
    "* [Delta Live Tables Documentation](https://docs.databricks.com/delta-live-tables/index.html)\n",
    "* [DLT Python API Reference](https://docs.databricks.com/delta-live-tables/python-ref.html)\n",
    "* [DLT Best Practices](https://docs.databricks.com/delta-live-tables/best-practices.html)\n",
    "* [Streaming Tables vs Views](https://docs.databricks.com/delta-live-tables/tables.html)\n",
    "\n",
    "---\n",
    "\n",
    "**You're now ready to build production data pipelines with Lakeflow!** ðŸš€\n",
    "\n",
    "*Happy pipeline building!*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lakeflow Pipeline Fundamentals",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
