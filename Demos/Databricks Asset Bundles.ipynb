{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94f20309-94be-4cf3-9b93-45bdb370ab8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Databricks Asset Bundles: Infrastructure as Code ğŸ¯\n",
    "\n",
    "Welcome to the **Databricks Asset Bundles (DABs)** demo! This notebook teaches you how to use bundles to manage your Databricks projects with software engineering best practices.\n",
    "\n",
    "---\n",
    "\n",
    "## What you'll learn:\n",
    "\n",
    "* ğŸ“š **Part 1:** Understanding Asset Bundles concepts\n",
    "* ğŸ—ï¸ **Part 2:** Bundle structure and configuration\n",
    "* ğŸ› ï¸ **Part 3:** Creating your first bundle\n",
    "* ğŸš€ **Part 4:** Deploying and managing bundles\n",
    "* ğŸ”„ **Part 5:** CI/CD with bundles\n",
    "* âœ… **Part 6:** Best practices and patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "* Databricks CLI installed locally\n",
    "* Git installed (for version control)\n",
    "* Basic understanding of YAML\n",
    "* Familiarity with Databricks Jobs and Pipelines\n",
    "* Completed previous Lakeflow demos (recommended)\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc4f2e7a-f4e1-4e85-9986-8d2bf39ea84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 1: Understanding Databricks Asset Bundles ğŸ“š\n",
    "\n",
    "Before creating bundles, let's understand what they are and why they matter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6af3e52-07e8-44fd-a56b-ec4d6aff6b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¯ What are Databricks Asset Bundles?\n",
    "\n",
    "**Definition:**\n",
    "* **Infrastructure as Code (IaC)** for Databricks\n",
    "* Package your entire project as a deployable unit\n",
    "* Include notebooks, jobs, pipelines, and configurations\n",
    "* Enable CI/CD and software engineering best practices\n",
    "* Version control everything\n",
    "\n",
    "---\n",
    "\n",
    "### **The Problem Without Bundles:**\n",
    "\n",
    "**Manual management:**\n",
    "* âŒ Click through UI to create jobs and pipelines\n",
    "* âŒ Hard to track changes over time\n",
    "* âŒ Difficult to promote from dev to prod\n",
    "* âŒ No code review for infrastructure changes\n",
    "* âŒ Inconsistent configurations across environments\n",
    "* âŒ Manual deployment process\n",
    "\n",
    "---\n",
    "\n",
    "### **The Solution With Bundles:**\n",
    "\n",
    "**Automated, version-controlled infrastructure:**\n",
    "* âœ… Define everything in code (YAML files)\n",
    "* âœ… Version control with Git\n",
    "* âœ… Code review for all changes\n",
    "* âœ… Automated deployment with CI/CD\n",
    "* âœ… Consistent environments (dev, staging, prod)\n",
    "* âœ… Easy rollback and disaster recovery\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Benefits:**\n",
    "\n",
    "* ğŸ“ **Version Control** - Track all changes in Git\n",
    "* ğŸ”„ **Reproducibility** - Recreate environments easily\n",
    "* ğŸš€ **CI/CD** - Automated testing and deployment\n",
    "* ğŸ‘¥ **Collaboration** - Code review and team workflows\n",
    "* ğŸ¯ **Consistency** - Same structure across projects\n",
    "* ğŸ”’ **Governance** - Audit trail and compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4244cb3-90e9-4da2-8f10-2bd5a231fb61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ§© What's Inside a Bundle?\n",
    "\n",
    "A Databricks Asset Bundle contains:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Configuration Files (YAML)**\n",
    "\n",
    "**databricks.yml:**\n",
    "* Main configuration file (required)\n",
    "* Defines bundle name and targets\n",
    "* References other configuration files\n",
    "\n",
    "**Resource files:**\n",
    "* Job definitions (`.job.yml`)\n",
    "* Pipeline definitions (`.pipeline.yml`)\n",
    "* Model serving endpoints\n",
    "* Experiments and models\n",
    "\n",
    "**Example structure:**\n",
    "```\n",
    "my_project/\n",
    "â”œâ”€â”€ databricks.yml          # Main config\n",
    "â”œâ”€â”€ resources/\n",
    "â”‚   â”œâ”€â”€ jobs/\n",
    "â”‚   â”‚   â””â”€â”€ etl_job.yml    # Job definition\n",
    "â”‚   â””â”€â”€ pipelines/\n",
    "â”‚       â””â”€â”€ data_pipeline.yml  # Pipeline definition\n",
    "â””â”€â”€ src/\n",
    "    â””â”€â”€ notebooks/\n",
    "        â””â”€â”€ transform.py    # Source code\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Source Code**\n",
    "\n",
    "**Notebooks:**\n",
    "* Python, SQL, Scala, R notebooks\n",
    "* Business logic and transformations\n",
    "\n",
    "**Python packages:**\n",
    "* Custom libraries and modules\n",
    "* Reusable functions\n",
    "\n",
    "**SQL files:**\n",
    "* Queries and DDL statements\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Targets (Environments)**\n",
    "\n",
    "**What they are:**\n",
    "* Different deployment environments\n",
    "* Each target has its own configuration\n",
    "\n",
    "**Common targets:**\n",
    "* `dev` - Development environment\n",
    "* `staging` - Pre-production testing\n",
    "* `prod` - Production environment\n",
    "\n",
    "**Example:**\n",
    "```yaml\n",
    "targets:\n",
    "  dev:\n",
    "    default: true\n",
    "    workspace:\n",
    "      host: https://dev.cloud.databricks.com\n",
    "  \n",
    "  prod:\n",
    "    workspace:\n",
    "      host: https://prod.cloud.databricks.com\n",
    "    run_as:\n",
    "      service_principal_name: prod-sp\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Variables**\n",
    "\n",
    "**What they are:**\n",
    "* Parameterized values\n",
    "* Different per environment\n",
    "* Reusable across resources\n",
    "\n",
    "**Example:**\n",
    "```yaml\n",
    "variables:\n",
    "  catalog:\n",
    "    description: Unity Catalog name\n",
    "    default: dev_catalog\n",
    "  \n",
    "  schema:\n",
    "    description: Schema name\n",
    "    default: bronze\n",
    "\n",
    "targets:\n",
    "  prod:\n",
    "    variables:\n",
    "      catalog: prod_catalog\n",
    "      schema: gold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb6f924e-8677-4f15-8ece-70ffa18c3ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”„ Bundle Development Workflow\n",
    "\n",
    "**Typical workflow with bundles:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Initialize**\n",
    "\n",
    "```bash\n",
    "# Create a new bundle from template\n",
    "databricks bundle init\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "* Creates project structure\n",
    "* Generates configuration files\n",
    "* Sets up source directories\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Develop**\n",
    "\n",
    "```bash\n",
    "# Edit configuration files\n",
    "vim databricks.yml\n",
    "vim resources/my_job.yml\n",
    "\n",
    "# Write source code\n",
    "vim src/notebooks/transform.py\n",
    "```\n",
    "\n",
    "**What you do:**\n",
    "* Define jobs, pipelines, resources\n",
    "* Write notebooks and code\n",
    "* Configure variables and settings\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Validate**\n",
    "\n",
    "```bash\n",
    "# Check configuration is valid\n",
    "databricks bundle validate\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "* Validates YAML syntax\n",
    "* Checks resource definitions\n",
    "* Verifies references\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Deploy**\n",
    "\n",
    "```bash\n",
    "# Deploy to dev environment\n",
    "databricks bundle deploy -t dev\n",
    "\n",
    "# Deploy to production\n",
    "databricks bundle deploy -t prod\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "* Uploads source files to workspace\n",
    "* Creates/updates jobs and pipelines\n",
    "* Applies configurations\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Run**\n",
    "\n",
    "```bash\n",
    "# Run a job from the bundle\n",
    "databricks bundle run my_job -t dev\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "* Triggers job execution\n",
    "* Uses deployed configuration\n",
    "* Returns run status\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Iterate**\n",
    "\n",
    "```bash\n",
    "# Make changes\n",
    "vim resources/my_job.yml\n",
    "\n",
    "# Validate and redeploy\n",
    "databricks bundle validate\n",
    "databricks bundle deploy -t dev\n",
    "```\n",
    "\n",
    "**Continuous improvement:**\n",
    "* Update configurations\n",
    "* Add new resources\n",
    "* Refine and optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023a2a6f-4949-45f6-9808-911cfcd77146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 2: Bundle Structure and Configuration ğŸ—ï¸\n",
    "\n",
    "Let's dive deep into bundle structure and configuration files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5fd22ce-8030-4ad5-be30-bad05926f97c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ“ Recommended Project Structure\n",
    "\n",
    "**Standard bundle layout:**\n",
    "\n",
    "```\n",
    "my_project/\n",
    "â”œâ”€â”€ databricks.yml              # Main bundle configuration\n",
    "â”œâ”€â”€ README.md                   # Project documentation\n",
    "â”œâ”€â”€ .gitignore                  # Git ignore file\n",
    "â”‚\n",
    "â”œâ”€â”€ resources/                  # Resource definitions\n",
    "â”‚   â”œâ”€â”€ jobs/\n",
    "â”‚   â”‚   â”œâ”€â”€ etl_job.yml        # Job configurations\n",
    "â”‚   â”‚   â””â”€â”€ ml_training.yml\n",
    "â”‚   â”œâ”€â”€ pipelines/\n",
    "â”‚   â”‚   â””â”€â”€ data_pipeline.yml  # DLT pipeline configs\n",
    "â”‚   â””â”€â”€ experiments/\n",
    "â”‚       â””â”€â”€ model_experiment.yml\n",
    "â”‚\n",
    "â”œâ”€â”€ src/                        # Source code\n",
    "â”‚   â”œâ”€â”€ notebooks/\n",
    "â”‚   â”‚   â”œâ”€â”€ bronze_ingestion.py\n",
    "â”‚   â”‚   â”œâ”€â”€ silver_transform.py\n",
    "â”‚   â”‚   â””â”€â”€ gold_aggregation.py\n",
    "â”‚   â”œâ”€â”€ pipelines/\n",
    "â”‚   â”‚   â””â”€â”€ dlt_pipeline.py    # DLT pipeline code\n",
    "â”‚   â””â”€â”€ python/\n",
    "â”‚       â””â”€â”€ utils/\n",
    "â”‚           â””â”€â”€ helpers.py      # Shared utilities\n",
    "â”‚\n",
    "â”œâ”€â”€ tests/                      # Unit and integration tests\n",
    "â”‚   â”œâ”€â”€ unit/\n",
    "â”‚   â”‚   â””â”€â”€ test_transforms.py\n",
    "â”‚   â””â”€â”€ integration/\n",
    "â”‚       â””â”€â”€ test_pipeline.py\n",
    "â”‚\n",
    "â”œâ”€â”€ fixtures/                   # Test data and fixtures\n",
    "â”‚   â””â”€â”€ sample_data.csv\n",
    "â”‚\n",
    "â””â”€â”€ .databricks/                # CLI configuration (auto-generated)\n",
    "    â””â”€â”€ project.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Directories:**\n",
    "\n",
    "**`resources/`:**\n",
    "* Contains all resource definitions\n",
    "* Organized by resource type\n",
    "* YAML configuration files\n",
    "\n",
    "**`src/`:**\n",
    "* Source code for notebooks and libraries\n",
    "* Business logic and transformations\n",
    "* Reusable modules\n",
    "\n",
    "**`tests/`:**\n",
    "* Unit tests for functions\n",
    "* Integration tests for workflows\n",
    "* Test fixtures and data\n",
    "\n",
    "---\n",
    "\n",
    "### **Naming Conventions:**\n",
    "\n",
    "**Resource files:**\n",
    "* `<resource_name>.job.yml` - Job definitions\n",
    "* `<resource_name>.pipeline.yml` - Pipeline definitions\n",
    "* `<resource_name>.experiment.yml` - MLflow experiments\n",
    "\n",
    "**Source files:**\n",
    "* Use descriptive names\n",
    "* Follow language conventions\n",
    "* Group by functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e364a7b6-a26b-4204-b44d-0c341032a734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### âš™ï¸ databricks.yml Configuration\n",
    "\n",
    "**The main configuration file:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Minimal Configuration:**\n",
    "\n",
    "```yaml\n",
    "bundle:\n",
    "  name: my_project\n",
    "\n",
    "targets:\n",
    "  dev:\n",
    "    default: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Configuration:**\n",
    "\n",
    "```yaml\n",
    "# Bundle identity\n",
    "bundle:\n",
    "  name: customer_analytics\n",
    "  \n",
    "# Include other config files\n",
    "include:\n",
    "  - resources/**/*.yml\n",
    "\n",
    "# Define variables\n",
    "variables:\n",
    "  catalog:\n",
    "    description: Unity Catalog name\n",
    "    default: dev_catalog\n",
    "  \n",
    "  schema:\n",
    "    description: Schema name  \n",
    "    default: analytics\n",
    "  \n",
    "  environment:\n",
    "    description: Environment name\n",
    "    default: development\n",
    "\n",
    "# Workspace configuration\n",
    "workspace:\n",
    "  root_path: ~/.bundle/${bundle.name}/${bundle.target}\n",
    "  file_path: /Workspace${workspace.root_path}/files\n",
    "  artifact_path: /Workspace${workspace.root_path}/artifacts\n",
    "  state_path: /Workspace${workspace.root_path}/.internal\n",
    "\n",
    "# Deployment targets\n",
    "targets:\n",
    "  # Development target\n",
    "  dev:\n",
    "    mode: development\n",
    "    default: true\n",
    "    workspace:\n",
    "      host: https://my-workspace.cloud.databricks.com\n",
    "    variables:\n",
    "      catalog: dev_catalog\n",
    "      schema: dev_analytics\n",
    "      environment: development\n",
    "  \n",
    "  # Staging target\n",
    "  staging:\n",
    "    mode: development\n",
    "    workspace:\n",
    "      host: https://my-workspace.cloud.databricks.com\n",
    "    variables:\n",
    "      catalog: staging_catalog\n",
    "      schema: staging_analytics\n",
    "      environment: staging\n",
    "  \n",
    "  # Production target\n",
    "  prod:\n",
    "    mode: production\n",
    "    workspace:\n",
    "      host: https://my-workspace.cloud.databricks.com\n",
    "      root_path: /Shared/.bundle/prod/${bundle.name}\n",
    "    \n",
    "    # Run as service principal in prod\n",
    "    run_as:\n",
    "      service_principal_name: prod-sp-analytics\n",
    "    \n",
    "    variables:\n",
    "      catalog: prod_catalog\n",
    "      schema: prod_analytics\n",
    "      environment: production\n",
    "    \n",
    "    # Permissions for production\n",
    "    permissions:\n",
    "      - level: CAN_VIEW\n",
    "        group_name: data-analysts\n",
    "      - level: CAN_MANAGE\n",
    "        group_name: data-engineers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Sections:**\n",
    "\n",
    "**`bundle:`**\n",
    "* Bundle name (required)\n",
    "* Must be unique\n",
    "\n",
    "**`include:`**\n",
    "* Reference other YAML files\n",
    "* Supports glob patterns\n",
    "* Modularize configuration\n",
    "\n",
    "**`variables:`**\n",
    "* Define reusable values\n",
    "* Override per target\n",
    "* Use with `${var.name}` syntax\n",
    "\n",
    "**`targets:`**\n",
    "* Define environments\n",
    "* Each has own configuration\n",
    "* One marked as `default: true`\n",
    "\n",
    "**`run_as:`**\n",
    "* Identity for resource execution\n",
    "* User or service principal\n",
    "* Important for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90ea345-d652-44a9-9d85-60cb5740daa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ“‹ Resource Configuration Files\n",
    "\n",
    "**Defining jobs, pipelines, and other resources:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Job Configuration Example:**\n",
    "\n",
    "**File: `resources/jobs/etl_job.yml`**\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    customer_etl:\n",
    "      name: \"${bundle.target} - Customer ETL\"\n",
    "      \n",
    "      tasks:\n",
    "        - task_key: bronze_ingestion\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebooks/bronze_ingestion.py\n",
    "            base_parameters:\n",
    "              catalog: ${var.catalog}\n",
    "              schema: ${var.schema}\n",
    "          new_cluster:\n",
    "            spark_version: 14.3.x-scala2.12\n",
    "            node_type_id: i3.xlarge\n",
    "            num_workers: 2\n",
    "        \n",
    "        - task_key: silver_transform\n",
    "          depends_on:\n",
    "            - task_key: bronze_ingestion\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebooks/silver_transform.py\n",
    "            base_parameters:\n",
    "              catalog: ${var.catalog}\n",
    "              schema: ${var.schema}\n",
    "          new_cluster:\n",
    "            spark_version: 14.3.x-scala2.12\n",
    "            node_type_id: i3.xlarge\n",
    "            num_workers: 2\n",
    "      \n",
    "      schedule:\n",
    "        quartz_cron_expression: \"0 0 2 * * ?\"\n",
    "        timezone_id: \"UTC\"\n",
    "        pause_status: UNPAUSED\n",
    "      \n",
    "      email_notifications:\n",
    "        on_failure:\n",
    "          - data-team@company.com\n",
    "      \n",
    "      tags:\n",
    "        environment: ${var.environment}\n",
    "        project: customer_analytics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Configuration Example:**\n",
    "\n",
    "**File: `resources/pipelines/data_pipeline.yml`**\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  pipelines:\n",
    "    customer_pipeline:\n",
    "      name: \"${bundle.target} - Customer Data Pipeline\"\n",
    "      \n",
    "      target: ${var.catalog}.${var.schema}\n",
    "      \n",
    "      libraries:\n",
    "        - notebook:\n",
    "            path: ../src/pipelines/dlt_pipeline.py\n",
    "      \n",
    "      configuration:\n",
    "        catalog: ${var.catalog}\n",
    "        schema: ${var.schema}\n",
    "        environment: ${var.environment}\n",
    "      \n",
    "      clusters:\n",
    "        - label: default\n",
    "          autoscale:\n",
    "            min_workers: 1\n",
    "            max_workers: 5\n",
    "            mode: ENHANCED\n",
    "      \n",
    "      development: true\n",
    "      continuous: false\n",
    "      channel: CURRENT\n",
    "      \n",
    "      notifications:\n",
    "        - email_recipients:\n",
    "            - data-team@company.com\n",
    "          alerts:\n",
    "            - on-update-failure\n",
    "            - on-flow-failure\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Variable References:**\n",
    "\n",
    "**Use variables in configurations:**\n",
    "\n",
    "```yaml\n",
    "# Reference bundle variables\n",
    "${var.catalog}\n",
    "${var.schema}\n",
    "${var.environment}\n",
    "\n",
    "# Reference bundle properties\n",
    "${bundle.name}\n",
    "${bundle.target}\n",
    "${bundle.environment}\n",
    "\n",
    "# Reference workspace properties\n",
    "${workspace.root_path}\n",
    "${workspace.file_path}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28d25064-6835-4ce4-8dc7-3e4bc1926747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 3: Creating Your First Bundle ğŸ› ï¸\n",
    "\n",
    "Let's create a complete bundle from scratch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "175885d7-23d4-4408-8876-5a84e63cd7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### âœ… Prerequisites\n",
    "\n",
    "**Before creating a bundle, ensure you have:**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Databricks CLI Installed**\n",
    "\n",
    "**Check installation:**\n",
    "```bash\n",
    "databricks --version\n",
    "```\n",
    "\n",
    "**Install if needed:**\n",
    "```bash\n",
    "# Using pip\n",
    "pip install databricks-cli\n",
    "\n",
    "# Or using Homebrew (macOS)\n",
    "brew tap databricks/tap\n",
    "brew install databricks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. CLI Authentication**\n",
    "\n",
    "**Configure authentication:**\n",
    "```bash\n",
    "databricks configure --token\n",
    "```\n",
    "\n",
    "**Or use OAuth:**\n",
    "```bash\n",
    "databricks auth login --host https://your-workspace.cloud.databricks.com\n",
    "```\n",
    "\n",
    "**Verify authentication:**\n",
    "```bash\n",
    "databricks workspace list /\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Git Installed**\n",
    "\n",
    "**Check installation:**\n",
    "```bash\n",
    "git --version\n",
    "```\n",
    "\n",
    "**Initialize repository:**\n",
    "```bash\n",
    "git init\n",
    "git config user.name \"Your Name\"\n",
    "git config user.email \"your.email@company.com\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Text Editor or IDE**\n",
    "\n",
    "**Recommended:**\n",
    "* VS Code with Databricks extension\n",
    "* PyCharm\n",
    "* Any text editor with YAML support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc590442-6205-41d1-8d97-35f6092677ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¬ Step 1: Initialize a New Bundle\n",
    "\n",
    "**Create a bundle from template:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Using Default Template:**\n",
    "\n",
    "```bash\n",
    "# Navigate to your projects directory\n",
    "cd ~/projects\n",
    "\n",
    "# Initialize a new bundle\n",
    "databricks bundle init\n",
    "```\n",
    "\n",
    "**You'll be prompted for:**\n",
    "* Template to use (choose \"default-python\")\n",
    "* Project name (e.g., \"customer_analytics\")\n",
    "* Workspace host\n",
    "\n",
    "---\n",
    "\n",
    "### **Available Templates:**\n",
    "\n",
    "**1. default-python:**\n",
    "* Basic Python project\n",
    "* Job with notebook task\n",
    "* Good for general workflows\n",
    "\n",
    "**2. dlt-pipeline:**\n",
    "* Delta Live Tables pipeline\n",
    "* Bronze, silver, gold layers\n",
    "* Data engineering workflows\n",
    "\n",
    "**3. mlops-stacks:**\n",
    "* ML project structure\n",
    "* Training and inference\n",
    "* MLflow integration\n",
    "\n",
    "---\n",
    "\n",
    "### **Manual Initialization:**\n",
    "\n",
    "```bash\n",
    "# Create project directory\n",
    "mkdir customer_analytics\n",
    "cd customer_analytics\n",
    "\n",
    "# Initialize bundle with specific template\n",
    "databricks bundle init default-python\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What Gets Created:**\n",
    "\n",
    "```\n",
    "customer_analytics/\n",
    "â”œâ”€â”€ databricks.yml\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ resources/\n",
    "â”‚   â””â”€â”€ customer_analytics_job.yml\n",
    "â””â”€â”€ src/\n",
    "    â””â”€â”€ notebook.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "730f42e3-5bbb-45fe-96b9-924625913972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### âš™ï¸ Step 2: Configure Your Bundle\n",
    "\n",
    "**Customize the generated configuration:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Edit databricks.yml:**\n",
    "\n",
    "```yaml\n",
    "# Bundle identity\n",
    "bundle:\n",
    "  name: customer_analytics\n",
    "\n",
    "# Include resource files\n",
    "include:\n",
    "  - resources/*.yml\n",
    "\n",
    "# Define variables\n",
    "variables:\n",
    "  catalog:\n",
    "    description: Unity Catalog name\n",
    "    default: dev_catalog\n",
    "  \n",
    "  schema:\n",
    "    description: Schema name\n",
    "    default: analytics\n",
    "\n",
    "# Deployment targets\n",
    "targets:\n",
    "  # Development environment\n",
    "  dev:\n",
    "    mode: development\n",
    "    default: true\n",
    "    workspace:\n",
    "      host: https://your-workspace.cloud.databricks.com\n",
    "    variables:\n",
    "      catalog: dev_catalog\n",
    "      schema: dev_analytics\n",
    "  \n",
    "  # Production environment\n",
    "  prod:\n",
    "    mode: production\n",
    "    workspace:\n",
    "      host: https://your-workspace.cloud.databricks.com\n",
    "      root_path: /Shared/.bundle/prod/${bundle.name}\n",
    "    run_as:\n",
    "      service_principal_name: prod-sp\n",
    "    variables:\n",
    "      catalog: prod_catalog\n",
    "      schema: prod_analytics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Create Job Configuration:**\n",
    "\n",
    "**File: `resources/etl_job.yml`**\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    customer_etl:\n",
    "      name: \"${bundle.target} - Customer ETL Job\"\n",
    "      \n",
    "      tasks:\n",
    "        - task_key: ingest_data\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/ingest.py\n",
    "            base_parameters:\n",
    "              catalog: ${var.catalog}\n",
    "              schema: ${var.schema}\n",
    "          new_cluster:\n",
    "            spark_version: 14.3.x-scala2.12\n",
    "            node_type_id: i3.xlarge\n",
    "            num_workers: 2\n",
    "      \n",
    "      schedule:\n",
    "        quartz_cron_expression: \"0 0 2 * * ?\"\n",
    "        timezone_id: \"UTC\"\n",
    "      \n",
    "      tags:\n",
    "        project: customer_analytics\n",
    "        environment: ${bundle.target}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Create Source Notebook:**\n",
    "\n",
    "**File: `src/ingest.py`**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog\", \"dev_catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"analytics\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "print(f\"Using catalog: {catalog}\")\n",
    "print(f\"Using schema: {schema}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Your ETL logic here\n",
    "df = spark.read.table(\"samples.tpch.customer\")\n",
    "\n",
    "# Write to target\n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.customers\")\n",
    "\n",
    "print(f\"âœ… Data written to {catalog}.{schema}.customers\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27243c1c-bed3-4e9d-8840-046d5496463b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### âœ… Step 3: Validate Configuration\n",
    "\n",
    "**Check that your bundle is valid:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Run Validation:**\n",
    "\n",
    "```bash\n",
    "# From bundle root directory\n",
    "databricks bundle validate\n",
    "```\n",
    "\n",
    "**Successful output:**\n",
    "```\n",
    "Name: customer_analytics\n",
    "Target: dev\n",
    "Workspace:\n",
    "  Host: https://your-workspace.cloud.databricks.com\n",
    "  User: your.email@company.com\n",
    "  Path: /Users/your.email@company.com/.bundle/customer_analytics/dev\n",
    "\n",
    "Validation OK!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Validation Errors:**\n",
    "\n",
    "**1. Invalid YAML syntax:**\n",
    "```\n",
    "Error: yaml: line 10: mapping values are not allowed in this context\n",
    "```\n",
    "**Fix:** Check indentation and colons\n",
    "\n",
    "**2. Missing required fields:**\n",
    "```\n",
    "Error: bundle.name is required\n",
    "```\n",
    "**Fix:** Add required field to databricks.yml\n",
    "\n",
    "**3. Invalid variable reference:**\n",
    "```\n",
    "Error: variable \"catalog\" is not defined\n",
    "```\n",
    "**Fix:** Define variable in variables section\n",
    "\n",
    "**4. Invalid resource configuration:**\n",
    "```\n",
    "Error: jobs.customer_etl.tasks[0].notebook_task.notebook_path: file not found\n",
    "```\n",
    "**Fix:** Ensure notebook path is correct\n",
    "\n",
    "---\n",
    "\n",
    "### **View Bundle Schema:**\n",
    "\n",
    "```bash\n",
    "# See full configuration schema\n",
    "databricks bundle schema\n",
    "```\n",
    "\n",
    "**Use for reference when writing configurations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "970a22e5-1653-4d45-ac95-a0f888f5364a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 4: Deploying and Managing Bundles ğŸš€\n",
    "\n",
    "Let's deploy and manage your bundle.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24a1a1e-8b6c-4e2e-a9ec-1c310f5dd7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸš€ Deploying Your Bundle\n",
    "\n",
    "**Deploy to different environments:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Deploy to Development:**\n",
    "\n",
    "```bash\n",
    "# Deploy to default target (dev)\n",
    "databricks bundle deploy\n",
    "\n",
    "# Or explicitly specify target\n",
    "databricks bundle deploy -t dev\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "1. Validates configuration\n",
    "2. Uploads source files to workspace\n",
    "3. Creates/updates jobs and pipelines\n",
    "4. Applies permissions and settings\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Uploading customer_analytics to /Users/you/.bundle/customer_analytics/dev/files...\n",
    "Deploying resources...\n",
    "  âœ“ Job: dev - Customer ETL Job\n",
    "\n",
    "Deployment complete!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Deploy to Production:**\n",
    "\n",
    "```bash\n",
    "# Deploy to production target\n",
    "databricks bundle deploy -t prod\n",
    "```\n",
    "\n",
    "**Important for production:**\n",
    "* Uses `run_as` service principal\n",
    "* Deploys to shared location\n",
    "* Applies production permissions\n",
    "* Uses production variables\n",
    "\n",
    "---\n",
    "\n",
    "### **Deployment Modes:**\n",
    "\n",
    "**Development mode:**\n",
    "```yaml\n",
    "targets:\n",
    "  dev:\n",
    "    mode: development\n",
    "```\n",
    "* Faster deployments\n",
    "* Allows workspace file edits\n",
    "* Personal workspace paths\n",
    "\n",
    "**Production mode:**\n",
    "```yaml\n",
    "targets:\n",
    "  prod:\n",
    "    mode: production\n",
    "```\n",
    "* Stricter validation\n",
    "* Immutable deployments\n",
    "* Shared workspace paths\n",
    "* Requires `run_as` configuration\n",
    "\n",
    "---\n",
    "\n",
    "### **View Deployed Resources:**\n",
    "\n",
    "```bash\n",
    "# List deployed resources\n",
    "databricks bundle summary\n",
    "```\n",
    "\n",
    "**Output shows:**\n",
    "* Jobs and their IDs\n",
    "* Pipelines and their IDs\n",
    "* Workspace paths\n",
    "* Resource URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d39b5f0-6b43-4ba9-8d2b-4da2c183c6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### â–¶ï¸ Running Bundle Resources\n",
    "\n",
    "**Execute jobs and pipelines from your bundle:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Run a Job:**\n",
    "\n",
    "```bash\n",
    "# Run job by name\n",
    "databricks bundle run customer_etl -t dev\n",
    "```\n",
    "\n",
    "**With parameters:**\n",
    "```bash\n",
    "# Override parameters at runtime\n",
    "databricks bundle run customer_etl -t dev \\\n",
    "  --params catalog=test_catalog \\\n",
    "  --params schema=test_schema\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Run URL: https://your-workspace.cloud.databricks.com/#job/123/run/456\n",
    "\n",
    "Run customer_etl (123): RUNNING\n",
    "  Task ingest_data: RUNNING\n",
    "\n",
    "Run customer_etl (123): SUCCESS\n",
    "  Task ingest_data: SUCCESS\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Monitor Run Status:**\n",
    "\n",
    "```bash\n",
    "# Get run status\n",
    "databricks runs get --run-id 456\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **View Run Logs:**\n",
    "\n",
    "```bash\n",
    "# Get run output\n",
    "databricks runs get-output --run-id 456\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cancel a Run:**\n",
    "\n",
    "```bash\n",
    "# Cancel running job\n",
    "databricks runs cancel --run-id 456\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39477bdd-03b5-4999-8a64-777b5a2e6ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”„ Updating Your Bundle\n",
    "\n",
    "**Make changes and redeploy:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow:**\n",
    "\n",
    "**1. Make changes:**\n",
    "```bash\n",
    "# Edit configuration\n",
    "vim resources/etl_job.yml\n",
    "\n",
    "# Edit source code\n",
    "vim src/ingest.py\n",
    "```\n",
    "\n",
    "**2. Validate changes:**\n",
    "```bash\n",
    "databricks bundle validate\n",
    "```\n",
    "\n",
    "**3. Deploy updates:**\n",
    "```bash\n",
    "databricks bundle deploy -t dev\n",
    "```\n",
    "\n",
    "**4. Test changes:**\n",
    "```bash\n",
    "databricks bundle run customer_etl -t dev\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What Gets Updated:**\n",
    "\n",
    "**Incremental updates:**\n",
    "* Only changed files are uploaded\n",
    "* Only modified resources are updated\n",
    "* Existing runs are not affected\n",
    "\n",
    "**Resource updates:**\n",
    "* Jobs: Configuration updated, existing runs continue\n",
    "* Pipelines: Configuration updated, requires manual refresh\n",
    "* Notebooks: New version uploaded\n",
    "\n",
    "---\n",
    "\n",
    "### **Destroy Resources:**\n",
    "\n",
    "```bash\n",
    "# Remove all deployed resources\n",
    "databricks bundle destroy -t dev\n",
    "```\n",
    "\n",
    "**âš ï¸ Warning:**\n",
    "* Deletes all jobs, pipelines, and resources\n",
    "* Cannot be undone\n",
    "* Use with caution, especially in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f450592-b29d-47c0-8db0-ba564caa2fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ“ Version Control with Git\n",
    "\n",
    "**Track your bundle in Git:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Initialize Git Repository:**\n",
    "\n",
    "```bash\n",
    "# Initialize Git\n",
    "git init\n",
    "\n",
    "# Create .gitignore\n",
    "cat > .gitignore << EOF\n",
    "# Databricks\n",
    ".databricks/\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "venv/\n",
    ".venv/\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "EOF\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Commit Your Bundle:**\n",
    "\n",
    "```bash\n",
    "# Add files\n",
    "git add .\n",
    "\n",
    "# Commit\n",
    "git commit -m \"Initial bundle configuration\"\n",
    "\n",
    "# Create remote repository (GitHub, GitLab, etc.)\n",
    "git remote add origin https://github.com/your-org/customer-analytics.git\n",
    "\n",
    "# Push to remote\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Branching Strategy:**\n",
    "\n",
    "**Feature branches:**\n",
    "```bash\n",
    "# Create feature branch\n",
    "git checkout -b feature/add-silver-layer\n",
    "\n",
    "# Make changes\n",
    "vim resources/etl_job.yml\n",
    "\n",
    "# Commit changes\n",
    "git add .\n",
    "git commit -m \"Add silver layer transformation\"\n",
    "\n",
    "# Push branch\n",
    "git push origin feature/add-silver-layer\n",
    "```\n",
    "\n",
    "**Pull request workflow:**\n",
    "1. Create feature branch\n",
    "2. Make changes\n",
    "3. Open pull request\n",
    "4. Code review\n",
    "5. Merge to main\n",
    "6. Deploy to production\n",
    "\n",
    "---\n",
    "\n",
    "### **What to Commit:**\n",
    "\n",
    "**âœ… Do commit:**\n",
    "* `databricks.yml`\n",
    "* `resources/*.yml`\n",
    "* `src/**/*`\n",
    "* `tests/**/*`\n",
    "* `README.md`\n",
    "* `.gitignore`\n",
    "\n",
    "**âŒ Don't commit:**\n",
    "* `.databricks/` (auto-generated)\n",
    "* Credentials or tokens\n",
    "* Local environment files\n",
    "* IDE-specific files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79f2d9a4-6589-4c89-8cd7-f9c31d198813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 5: CI/CD with Asset Bundles ğŸ”„\n",
    "\n",
    "Automate testing and deployment with CI/CD pipelines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68af77bc-de8a-4c3e-92cf-c5e42755c0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”„ CI/CD Pipeline Overview\n",
    "\n",
    "**Automated deployment workflow:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Typical CI/CD Flow:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Developer Workflow                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Create feature branch                               â”‚\n",
    "â”‚  2. Make changes to bundle                              â”‚\n",
    "â”‚  3. Commit and push to Git                              â”‚\n",
    "â”‚  4. Open pull request                                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CI Pipeline (Automated)                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Checkout code                                       â”‚\n",
    "â”‚  2. Install dependencies                                â”‚\n",
    "â”‚  3. Run unit tests                                      â”‚\n",
    "â”‚  4. Validate bundle configuration                       â”‚\n",
    "â”‚  5. Deploy to dev/staging                               â”‚\n",
    "â”‚  6. Run integration tests                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Code Review                                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Review changes                                      â”‚\n",
    "â”‚  2. Check test results                                  â”‚\n",
    "â”‚  3. Approve or request changes                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CD Pipeline (Automated)                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Merge to main branch                                â”‚\n",
    "â”‚  2. Deploy to production                                â”‚\n",
    "â”‚  3. Run smoke tests                                     â”‚\n",
    "â”‚  4. Send notifications                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits:**\n",
    "\n",
    "* âœ… **Automated testing** - Catch errors early\n",
    "* âœ… **Consistent deployments** - Same process every time\n",
    "* âœ… **Fast feedback** - Know immediately if something breaks\n",
    "* âœ… **Audit trail** - Track all changes\n",
    "* âœ… **Rollback capability** - Easy to revert changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a231a751-7aa8-4400-898a-87c0feb93216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”§ GitHub Actions CI/CD\n",
    "\n",
    "**Automate with GitHub Actions:**\n",
    "\n",
    "---\n",
    "\n",
    "### **File: `.github/workflows/deploy.yml`**\n",
    "\n",
    "```yaml\n",
    "name: Deploy Bundle\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "  pull_request:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  # Validate and test on pull requests\n",
    "  validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.event_name == 'pull_request'\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      \n",
    "      - name: Install Databricks CLI\n",
    "        run: |\n",
    "          pip install databricks-cli\n",
    "      \n",
    "      - name: Configure Databricks CLI\n",
    "        env:\n",
    "          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n",
    "          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n",
    "        run: |\n",
    "          echo \"[DEFAULT]\" > ~/.databrickscfg\n",
    "          echo \"host = $DATABRICKS_HOST\" >> ~/.databrickscfg\n",
    "          echo \"token = $DATABRICKS_TOKEN\" >> ~/.databrickscfg\n",
    "      \n",
    "      - name: Validate bundle\n",
    "        run: |\n",
    "          databricks bundle validate -t dev\n",
    "      \n",
    "      - name: Run unit tests\n",
    "        run: |\n",
    "          pip install pytest\n",
    "          pytest tests/unit/\n",
    "  \n",
    "  # Deploy to dev on pull request\n",
    "  deploy-dev:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: validate\n",
    "    if: github.event_name == 'pull_request'\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Install Databricks CLI\n",
    "        run: pip install databricks-cli\n",
    "      \n",
    "      - name: Configure Databricks CLI\n",
    "        env:\n",
    "          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n",
    "          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n",
    "        run: |\n",
    "          echo \"[DEFAULT]\" > ~/.databrickscfg\n",
    "          echo \"host = $DATABRICKS_HOST\" >> ~/.databrickscfg\n",
    "          echo \"token = $DATABRICKS_TOKEN\" >> ~/.databrickscfg\n",
    "      \n",
    "      - name: Deploy to dev\n",
    "        run: |\n",
    "          databricks bundle deploy -t dev\n",
    "  \n",
    "  # Deploy to production on merge to main\n",
    "  deploy-prod:\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Install Databricks CLI\n",
    "        run: pip install databricks-cli\n",
    "      \n",
    "      - name: Configure Databricks CLI\n",
    "        env:\n",
    "          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n",
    "          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n",
    "        run: |\n",
    "          echo \"[DEFAULT]\" > ~/.databrickscfg\n",
    "          echo \"host = $DATABRICKS_HOST\" >> ~/.databrickscfg\n",
    "          echo \"token = $DATABRICKS_TOKEN\" >> ~/.databrickscfg\n",
    "      \n",
    "      - name: Deploy to production\n",
    "        run: |\n",
    "          databricks bundle deploy -t prod\n",
    "      \n",
    "      - name: Notify deployment\n",
    "        if: success()\n",
    "        run: |\n",
    "          echo \"âœ… Successfully deployed to production\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Required Secrets:**\n",
    "\n",
    "**Add to GitHub repository settings:**\n",
    "* `DATABRICKS_HOST` - Workspace URL\n",
    "* `DATABRICKS_TOKEN` - Service principal token\n",
    "\n",
    "**Navigate to:**\n",
    "```\n",
    "Repository â†’ Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fed8580-6b41-4396-850b-78e6a3534e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ§ª Testing Your Bundle\n",
    "\n",
    "**Implement comprehensive testing:**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Unit Tests**\n",
    "\n",
    "**Test individual functions:**\n",
    "\n",
    "**File: `tests/unit/test_transforms.py`**\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from src.utils.transforms import clean_customer_data\n",
    "\n",
    "@pytest.fixture\n",
    "def spark():\n",
    "    return SparkSession.builder.master(\"local[1]\").getOrCreate()\n",
    "\n",
    "def test_clean_customer_data(spark):\n",
    "    # Create test data\n",
    "    data = [\n",
    "        (1, \"John Doe\", \"john@email.com\"),\n",
    "        (2, \"Jane Smith\", \"jane@email.com\"),\n",
    "        (3, None, \"invalid@email.com\")  # Invalid record\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"id\", \"name\", \"email\"])\n",
    "    \n",
    "    # Apply transformation\n",
    "    result = clean_customer_data(df)\n",
    "    \n",
    "    # Assert results\n",
    "    assert result.count() == 2  # Invalid record removed\n",
    "    assert result.filter(\"name IS NULL\").count() == 0\n",
    "```\n",
    "\n",
    "**Run tests:**\n",
    "```bash\n",
    "pytest tests/unit/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Integration Tests**\n",
    "\n",
    "**Test end-to-end workflows:**\n",
    "\n",
    "**File: `tests/integration/test_pipeline.py`**\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "def test_job_execution():\n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    # Find the deployed job\n",
    "    jobs = list(w.jobs.list(name=\"dev - Customer ETL Job\"))\n",
    "    assert len(jobs) > 0, \"Job not found\"\n",
    "    \n",
    "    job = jobs[0]\n",
    "    \n",
    "    # Run the job\n",
    "    run = w.jobs.run_now(job_id=job.job_id)\n",
    "    \n",
    "    # Wait for completion\n",
    "    result = w.jobs.wait_get_run_job_terminated_or_skipped(run_id=run.run_id)\n",
    "    \n",
    "    # Assert success\n",
    "    assert result.state.result_state == \"SUCCESS\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Bundle Validation Tests**\n",
    "\n",
    "**Test configuration validity:**\n",
    "\n",
    "**File: `tests/test_bundle.py`**\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "import pytest\n",
    "\n",
    "def test_bundle_validate():\n",
    "    \"\"\"Test that bundle configuration is valid\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"databricks\", \"bundle\", \"validate\", \"-t\", \"dev\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    assert result.returncode == 0, f\"Bundle validation failed: {result.stderr}\"\n",
    "\n",
    "def test_bundle_schema():\n",
    "    \"\"\"Test that bundle follows schema\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"databricks\", \"bundle\", \"schema\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    assert result.returncode == 0\n",
    "    assert \"bundle\" in result.stdout\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Run All Tests:**\n",
    "\n",
    "```bash\n",
    "# Run all tests\n",
    "pytest tests/\n",
    "\n",
    "# Run with coverage\n",
    "pytest tests/ --cov=src --cov-report=html\n",
    "\n",
    "# Run specific test file\n",
    "pytest tests/unit/test_transforms.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8138c54-1770-463c-a38d-7a4d8ec16f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Part 6: Best Practices and Patterns âœ…\n",
    "\n",
    "Let's cover best practices for production bundles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a205c1-9569-4fce-b587-dd25f51b6e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ¯ Asset Bundles Best Practices\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Project Organization**\n",
    "\n",
    "**Use consistent structure:**\n",
    "\n",
    "```\n",
    "âœ… Good:\n",
    "my_project/\n",
    "â”œâ”€â”€ databricks.yml\n",
    "â”œâ”€â”€ resources/\n",
    "â”‚   â”œâ”€â”€ jobs/\n",
    "â”‚   â”œâ”€â”€ pipelines/\n",
    "â”‚   â””â”€â”€ experiments/\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ notebooks/\n",
    "â”‚   â”œâ”€â”€ pipelines/\n",
    "â”‚   â””â”€â”€ utils/\n",
    "â””â”€â”€ tests/\n",
    "\n",
    "âŒ Bad:\n",
    "my_project/\n",
    "â”œâ”€â”€ databricks.yml\n",
    "â”œâ”€â”€ job1.yml\n",
    "â”œâ”€â”€ job2.yml\n",
    "â”œâ”€â”€ notebook1.py\n",
    "â””â”€â”€ notebook2.py\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "* Easy to navigate\n",
    "* Clear separation of concerns\n",
    "* Scalable structure\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Naming Conventions**\n",
    "\n",
    "**Use descriptive names:**\n",
    "\n",
    "```yaml\n",
    "âœ… Good:\n",
    "resources:\n",
    "  jobs:\n",
    "    customer_etl_daily:\n",
    "      name: \"${bundle.target} - Customer ETL Daily\"\n",
    "    \n",
    "  pipelines:\n",
    "    customer_bronze_to_gold:\n",
    "      name: \"${bundle.target} - Customer Data Pipeline\"\n",
    "\n",
    "âŒ Bad:\n",
    "resources:\n",
    "  jobs:\n",
    "    job1:\n",
    "      name: \"job1\"\n",
    "    \n",
    "  pipelines:\n",
    "    pipeline:\n",
    "      name: \"pipeline\"\n",
    "```\n",
    "\n",
    "**Include:**\n",
    "* Environment prefix (`${bundle.target}`)\n",
    "* Purpose/function\n",
    "* Frequency (if applicable)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Variable Management**\n",
    "\n",
    "**Define reusable variables:**\n",
    "\n",
    "```yaml\n",
    "âœ… Good:\n",
    "variables:\n",
    "  catalog:\n",
    "    description: Unity Catalog name\n",
    "    default: dev_catalog\n",
    "  \n",
    "  schema:\n",
    "    description: Schema name\n",
    "    default: analytics\n",
    "  \n",
    "  notification_email:\n",
    "    description: Email for notifications\n",
    "    default: team@company.com\n",
    "\n",
    "targets:\n",
    "  prod:\n",
    "    variables:\n",
    "      catalog: prod_catalog\n",
    "      notification_email: oncall@company.com\n",
    "\n",
    "âŒ Bad:\n",
    "# Hardcoded values in resources\n",
    "resources:\n",
    "  jobs:\n",
    "    my_job:\n",
    "      tasks:\n",
    "        - notebook_task:\n",
    "            base_parameters:\n",
    "              catalog: dev_catalog  # Hardcoded!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Environment Separation**\n",
    "\n",
    "**Use distinct targets:**\n",
    "\n",
    "```yaml\n",
    "âœ… Good:\n",
    "targets:\n",
    "  dev:\n",
    "    mode: development\n",
    "    workspace:\n",
    "      host: https://dev.cloud.databricks.com\n",
    "    variables:\n",
    "      catalog: dev_catalog\n",
    "  \n",
    "  staging:\n",
    "    mode: development\n",
    "    workspace:\n",
    "      host: https://staging.cloud.databricks.com\n",
    "    variables:\n",
    "      catalog: staging_catalog\n",
    "  \n",
    "  prod:\n",
    "    mode: production\n",
    "    workspace:\n",
    "      host: https://prod.cloud.databricks.com\n",
    "    run_as:\n",
    "      service_principal_name: prod-sp\n",
    "    variables:\n",
    "      catalog: prod_catalog\n",
    "\n",
    "âŒ Bad:\n",
    "targets:\n",
    "  dev:\n",
    "    # Same config for all environments\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Security Best Practices**\n",
    "\n",
    "**Use service principals in production:**\n",
    "\n",
    "```yaml\n",
    "âœ… Good:\n",
    "targets:\n",
    "  prod:\n",
    "    mode: production\n",
    "    run_as:\n",
    "      service_principal_name: prod-analytics-sp\n",
    "    permissions:\n",
    "      - level: CAN_VIEW\n",
    "        group_name: data-analysts\n",
    "      - level: CAN_MANAGE\n",
    "        group_name: data-engineers\n",
    "\n",
    "âŒ Bad:\n",
    "targets:\n",
    "  prod:\n",
    "    # Running as personal user account\n",
    "    run_as:\n",
    "      user_name: john.doe@company.com\n",
    "```\n",
    "\n",
    "**Never commit secrets:**\n",
    "```yaml\n",
    "âŒ Bad:\n",
    "variables:\n",
    "  api_key: \"sk-1234567890abcdef\"  # Never do this!\n",
    "  password: \"mypassword\"          # Never do this!\n",
    "\n",
    "âœ… Good:\n",
    "# Use Databricks secrets\n",
    "${secrets.my_scope.api_key}\n",
    "${secrets.my_scope.password}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Resource Tagging**\n",
    "\n",
    "**Tag all resources:**\n",
    "\n",
    "```yaml\n",
    "âœ… Good:\n",
    "resources:\n",
    "  jobs:\n",
    "    customer_etl:\n",
    "      tags:\n",
    "        environment: ${bundle.target}\n",
    "        project: customer_analytics\n",
    "        team: data-engineering\n",
    "        cost_center: analytics\n",
    "        owner: data-team@company.com\n",
    "\n",
    "âŒ Bad:\n",
    "resources:\n",
    "  jobs:\n",
    "    customer_etl:\n",
    "      # No tags\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "* Cost tracking\n",
    "* Resource organization\n",
    "* Ownership clarity\n",
    "* Governance\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Documentation**\n",
    "\n",
    "**Document your bundle:**\n",
    "\n",
    "**README.md:**\n",
    "```markdown\n",
    "# Customer Analytics Bundle\n",
    "\n",
    "## Overview\n",
    "This bundle contains ETL jobs and pipelines for customer analytics.\n",
    "\n",
    "## Structure\n",
    "- `resources/jobs/` - Job definitions\n",
    "- `resources/pipelines/` - DLT pipeline definitions\n",
    "- `src/notebooks/` - Notebook source code\n",
    "- `tests/` - Unit and integration tests\n",
    "\n",
    "## Deployment\n",
    "\n",
    "### Development\n",
    "```bash\n",
    "databricks bundle deploy -t dev\n",
    "```\n",
    "\n",
    "### Production\n",
    "```bash\n",
    "databricks bundle deploy -t prod\n",
    "```\n",
    "\n",
    "## Variables\n",
    "- `catalog` - Unity Catalog name\n",
    "- `schema` - Schema name\n",
    "- `notification_email` - Email for alerts\n",
    "\n",
    "## Owners\n",
    "- Team: Data Engineering\n",
    "- Contact: data-team@company.com\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Testing**\n",
    "\n",
    "**Test everything:**\n",
    "\n",
    "```bash\n",
    "âœ… Good:\n",
    "# Validate configuration\n",
    "databricks bundle validate\n",
    "\n",
    "# Run unit tests\n",
    "pytest tests/unit/\n",
    "\n",
    "# Deploy to dev\n",
    "databricks bundle deploy -t dev\n",
    "\n",
    "# Run integration tests\n",
    "pytest tests/integration/\n",
    "\n",
    "# Deploy to prod\n",
    "databricks bundle deploy -t prod\n",
    "\n",
    "âŒ Bad:\n",
    "# Deploy directly to prod without testing\n",
    "databricks bundle deploy -t prod\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Version Control**\n",
    "\n",
    "**Use Git properly:**\n",
    "\n",
    "```bash\n",
    "âœ… Good:\n",
    "# Feature branch workflow\n",
    "git checkout -b feature/add-silver-layer\n",
    "# Make changes\n",
    "git commit -m \"Add silver layer transformation\"\n",
    "git push origin feature/add-silver-layer\n",
    "# Open PR, review, merge\n",
    "\n",
    "âŒ Bad:\n",
    "# Commit directly to main\n",
    "git commit -m \"changes\"\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Modularization**\n",
    "\n",
    "**Split configuration into files:**\n",
    "\n",
    "```yaml\n",
    "âœ… Good:\n",
    "databricks.yml:\n",
    "  include:\n",
    "    - resources/**/*.yml\n",
    "\n",
    "resources/jobs/etl_job.yml\n",
    "resources/jobs/ml_training.yml\n",
    "resources/pipelines/data_pipeline.yml\n",
    "\n",
    "âŒ Bad:\n",
    "databricks.yml:\n",
    "  # Everything in one file (1000+ lines)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b80651-4c21-43e2-9017-d1c0feaa348a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”§ Common Bundle Patterns\n",
    "\n",
    "**Reusable patterns for common scenarios:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern 1: Multi-Environment Deployment**\n",
    "\n",
    "**Use case:** Deploy same bundle to dev, staging, prod\n",
    "\n",
    "```yaml\n",
    "variables:\n",
    "  catalog: ${bundle.target}_catalog\n",
    "  schema: analytics\n",
    "  cluster_size:\n",
    "    description: Cluster size\n",
    "    default: small\n",
    "\n",
    "targets:\n",
    "  dev:\n",
    "    default: true\n",
    "    variables:\n",
    "      cluster_size: small\n",
    "  \n",
    "  staging:\n",
    "    variables:\n",
    "      cluster_size: medium\n",
    "  \n",
    "  prod:\n",
    "    mode: production\n",
    "    run_as:\n",
    "      service_principal_name: prod-sp\n",
    "    variables:\n",
    "      cluster_size: large\n",
    "\n",
    "resources:\n",
    "  jobs:\n",
    "    etl:\n",
    "      tasks:\n",
    "        - new_cluster:\n",
    "            # Use variable for cluster size\n",
    "            num_workers: |\n",
    "              ${\n",
    "                var.cluster_size == \"small\" ? 1 :\n",
    "                var.cluster_size == \"medium\" ? 3 : 5\n",
    "              }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern 2: Shared Configuration**\n",
    "\n",
    "**Use case:** Reuse common settings across resources\n",
    "\n",
    "**File: `resources/_common.yml`**\n",
    "```yaml\n",
    "# Common cluster configuration\n",
    "common_cluster: &common_cluster\n",
    "  spark_version: 14.3.x-scala2.12\n",
    "  node_type_id: i3.xlarge\n",
    "  spark_conf:\n",
    "    spark.databricks.delta.preview.enabled: \"true\"\n",
    "  \n",
    "# Common notification settings\n",
    "common_notifications: &common_notifications\n",
    "  email_notifications:\n",
    "    on_failure:\n",
    "      - ${var.notification_email}\n",
    "```\n",
    "\n",
    "**File: `resources/jobs/etl_job.yml`**\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    etl:\n",
    "      <<: *common_notifications\n",
    "      tasks:\n",
    "        - new_cluster:\n",
    "            <<: *common_cluster\n",
    "            num_workers: 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern 3: Dynamic Resource Names**\n",
    "\n",
    "**Use case:** Unique names per environment\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    customer_etl:\n",
    "      name: \"[${bundle.target}] Customer ETL - ${bundle.name}\"\n",
    "  \n",
    "  pipelines:\n",
    "    data_pipeline:\n",
    "      name: \"[${bundle.target}] Data Pipeline - ${bundle.name}\"\n",
    "      target: ${var.catalog}.${var.schema}_${bundle.target}\n",
    "```\n",
    "\n",
    "**Results in:**\n",
    "* Dev: `[dev] Customer ETL - customer_analytics`\n",
    "* Prod: `[prod] Customer ETL - customer_analytics`\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern 4: Conditional Resources**\n",
    "\n",
    "**Use case:** Deploy different resources per environment\n",
    "\n",
    "```yaml\n",
    "targets:\n",
    "  dev:\n",
    "    resources:\n",
    "      jobs:\n",
    "        # Dev-only job for testing\n",
    "        test_job:\n",
    "          name: \"Test Job\"\n",
    "          tasks:\n",
    "            - notebook_task:\n",
    "                notebook_path: ../src/test.py\n",
    "  \n",
    "  prod:\n",
    "    resources:\n",
    "      jobs:\n",
    "        # Prod-only monitoring job\n",
    "        monitoring_job:\n",
    "          name: \"Monitoring Job\"\n",
    "          schedule:\n",
    "            quartz_cron_expression: \"0 */5 * * * ?\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern 5: Parameterized Notebooks**\n",
    "\n",
    "**Use case:** Pass bundle variables to notebooks\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    etl:\n",
    "      tasks:\n",
    "        - notebook_task:\n",
    "            notebook_path: ../src/etl.py\n",
    "            base_parameters:\n",
    "              catalog: ${var.catalog}\n",
    "              schema: ${var.schema}\n",
    "              environment: ${bundle.target}\n",
    "              run_date: \"{{job.start_time.iso_date}}\"\n",
    "```\n",
    "\n",
    "**In notebook:**\n",
    "```python\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema\", \"\")\n",
    "dbutils.widgets.text(\"environment\", \"\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e3321b-af81-49ab-94ff-c33254d49abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "### ğŸ”§ Troubleshooting Common Issues\n",
    "\n",
    "**Solutions to common problems:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Issue 1: Bundle Validation Fails**\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: yaml: line 10: mapping values are not allowed in this context\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "* Check YAML indentation (use 2 spaces)\n",
    "* Ensure colons have space after them\n",
    "* Validate YAML syntax online\n",
    "\n",
    "---\n",
    "\n",
    "### **Issue 2: File Not Found**\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: notebook_path: ../src/notebook.py: file not found\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "* Check file path is relative to resource file\n",
    "* Ensure file exists in bundle\n",
    "* Use correct path separator (`/`)\n",
    "\n",
    "---\n",
    "\n",
    "### **Issue 3: Variable Not Defined**\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: variable \"catalog\" is not defined\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```yaml\n",
    "# Define variable in databricks.yml\n",
    "variables:\n",
    "  catalog:\n",
    "    description: Catalog name\n",
    "    default: dev_catalog\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Issue 4: Permission Denied**\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: User does not have permission to create job\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "* Check workspace permissions\n",
    "* Ensure user has `CAN_MANAGE` on workspace\n",
    "* For production, use service principal\n",
    "\n",
    "---\n",
    "\n",
    "### **Issue 5: Deployment Fails**\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: Failed to upload files to workspace\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "* Check network connectivity\n",
    "* Verify authentication (token not expired)\n",
    "* Ensure workspace path is accessible\n",
    "\n",
    "---\n",
    "\n",
    "### **Issue 6: Resource Already Exists**\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: Job with name \"Customer ETL\" already exists\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "* Use unique names per target: `\"${bundle.target} - Customer ETL\"`\n",
    "* Or destroy existing resources: `databricks bundle destroy`\n",
    "\n",
    "---\n",
    "\n",
    "### **Debugging Tips:**\n",
    "\n",
    "**1. Verbose output:**\n",
    "```bash\n",
    "databricks bundle deploy -t dev --debug\n",
    "```\n",
    "\n",
    "**2. Dry run:**\n",
    "```bash\n",
    "# See what would be deployed without deploying\n",
    "databricks bundle deploy -t dev --dry-run\n",
    "```\n",
    "\n",
    "**3. Check deployed resources:**\n",
    "```bash\n",
    "databricks bundle summary -t dev\n",
    "```\n",
    "\n",
    "**4. View bundle configuration:**\n",
    "```bash\n",
    "# See resolved configuration\n",
    "databricks bundle validate -t dev --output json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be5bb75-4cb1-4a6c-bc03-5dc4b86e1f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Summary: Asset Bundles Mastery ğŸ“\n",
    "\n",
    "Congratulations! You've learned how to use Databricks Asset Bundles.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts:**\n",
    "\n",
    "**1. What are Bundles:**\n",
    "* âœ… Infrastructure as Code for Databricks\n",
    "* âœ… Package projects as deployable units\n",
    "* âœ… Enable CI/CD and best practices\n",
    "* âœ… Version control everything\n",
    "\n",
    "**2. Bundle Structure:**\n",
    "* âœ… `databricks.yml` - Main configuration\n",
    "* âœ… `resources/` - Job and pipeline definitions\n",
    "* âœ… `src/` - Source code and notebooks\n",
    "* âœ… `tests/` - Unit and integration tests\n",
    "\n",
    "**3. Key Commands:**\n",
    "* âœ… `databricks bundle init` - Create new bundle\n",
    "* âœ… `databricks bundle validate` - Check configuration\n",
    "* âœ… `databricks bundle deploy` - Deploy to environment\n",
    "* âœ… `databricks bundle run` - Execute resources\n",
    "* âœ… `databricks bundle destroy` - Remove resources\n",
    "\n",
    "**4. Targets:**\n",
    "* âœ… Define multiple environments (dev, staging, prod)\n",
    "* âœ… Different configurations per target\n",
    "* âœ… Use variables for flexibility\n",
    "\n",
    "**5. CI/CD:**\n",
    "* âœ… Automate testing and deployment\n",
    "* âœ… Use GitHub Actions or similar\n",
    "* âœ… Deploy on merge to main\n",
    "* âœ… Run tests before deployment\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices:**\n",
    "\n",
    "* âœ… Use consistent project structure\n",
    "* âœ… Define reusable variables\n",
    "* âœ… Separate environments with targets\n",
    "* âœ… Use service principals in production\n",
    "* âœ… Tag all resources\n",
    "* âœ… Document your bundle\n",
    "* âœ… Test before deploying\n",
    "* âœ… Version control with Git\n",
    "* âœ… Implement CI/CD pipelines\n",
    "* âœ… Modularize configuration\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "1. **Create your first bundle:**\n",
    "   * Initialize with template\n",
    "   * Configure for your project\n",
    "   * Deploy to dev environment\n",
    "\n",
    "2. **Set up CI/CD:**\n",
    "   * Create GitHub repository\n",
    "   * Add GitHub Actions workflow\n",
    "   * Automate deployments\n",
    "\n",
    "3. **Expand your bundle:**\n",
    "   * Add more jobs and pipelines\n",
    "   * Implement testing\n",
    "   * Deploy to production\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources:**\n",
    "\n",
    "* [Databricks Asset Bundles Documentation](https://docs.databricks.com/dev-tools/bundles/)\n",
    "* [Bundle Configuration Reference](https://docs.databricks.com/dev-tools/bundles/settings/)\n",
    "* [Bundle Templates](https://docs.databricks.com/dev-tools/bundles/templates/)\n",
    "* [Databricks CLI Documentation](https://docs.databricks.com/dev-tools/cli/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy bundling!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f14485-5107-429d-ba42-64b914ed10bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%undefined\n",
    "## Hands-On Exercise: Create Your Bundle ğŸ¯\n",
    "\n",
    "**Follow these steps to create your first bundle:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Prerequisites**\n",
    "\n",
    "```bash\n",
    "# Install Databricks CLI\n",
    "pip install databricks-cli\n",
    "\n",
    "# Authenticate\n",
    "databricks auth login --host https://your-workspace.cloud.databricks.com\n",
    "\n",
    "# Verify\n",
    "databricks workspace list /\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Initialize Bundle**\n",
    "\n",
    "```bash\n",
    "# Create project directory\n",
    "mkdir ~/projects/my_first_bundle\n",
    "cd ~/projects/my_first_bundle\n",
    "\n",
    "# Initialize bundle\n",
    "databricks bundle init default-python\n",
    "\n",
    "# When prompted:\n",
    "# - Project name: my_first_bundle\n",
    "# - Workspace host: (your workspace URL)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Explore Structure**\n",
    "\n",
    "```bash\n",
    "# View created files\n",
    "tree .\n",
    "\n",
    "# Should see:\n",
    "# .\n",
    "# â”œâ”€â”€ databricks.yml\n",
    "# â”œâ”€â”€ README.md\n",
    "# â”œâ”€â”€ resources/\n",
    "# â”‚   â””â”€â”€ my_first_bundle_job.yml\n",
    "# â””â”€â”€ src/\n",
    "#     â””â”€â”€ notebook.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Customize Configuration**\n",
    "\n",
    "**Edit `databricks.yml`:**\n",
    "\n",
    "```yaml\n",
    "bundle:\n",
    "  name: my_first_bundle\n",
    "\n",
    "include:\n",
    "  - resources/*.yml\n",
    "\n",
    "variables:\n",
    "  catalog:\n",
    "    default: main\n",
    "  schema:\n",
    "    default: default\n",
    "\n",
    "targets:\n",
    "  dev:\n",
    "    mode: development\n",
    "    default: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Validate**\n",
    "\n",
    "```bash\n",
    "# Validate configuration\n",
    "databricks bundle validate\n",
    "\n",
    "# Should see:\n",
    "# Name: my_first_bundle\n",
    "# Target: dev\n",
    "# Validation OK!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Deploy**\n",
    "\n",
    "```bash\n",
    "# Deploy to dev\n",
    "databricks bundle deploy -t dev\n",
    "\n",
    "# Should see:\n",
    "# Uploading my_first_bundle to workspace...\n",
    "# Deploying resources...\n",
    "#   âœ“ Job: dev - my_first_bundle_job\n",
    "# Deployment complete!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Run**\n",
    "\n",
    "```bash\n",
    "# Run the job\n",
    "databricks bundle run my_first_bundle_job -t dev\n",
    "\n",
    "# Monitor in UI or wait for completion\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Version Control**\n",
    "\n",
    "```bash\n",
    "# Initialize Git\n",
    "git init\n",
    "\n",
    "# Create .gitignore\n",
    "echo \".databricks/\" > .gitignore\n",
    "\n",
    "# Commit\n",
    "git add .\n",
    "git commit -m \"Initial bundle\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 9: Make Changes**\n",
    "\n",
    "```bash\n",
    "# Edit job configuration\n",
    "vim resources/my_first_bundle_job.yml\n",
    "\n",
    "# Validate changes\n",
    "databricks bundle validate\n",
    "\n",
    "# Redeploy\n",
    "databricks bundle deploy -t dev\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 10: Clean Up**\n",
    "\n",
    "```bash\n",
    "# Remove deployed resources\n",
    "databricks bundle destroy -t dev\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You've created and deployed your first bundle!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28075c06-35d8-40e1-b636-488c0fe6c3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%undefined\n",
    "## Complete Example Bundle Files ğŸ“š\n",
    "\n",
    "**Copy these files to create a working bundle:**\n",
    "\n",
    "---\n",
    "\n",
    "### **File 1: databricks.yml**\n",
    "\n",
    "```yaml\n",
    "# Databricks Asset Bundle Configuration\n",
    "# This is the main configuration file for the customer_analytics bundle\n",
    "\n",
    "bundle:\n",
    "  name: customer_analytics\n",
    "\n",
    "# Include all resource definition files\n",
    "include:\n",
    "  - resources/**/*.yml\n",
    "\n",
    "# Define reusable variables\n",
    "variables:\n",
    "  # Unity Catalog configuration\n",
    "  catalog:\n",
    "    description: Unity Catalog name\n",
    "    default: dev_catalog\n",
    "  \n",
    "  schema:\n",
    "    description: Schema name within the catalog\n",
    "    default: analytics\n",
    "  \n",
    "  # Notification settings\n",
    "  notification_email:\n",
    "    description: Email address for job notifications\n",
    "    default: data-team@company.com\n",
    "  \n",
    "  # Environment identifier\n",
    "  environment:\n",
    "    description: Environment name\n",
    "    default: development\n",
    "\n",
    "# Workspace configuration\n",
    "workspace:\n",
    "  root_path: ~/.bundle/${bundle.name}/${bundle.target}\n",
    "  file_path: /Workspace${workspace.root_path}/files\n",
    "  artifact_path: /Workspace${workspace.root_path}/artifacts\n",
    "\n",
    "# Deployment targets (environments)\n",
    "targets:\n",
    "  # Development environment\n",
    "  dev:\n",
    "    mode: development\n",
    "    default: true\n",
    "    workspace:\n",
    "      host: https://your-workspace.cloud.databricks.com\n",
    "    variables:\n",
    "      catalog: dev_catalog\n",
    "      schema: dev_analytics\n",
    "      environment: development\n",
    "  \n",
    "  # Staging environment\n",
    "  staging:\n",
    "    mode: development\n",
    "    workspace:\n",
    "      host: https://your-workspace.cloud.databricks.com\n",
    "    variables:\n",
    "      catalog: staging_catalog\n",
    "      schema: staging_analytics\n",
    "      environment: staging\n",
    "  \n",
    "  # Production environment\n",
    "  prod:\n",
    "    mode: production\n",
    "    workspace:\n",
    "      host: https://your-workspace.cloud.databricks.com\n",
    "      root_path: /Shared/.bundle/prod/${bundle.name}\n",
    "    \n",
    "    # Run as service principal in production\n",
    "    run_as:\n",
    "      service_principal_name: prod-analytics-sp\n",
    "    \n",
    "    variables:\n",
    "      catalog: prod_catalog\n",
    "      schema: prod_analytics\n",
    "      environment: production\n",
    "    \n",
    "    # Production permissions\n",
    "    permissions:\n",
    "      - level: CAN_VIEW\n",
    "        group_name: data-analysts\n",
    "      - level: CAN_MANAGE\n",
    "        group_name: data-engineers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **File 2: resources/jobs/etl_job.yml**\n",
    "\n",
    "```yaml\n",
    "# Customer ETL Job Configuration\n",
    "# This job ingests and transforms customer data\n",
    "\n",
    "resources:\n",
    "  jobs:\n",
    "    customer_etl:\n",
    "      name: \"[${bundle.target}] Customer ETL Job\"\n",
    "      \n",
    "      tasks:\n",
    "        # Task 1: Ingest raw data to bronze layer\n",
    "        - task_key: bronze_ingestion\n",
    "          description: \"Ingest raw customer data from source\"\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebooks/bronze_ingestion.py\n",
    "            base_parameters:\n",
    "              catalog: ${var.catalog}\n",
    "              schema: ${var.schema}\n",
    "              environment: ${var.environment}\n",
    "          new_cluster:\n",
    "            spark_version: 14.3.x-scala2.12\n",
    "            node_type_id: i3.xlarge\n",
    "            num_workers: 2\n",
    "            spark_conf:\n",
    "              spark.databricks.delta.preview.enabled: \"true\"\n",
    "        \n",
    "        # Task 2: Transform to silver layer\n",
    "        - task_key: silver_transform\n",
    "          description: \"Clean and transform data to silver layer\"\n",
    "          depends_on:\n",
    "            - task_key: bronze_ingestion\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebooks/silver_transform.py\n",
    "            base_parameters:\n",
    "              catalog: ${var.catalog}\n",
    "              schema: ${var.schema}\n",
    "              environment: ${var.environment}\n",
    "          new_cluster:\n",
    "            spark_version: 14.3.x-scala2.12\n",
    "            node_type_id: i3.xlarge\n",
    "            num_workers: 2\n",
    "        \n",
    "        # Task 3: Aggregate to gold layer\n",
    "        - task_key: gold_aggregation\n",
    "          description: \"Create business aggregations in gold layer\"\n",
    "          depends_on:\n",
    "            - task_key: silver_transform\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebooks/gold_aggregation.py\n",
    "            base_parameters:\n",
    "              catalog: ${var.catalog}\n",
    "              schema: ${var.schema}\n",
    "              environment: ${var.environment}\n",
    "          new_cluster:\n",
    "            spark_version: 14.3.x-scala2.12\n",
    "            node_type_id: i3.xlarge\n",
    "            num_workers: 2\n",
    "      \n",
    "      # Schedule: Run daily at 2 AM UTC\n",
    "      schedule:\n",
    "        quartz_cron_expression: \"0 0 2 * * ?\"\n",
    "        timezone_id: \"UTC\"\n",
    "        pause_status: UNPAUSED\n",
    "      \n",
    "      # Email notifications\n",
    "      email_notifications:\n",
    "        on_failure:\n",
    "          - ${var.notification_email}\n",
    "        no_alert_for_skipped_runs: true\n",
    "      \n",
    "      # Retry configuration\n",
    "      max_retries: 2\n",
    "      min_retry_interval_millis: 300000  # 5 minutes\n",
    "      \n",
    "      # Timeout\n",
    "      timeout_seconds: 7200  # 2 hours\n",
    "      \n",
    "      # Tags for organization\n",
    "      tags:\n",
    "        environment: ${var.environment}\n",
    "        project: customer_analytics\n",
    "        team: data-engineering\n",
    "        cost_center: analytics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **File 3: resources/pipelines/data_pipeline.yml**\n",
    "\n",
    "```yaml\n",
    "# Customer Data Pipeline Configuration\n",
    "# Delta Live Tables pipeline for customer analytics\n",
    "\n",
    "resources:\n",
    "  pipelines:\n",
    "    customer_pipeline:\n",
    "      name: \"[${bundle.target}] Customer Data Pipeline\"\n",
    "      \n",
    "      # Target catalog and schema\n",
    "      target: ${var.catalog}.${var.schema}\n",
    "      \n",
    "      # Pipeline libraries (notebooks)\n",
    "      libraries:\n",
    "        - notebook:\n",
    "            path: ../src/pipelines/customer_dlt_pipeline.py\n",
    "      \n",
    "      # Pipeline configuration\n",
    "      configuration:\n",
    "        catalog: ${var.catalog}\n",
    "        schema: ${var.schema}\n",
    "        environment: ${var.environment}\n",
    "      \n",
    "      # Cluster configuration\n",
    "      clusters:\n",
    "        - label: default\n",
    "          autoscale:\n",
    "            min_workers: 1\n",
    "            max_workers: 5\n",
    "            mode: ENHANCED\n",
    "      \n",
    "      # Development mode (set to false for production)\n",
    "      development: true\n",
    "      \n",
    "      # Continuous or triggered\n",
    "      continuous: false\n",
    "      \n",
    "      # Channel (CURRENT or PREVIEW)\n",
    "      channel: CURRENT\n",
    "      \n",
    "      # Notifications\n",
    "      notifications:\n",
    "        - email_recipients:\n",
    "            - ${var.notification_email}\n",
    "          alerts:\n",
    "            - on-update-failure\n",
    "            - on-flow-failure\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **File 4: src/notebooks/bronze_ingestion.py**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Bronze Layer: Customer Data Ingestion\n",
    "# MAGIC \n",
    "# MAGIC This notebook ingests raw customer data into the bronze layer.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog\", \"dev_catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"analytics\")\n",
    "dbutils.widgets.text(\"environment\", \"development\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "\n",
    "print(f\"âš™ï¸ Configuration:\")\n",
    "print(f\"  Catalog: {catalog}\")\n",
    "print(f\"  Schema: {schema}\")\n",
    "print(f\"  Environment: {environment}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Ingest Customer Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# Read from sample data (replace with your actual source)\n",
    "df = (\n",
    "    spark.read\n",
    "    .table(\"samples.tpch.customer\")\n",
    "    .select(\n",
    "        col(\"c_custkey\").alias(\"customer_id\"),\n",
    "        col(\"c_name\").alias(\"customer_name\"),\n",
    "        col(\"c_address\").alias(\"address\"),\n",
    "        col(\"c_phone\").alias(\"phone\"),\n",
    "        col(\"c_mktsegment\").alias(\"market_segment\"),\n",
    "        col(\"c_acctbal\").alias(\"account_balance\"),\n",
    "        current_timestamp().alias(\"ingestion_timestamp\"),\n",
    "        lit(environment).alias(\"source_environment\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Ingested {df.count():,} customer records\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write to Bronze Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "\n",
    "# Write to bronze table\n",
    "target_table = f\"{catalog}.{schema}.bronze_customers\"\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "print(f\"âœ… Data written to {target_table}\")\n",
    "print(f\"ğŸ“Š Record count: {spark.table(target_table).count():,}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **File 5: src/notebooks/silver_transform.py**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Silver Layer: Customer Data Transformation\n",
    "# MAGIC \n",
    "# MAGIC This notebook cleans and transforms customer data for the silver layer.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog\", \"dev_catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"analytics\")\n",
    "dbutils.widgets.text(\"environment\", \"development\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "\n",
    "print(f\"âš™ï¸ Configuration:\")\n",
    "print(f\"  Catalog: {catalog}\")\n",
    "print(f\"  Schema: {schema}\")\n",
    "print(f\"  Environment: {environment}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read Bronze Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read bronze table\n",
    "bronze_table = f\"{catalog}.{schema}.bronze_customers\"\n",
    "df_bronze = spark.table(bronze_table)\n",
    "\n",
    "print(f\"ğŸ“Š Bronze records: {df_bronze.count():,}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Clean and Transform\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Clean and transform data\n",
    "df_silver = (\n",
    "    df_bronze\n",
    "    # Remove nulls\n",
    "    .filter(col(\"customer_id\").isNotNull())\n",
    "    .filter(col(\"customer_name\").isNotNull())\n",
    "    \n",
    "    # Clean phone numbers (remove spaces and dashes)\n",
    "    .withColumn(\"phone_clean\", regexp_replace(col(\"phone\"), \"[\\\\s-]\", \"\"))\n",
    "    \n",
    "    # Standardize market segment\n",
    "    .withColumn(\"market_segment_clean\", upper(trim(col(\"market_segment\"))))\n",
    "    \n",
    "    # Add data quality flags\n",
    "    .withColumn(\"has_valid_balance\", col(\"account_balance\") > 0)\n",
    "    .withColumn(\"has_valid_phone\", length(col(\"phone_clean\")) >= 10)\n",
    "    \n",
    "    # Add processing timestamp\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    \n",
    "    # Select final columns\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        \"customer_name\",\n",
    "        \"address\",\n",
    "        col(\"phone_clean\").alias(\"phone\"),\n",
    "        col(\"market_segment_clean\").alias(\"market_segment\"),\n",
    "        \"account_balance\",\n",
    "        \"has_valid_balance\",\n",
    "        \"has_valid_phone\",\n",
    "        \"ingestion_timestamp\",\n",
    "        \"processed_timestamp\",\n",
    "        \"source_environment\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Silver records: {df_silver.count():,}\")\n",
    "print(f\"âœ… Data quality: {df_silver.filter(col('has_valid_balance') & col('has_valid_phone')).count():,} valid records\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write to Silver Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Write to silver table\n",
    "target_table = f\"{catalog}.{schema}.silver_customers\"\n",
    "\n",
    "df_silver.write.mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "print(f\"âœ… Data written to {target_table}\")\n",
    "print(f\"ğŸ“Š Record count: {spark.table(target_table).count():,}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **File 6: src/notebooks/gold_aggregation.py**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Gold Layer: Customer Analytics\n",
    "# MAGIC \n",
    "# MAGIC This notebook creates business aggregations for the gold layer.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog\", \"dev_catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"analytics\")\n",
    "dbutils.widgets.text(\"environment\", \"development\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "\n",
    "print(f\"âš™ï¸ Configuration:\")\n",
    "print(f\"  Catalog: {catalog}\")\n",
    "print(f\"  Schema: {schema}\")\n",
    "print(f\"  Environment: {environment}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read Silver Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read silver table\n",
    "silver_table = f\"{catalog}.{schema}.silver_customers\"\n",
    "df_silver = spark.table(silver_table)\n",
    "\n",
    "print(f\"ğŸ“Š Silver records: {df_silver.count():,}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Create Business Aggregations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Aggregate by market segment\n",
    "df_gold = (\n",
    "    df_silver\n",
    "    .groupBy(\"market_segment\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        sum(\"account_balance\").alias(\"total_balance\"),\n",
    "        avg(\"account_balance\").alias(\"avg_balance\"),\n",
    "        min(\"account_balance\").alias(\"min_balance\"),\n",
    "        max(\"account_balance\").alias(\"max_balance\"),\n",
    "        sum(when(col(\"has_valid_balance\"), 1).otherwise(0)).alias(\"valid_balance_count\"),\n",
    "        sum(when(col(\"has_valid_phone\"), 1).otherwise(0)).alias(\"valid_phone_count\")\n",
    "    )\n",
    "    .withColumn(\"valid_balance_pct\", round(col(\"valid_balance_count\") / col(\"customer_count\") * 100, 2))\n",
    "    .withColumn(\"valid_phone_pct\", round(col(\"valid_phone_count\") / col(\"customer_count\") * 100, 2))\n",
    "    .withColumn(\"aggregation_timestamp\", current_timestamp())\n",
    "    .withColumn(\"environment\", lit(environment))\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Market segments: {df_gold.count()}\")\n",
    "df_gold.show()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Write to Gold Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Write to gold table\n",
    "target_table = f\"{catalog}.{schema}.gold_customer_segments\"\n",
    "\n",
    "df_gold.write.mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "print(f\"âœ… Data written to {target_table}\")\n",
    "print(f\"ğŸ“Š Segment count: {spark.table(target_table).count()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **File 7: .gitignore**\n",
    "\n",
    "```\n",
    "# Databricks\n",
    ".databricks/\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "build/\n",
    "develop-eggs/\n",
    "dist/\n",
    "downloads/\n",
    "eggs/\n",
    ".eggs/\n",
    "lib/\n",
    "lib64/\n",
    "parts/\n",
    "sdist/\n",
    "var/\n",
    "wheels/\n",
    "*.egg-info/\n",
    ".installed.cfg\n",
    "*.egg\n",
    "venv/\n",
    ".venv/\n",
    "ENV/\n",
    "env/\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "*~\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Logs\n",
    "*.log\n",
    "\n",
    "# Testing\n",
    ".pytest_cache/\n",
    ".coverage\n",
    "htmlcov/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **File 8: README.md**\n",
    "\n",
    "```markdown\n",
    "# Customer Analytics Bundle\n",
    "\n",
    "Databricks Asset Bundle for customer analytics workflows.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This bundle contains:\n",
    "- ETL jobs for customer data processing\n",
    "- Delta Live Tables pipeline for data transformation\n",
    "- Bronze, silver, and gold layer notebooks\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "customer_analytics/\n",
    "â”œâ”€â”€ databricks.yml              # Main bundle configuration\n",
    "â”œâ”€â”€ README.md                   # This file\n",
    "â”œâ”€â”€ .gitignore                  # Git ignore rules\n",
    "â”œâ”€â”€ resources/                  # Resource definitions\n",
    "â”‚   â”œâ”€â”€ jobs/\n",
    "â”‚   â”‚   â””â”€â”€ etl_job.yml        # ETL job configuration\n",
    "â”‚   â””â”€â”€ pipelines/\n",
    "â”‚       â””â”€â”€ data_pipeline.yml  # DLT pipeline configuration\n",
    "â””â”€â”€ src/                        # Source code\n",
    "    â”œâ”€â”€ notebooks/\n",
    "    â”‚   â”œâ”€â”€ bronze_ingestion.py\n",
    "    â”‚   â”œâ”€â”€ silver_transform.py\n",
    "    â”‚   â””â”€â”€ gold_aggregation.py\n",
    "    â””â”€â”€ pipelines/\n",
    "        â””â”€â”€ customer_dlt_pipeline.py\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Databricks CLI installed\n",
    "- Databricks workspace access\n",
    "- Unity Catalog enabled\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### 1. Install Databricks CLI\n",
    "\n",
    "```bash\n",
    "pip install databricks-cli\n",
    "```\n",
    "\n",
    "### 2. Authenticate\n",
    "\n",
    "```bash\n",
    "databricks auth login --host https://your-workspace.cloud.databricks.com\n",
    "```\n",
    "\n",
    "### 3. Validate Bundle\n",
    "\n",
    "```bash\n",
    "databricks bundle validate\n",
    "```\n",
    "\n",
    "### 4. Deploy to Development\n",
    "\n",
    "```bash\n",
    "databricks bundle deploy -t dev\n",
    "```\n",
    "\n",
    "### 5. Run ETL Job\n",
    "\n",
    "```bash\n",
    "databricks bundle run customer_etl -t dev\n",
    "```\n",
    "\n",
    "## Environments\n",
    "\n",
    "### Development (`dev`)\n",
    "- Default target\n",
    "- Uses `dev_catalog.dev_analytics`\n",
    "- Personal workspace paths\n",
    "\n",
    "### Staging (`staging`)\n",
    "- Pre-production testing\n",
    "- Uses `staging_catalog.staging_analytics`\n",
    "\n",
    "### Production (`prod`)\n",
    "- Production environment\n",
    "- Uses `prod_catalog.prod_analytics`\n",
    "- Runs as service principal\n",
    "- Shared workspace paths\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Variables\n",
    "\n",
    "Customize these variables in `databricks.yml`:\n",
    "\n",
    "- `catalog` - Unity Catalog name\n",
    "- `schema` - Schema name\n",
    "- `notification_email` - Email for job notifications\n",
    "- `environment` - Environment identifier\n",
    "\n",
    "### Override at Runtime\n",
    "\n",
    "```bash\n",
    "databricks bundle deploy -t dev --var=\"catalog=custom_catalog\"\n",
    "```\n",
    "\n",
    "## CI/CD\n",
    "\n",
    "This bundle includes GitHub Actions workflows for automated deployment.\n",
    "\n",
    "See `.github/workflows/deploy.yml` for details.\n",
    "\n",
    "## Owners\n",
    "\n",
    "- **Team:** Data Engineering\n",
    "- **Contact:** data-team@company.com\n",
    "- **Slack:** #data-engineering\n",
    "\n",
    "## License\n",
    "\n",
    "Internal use only.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¾ Save these files to create your bundle project!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe3e73d-bf8b-4386-ad24-c3093466b7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Asset Bundles",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
