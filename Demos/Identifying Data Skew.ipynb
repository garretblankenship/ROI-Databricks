{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec97117c-c8a1-4bf7-a084-0488497ef993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Identifying and Debugging Data Skew - Interactive Demo\n",
    "\n",
    "Welcome! This demo will teach you how to identify and debug one of the most common Spark performance issues: **data skew**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà What is Data Skew?\n",
    "\n",
    "**Data skew** occurs when data is unevenly distributed across partitions, causing some tasks to process significantly more data than others.\n",
    "\n",
    "**Example:**\n",
    "* Partition 1: 1,000 records\n",
    "* Partition 2: 1,000 records  \n",
    "* Partition 3: 1,000 records\n",
    "* Partition 4: **1,000,000 records** ‚Üê SKEWED!\n",
    "\n",
    "The job runs as slow as the slowest task (Partition 4), wasting resources on idle executors.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Why Does Data Skew Matter?\n",
    "\n",
    "* üêå **Slow Performance** - One task takes 100x longer than others\n",
    "* üí∏ **Wasted Resources** - Most executors sit idle waiting for the slow task\n",
    "* üö´ **Job Failures** - Skewed tasks may run out of memory (OOM errors)\n",
    "* üîÑ **Stragglers** - The \"straggler\" task delays the entire job\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. üì¶ **Generate skewed data** for demonstration\n",
    "2. üî¥ **Observe the problem** - See skew in action\n",
    "3. üîç **Identify skew in Spark UI** - Learn what to look for\n",
    "4. üìä **Detect skew programmatically** - Check data distribution\n",
    "5. ‚úÖ **Fix data skew** - Apply mitigation techniques\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e2169e-090e-403d-b8eb-d235c5641f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Generate Skewed Data üì¶\n",
    "\n",
    "We'll create a realistic e-commerce dataset with **intentional data skew**.\n",
    "\n",
    "**Scenario:** Customer order data where:\n",
    "* Most customers have 1-10 orders\n",
    "* One \"power user\" (customer_id = 1) has 500,000 orders\n",
    "* This creates severe skew when grouping by customer_id\n",
    "\n",
    "**Why this matters:** This pattern is common in real-world data:\n",
    "* Popular products with millions of sales\n",
    "* High-volume customers or accounts\n",
    "* Hot keys in time-series data (e.g., peak hours)\n",
    "* Geographic concentrations (e.g., major cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226c588c-64a1-461c-a85d-c5d6dc70f509",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Orders Data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand, when, lit, monotonically_increasing_id\n",
    "import random\n",
    "\n",
    "print(\"Generating skewed customer order data...\")\n",
    "print(\"This will take a moment to create enough data to demonstrate skew.\\n\")\n",
    "\n",
    "# Create a large dataset with intentional skew\n",
    "# Customer 1 will have 500,000 orders (the skewed key)\n",
    "# Other customers (2-1000) will have 1-10 orders each\n",
    "\n",
    "# Generate the skewed customer (customer_id = 1) with 500,000 orders\n",
    "skewed_orders = spark.range(0, 500000).select(\n",
    "    lit(1).alias(\"customer_id\"),\n",
    "    (rand() * 1000).cast(\"int\").alias(\"order_amount\"),\n",
    "    (rand() * 100).cast(\"int\").alias(\"product_id\")\n",
    ")\n",
    "\n",
    "# Generate normal customers (customer_id 2-1000) with 1-10 orders each\n",
    "# Total: ~5,000 orders across 999 customers\n",
    "normal_orders = spark.range(0, 5000).select(\n",
    "    ((rand() * 999) + 2).cast(\"int\").alias(\"customer_id\"),\n",
    "    (rand() * 1000).cast(\"int\").alias(\"order_amount\"),\n",
    "    (rand() * 100).cast(\"int\").alias(\"product_id\")\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "orders_df = skewed_orders.union(normal_orders)\n",
    "\n",
    "print(f\"‚úÖ Generated {orders_df.count():,} total orders\")\n",
    "print(f\"   - Customer 1 (skewed): 500,000 orders\")\n",
    "print(f\"   - Other customers: ~5,000 orders\")\n",
    "print(f\"\\n‚ö†Ô∏è  Customer 1 has 100x more data than all other customers combined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2bedea-6a90-4996-acf5-9c29d0f0d828",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preview the Data"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "print(\"Sample of the orders data:\")\n",
    "display(orders_df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f26840b-ef1a-4f74-b0ea-c7c07613fb5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Data Distribution"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the distribution of orders per customer\n",
    "print(\"Orders per customer (top 10):\")\n",
    "\n",
    "customer_counts = orders_df.groupBy(\"customer_id\").count().orderBy(col(\"count\").desc())\n",
    "display(customer_counts.limit(10))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Notice: Customer 1 has 500,000 orders while others have < 20!\")\n",
    "print(\"This is SEVERE data skew!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10c9d4c4-7c6c-4a3d-adc8-a9cb3cd3fe7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Observe the Skew Problem üî¥\n",
    "\n",
    "Now let's perform operations that will suffer from data skew.\n",
    "\n",
    "**Operations that cause skew:**\n",
    "* **groupBy()** - Shuffles data by key, skewed keys go to one partition\n",
    "* **join()** - Skewed join keys create hot partitions\n",
    "* **distinct()** - Deduplication shuffles by value\n",
    "* **repartition()** - Can create uneven partitions\n",
    "\n",
    "We'll run a **groupBy** aggregation that will clearly show the skew problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd1a6f1-f56e-4da1-9d6c-c3493632756f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Perform Skewed GroupBy"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count\n",
    "\n",
    "print(\"üî¥ Running aggregation with skewed data...\")\n",
    "print(\"This will trigger a shuffle and you'll see skew in the Spark UI.\\n\")\n",
    "\n",
    "# This aggregation will suffer from skew\n",
    "# Customer 1's data will all go to ONE partition\n",
    "customer_summary = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    sum(\"order_amount\").alias(\"total_spent\"),\n",
    "    avg(\"order_amount\").alias(\"avg_order_value\")\n",
    ").orderBy(col(\"total_orders\").desc())\n",
    "\n",
    "print(\"‚è≥ Running query... Watch the Spark UI!\")\n",
    "print(\"\\nüëâ IMPORTANT: Open the Spark UI now to see the skew!\")\n",
    "print(\"   Click on the 'Spark UI' link that appears below after running this cell.\\n\")\n",
    "\n",
    "# Force execution with an action\n",
    "result = customer_summary.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Query completed!\")\n",
    "print(f\"   Processed {len(result):,} customers\")\n",
    "print(\"\\n‚ö†Ô∏è  Did you notice one task took much longer than others?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90b1b36-ecfe-4851-a234-73b56b78970a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Results"
    }
   },
   "outputs": [],
   "source": [
    "# Show the results\n",
    "print(\"Top customers by order count:\")\n",
    "display(customer_summary.limit(10))\n",
    "\n",
    "print(\"\\nüìä Notice: Customer 1 has 500,000 orders while others have < 20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb06efd4-45a9-4c5c-925a-20ccc09e2bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Identify Skew in Spark UI üîç\n",
    "\n",
    "The **Spark UI** is your primary tool for identifying data skew. Let's learn how to use it!\n",
    "\n",
    "### üìç How to Access Spark UI\n",
    "\n",
    "1. **During job execution:** Click the \"Spark UI\" link that appears below the cell\n",
    "2. **After execution:** Go to the cluster page and click \"Spark UI\"\n",
    "3. **From notebook:** Look for the Spark UI icon in the cell output\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What to Look For in Spark UI\n",
    "\n",
    "Data skew shows up in several places in the Spark UI. Here's where to look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a40a018-22c1-44a9-8fe1-8ccaab3e5983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä 1. Jobs Tab - Overview\n",
    "\n",
    "**Navigation:** Spark UI ‚Üí Jobs Tab\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "* **Job Duration** - One job taking much longer than expected\n",
    "* **Active Tasks** - Most tasks complete quickly, but 1-2 tasks still running\n",
    "* **Event Timeline** - Visual representation shows tasks finishing at different times\n",
    "\n",
    "**Signs of Skew:**\n",
    "* ‚ö†Ô∏è Most tasks complete in seconds, but a few take minutes/hours\n",
    "* ‚ö†Ô∏è Job progress stuck at 99% for a long time (waiting for straggler tasks)\n",
    "* ‚ö†Ô∏è Event timeline shows long tail of tasks\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Task 1: ========== (10 seconds)\n",
    "Task 2: ========== (10 seconds)\n",
    "Task 3: ========== (10 seconds)\n",
    "Task 4: ============================================== (5 minutes) ‚Üê SKEWED!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ddff17d-7ce5-4dc8-8eae-f12033c6b576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä 2. Stages Tab - Detailed View\n",
    "\n",
    "**Navigation:** Spark UI ‚Üí Stages Tab ‚Üí Click on a Stage\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "#### **Summary Metrics Section:**\n",
    "\n",
    "Look at the **Task Metrics** table:\n",
    "\n",
    "| Metric | Min | 25th % | Median | 75th % | Max |\n",
    "|--------|-----|--------|--------|--------|-----|\n",
    "| Duration | 1s | 2s | 2s | 3s | **300s** ‚Üê SKEW! |\n",
    "| Input Size | 1 MB | 2 MB | 2 MB | 3 MB | **500 MB** ‚Üê SKEW! |\n",
    "| Records | 1K | 2K | 2K | 3K | **500K** ‚Üê SKEW! |\n",
    "\n",
    "**Signs of Skew:**\n",
    "* ‚ö†Ô∏è **Max >> Median** - Max is 10x-100x larger than median\n",
    "* ‚ö†Ô∏è **Max >> 75th percentile** - One task is an outlier\n",
    "* ‚ö†Ô∏è **Shuffle Read Size** - One task reads much more data\n",
    "* ‚ö†Ô∏è **Shuffle Write Size** - One task writes much more data\n",
    "\n",
    "#### **Tasks Table:**\n",
    "\n",
    "Scroll down to see individual tasks:\n",
    "\n",
    "* **Sort by Duration** - Find the slowest tasks\n",
    "* **Sort by Input Size** - Find tasks processing the most data\n",
    "* **Look for outliers** - One task with 100x more data than others\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Task 0: Duration: 2s,  Input: 2 MB,   Records: 2,000\n",
    "Task 1: Duration: 2s,  Input: 2 MB,   Records: 2,000\n",
    "Task 2: Duration: 2s,  Input: 2 MB,   Records: 2,000\n",
    "Task 3: Duration: 180s, Input: 500 MB, Records: 500,000 ‚Üê SKEWED!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a9eddc-3c52-4856-893b-fb5635066a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä 3. SQL Tab - Query Execution\n",
    "\n",
    "**Navigation:** Spark UI ‚Üí SQL Tab ‚Üí Click on a Query\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "#### **Query Execution Plan:**\n",
    "\n",
    "Look at the visual DAG (Directed Acyclic Graph):\n",
    "\n",
    "* **Exchange nodes** - These are shuffle operations (potential skew points)\n",
    "* **HashAggregate** - GroupBy operations (common skew source)\n",
    "* **SortMergeJoin** - Join operations (common skew source)\n",
    "\n",
    "#### **Metrics for Each Stage:**\n",
    "\n",
    "Click on a stage in the DAG to see:\n",
    "\n",
    "* **Number of output rows** - Shows data distribution\n",
    "* **Data size** - Shows how much data each stage processes\n",
    "* **Time spent** - Shows which stages are slow\n",
    "\n",
    "**Signs of Skew:**\n",
    "* ‚ö†Ô∏è One partition in Exchange has 100x more rows than others\n",
    "* ‚ö†Ô∏è HashAggregate shows uneven data distribution\n",
    "* ‚ö†Ô∏è SortMergeJoin has one side much larger than expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ed7f2e-44e7-4ac8-a6d5-cc530b7f5e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä 4. Executors Tab - Resource Usage\n",
    "\n",
    "**Navigation:** Spark UI ‚Üí Executors Tab\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "#### **Executor Metrics:**\n",
    "\n",
    "| Executor | Tasks | Duration | Input | Shuffle Read | Shuffle Write |\n",
    "|----------|-------|----------|-------|--------------|---------------|\n",
    "| 0 | 10 | 20s | 20 MB | 20 MB | 20 MB |\n",
    "| 1 | 10 | 20s | 20 MB | 20 MB | 20 MB |\n",
    "| 2 | 10 | 20s | 20 MB | 20 MB | 20 MB |\n",
    "| 3 | 1 | **300s** | **500 MB** | **500 MB** | **500 MB** ‚Üê SKEWED! |\n",
    "\n",
    "**Signs of Skew:**\n",
    "* ‚ö†Ô∏è One executor has much longer task duration\n",
    "* ‚ö†Ô∏è One executor processes much more data\n",
    "* ‚ö†Ô∏è Uneven task distribution across executors\n",
    "* ‚ö†Ô∏è Most executors idle while one is working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814c7996-1feb-46d5-9ef3-7b91b73e0de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ Quick Skew Identification Checklist\n",
    "\n",
    "Use this checklist when debugging slow Spark jobs:\n",
    "\n",
    "**‚òê Jobs Tab:**\n",
    "- [ ] Is the job stuck at 99% for a long time?\n",
    "- [ ] Are most tasks complete but 1-2 still running?\n",
    "- [ ] Is there a long tail in the event timeline?\n",
    "\n",
    "**‚òê Stages Tab:**\n",
    "- [ ] Is Max duration >> Median duration (10x or more)?\n",
    "- [ ] Is Max input size >> Median input size?\n",
    "- [ ] Are there outlier tasks in the tasks table?\n",
    "- [ ] Is shuffle read/write size uneven?\n",
    "\n",
    "**‚òê SQL Tab:**\n",
    "- [ ] Do Exchange nodes show uneven data distribution?\n",
    "- [ ] Are HashAggregate or Join operations slow?\n",
    "- [ ] Is one partition processing 100x more rows?\n",
    "\n",
    "**‚òê Executors Tab:**\n",
    "- [ ] Is one executor much busier than others?\n",
    "- [ ] Are most executors idle?\n",
    "- [ ] Is task distribution uneven?\n",
    "\n",
    "**If you checked 3+ boxes, you likely have data skew!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a5e9ba-6a68-4738-8dd5-5977d6f10b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Detect Skew Programmatically üìä\n",
    "\n",
    "You can detect skew **before** running expensive operations by analyzing data distribution.\n",
    "\n",
    "**Why detect skew programmatically?**\n",
    "* ‚úÖ Catch skew early in development\n",
    "* ‚úÖ Automate skew detection in data pipelines\n",
    "* ‚úÖ Monitor data quality over time\n",
    "* ‚úÖ Make informed decisions about optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91077e0c-9464-45f2-9ec0-e55a129e7baa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Method 1: Check Key Distribution"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, max as spark_max, min as spark_min, avg, stddev\n",
    "\n",
    "print(\"üîç Method 1: Analyze key distribution\\n\")\n",
    "\n",
    "# Count records per key\n",
    "key_distribution = orders_df.groupBy(\"customer_id\").count()\n",
    "\n",
    "# Calculate statistics\n",
    "stats = key_distribution.select(\n",
    "    spark_min(\"count\").alias(\"min_records\"),\n",
    "    avg(\"count\").alias(\"avg_records\"),\n",
    "    spark_max(\"count\").alias(\"max_records\"),\n",
    "    stddev(\"count\").alias(\"stddev_records\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Key Distribution Statistics:\")\n",
    "print(f\"  Min records per key:    {stats['min_records']:,}\")\n",
    "print(f\"  Avg records per key:    {stats['avg_records']:,.2f}\")\n",
    "print(f\"  Max records per key:    {stats['max_records']:,}\")\n",
    "print(f\"  Std deviation:          {stats['stddev_records']:,.2f}\")\n",
    "print(f\"\\n  Skew ratio (max/avg):   {stats['max_records'] / stats['avg_records']:.2f}x\")\n",
    "\n",
    "if stats['max_records'] / stats['avg_records'] > 10:\n",
    "    print(\"\\n‚ö†Ô∏è  SEVERE SKEW DETECTED! Max is >10x the average.\")\n",
    "elif stats['max_records'] / stats['avg_records'] > 3:\n",
    "    print(\"\\n‚ö†Ô∏è  MODERATE SKEW DETECTED! Max is >3x the average.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Data distribution looks balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0bcc3da-82b2-4450-9c94-5ddf3defd7b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Method 2: Identify Hot Keys"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîç Method 2: Identify hot keys (top skewed keys)\\n\")\n",
    "\n",
    "# Find the top 10 keys with the most records\n",
    "hot_keys = orders_df.groupBy(\"customer_id\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"Top 10 customers by order count:\")\n",
    "display(hot_keys)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  These are your 'hot keys' that will cause skew in groupBy/join operations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8469e42-c9d1-4588-8f18-d35d4c1fc9f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Method 3: Calculate Skew Percentage"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîç Method 3: Calculate what % of data is in the top key\\n\")\n",
    "\n",
    "# Get total records\n",
    "total_records = orders_df.count()\n",
    "\n",
    "# Get records in the top key\n",
    "top_key_records = hot_keys.first()['count']\n",
    "top_key_id = hot_keys.first()['customer_id']\n",
    "\n",
    "# Calculate percentage\n",
    "skew_percentage = (top_key_records / total_records) * 100\n",
    "\n",
    "print(f\"Total records:           {total_records:,}\")\n",
    "print(f\"Top key (customer {top_key_id}):  {top_key_records:,} records\")\n",
    "print(f\"Skew percentage:         {skew_percentage:.2f}%\")\n",
    "\n",
    "if skew_percentage > 50:\n",
    "    print(\"\\n‚ö†Ô∏è  CRITICAL! One key contains >50% of all data!\")\n",
    "elif skew_percentage > 20:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING! One key contains >20% of all data!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Data distribution is acceptable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "300facd5-f3e1-4b2f-b4f9-9edf67556001",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Method 4: Partition Size Analysis"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîç Method 4: Analyze partition sizes (advanced)\\n\")\n",
    "\n",
    "# Repartition by the skewed key to see partition distribution\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "partitioned_df = orders_df.repartition(4, \"customer_id\")\n",
    "\n",
    "# Count records per partition\n",
    "partition_sizes = partitioned_df.groupBy(spark_partition_id().alias(\"partition_id\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"partition_id\")\n",
    "\n",
    "print(\"Records per partition after repartitioning by customer_id:\")\n",
    "display(partition_sizes)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Notice: One partition has 500,000 records while others have ~1,600!\")\n",
    "print(\"This is what causes the performance problem in Spark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2328947f-966a-4177-a7ae-d8a166759ede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üõ†Ô∏è Reusable Skew Detection Function\n",
    "\n",
    "Here's a function you can use in your own projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae1ae9ba-2710-41ad-8e15-b715375c5f2a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Skew Detection Utility"
    }
   },
   "outputs": [],
   "source": [
    "def detect_skew(df, key_column, threshold=10):\n",
    "    \"\"\"\n",
    "    Detect data skew in a DataFrame by analyzing key distribution.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        key_column: Column to check for skew\n",
    "        threshold: Skew ratio threshold (default: 10x)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with skew metrics and recommendations\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import col, count, max as spark_max, avg\n",
    "    \n",
    "    # Calculate key distribution\n",
    "    key_dist = df.groupBy(key_column).count()\n",
    "    \n",
    "    # Get statistics\n",
    "    stats = key_dist.select(\n",
    "        avg(\"count\").alias(\"avg\"),\n",
    "        spark_max(\"count\").alias(\"max\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    skew_ratio = stats['max'] / stats['avg']\n",
    "    has_skew = skew_ratio > threshold\n",
    "    \n",
    "    # Get hot keys\n",
    "    hot_keys = key_dist.orderBy(col(\"count\").desc()).limit(5).collect()\n",
    "    \n",
    "    result = {\n",
    "        'has_skew': has_skew,\n",
    "        'skew_ratio': skew_ratio,\n",
    "        'avg_records': stats['avg'],\n",
    "        'max_records': stats['max'],\n",
    "        'hot_keys': [(row[key_column], row['count']) for row in hot_keys],\n",
    "        'recommendation': 'Apply skew mitigation techniques' if has_skew else 'No action needed'\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "print(\"üõ†Ô∏è Testing skew detection function:\\n\")\n",
    "result = detect_skew(orders_df, \"customer_id\", threshold=10)\n",
    "\n",
    "print(f\"Has skew: {result['has_skew']}\")\n",
    "print(f\"Skew ratio: {result['skew_ratio']:.2f}x\")\n",
    "print(f\"Avg records per key: {result['avg_records']:,.2f}\")\n",
    "print(f\"Max records per key: {result['max_records']:,}\")\n",
    "print(f\"\\nTop 5 hot keys:\")\n",
    "for key, count in result['hot_keys']:\n",
    "    print(f\"  Key {key}: {count:,} records\")\n",
    "print(f\"\\nRecommendation: {result['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ca601d8-981d-4e0d-b5b8-e9ff3274f8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Fix Data Skew ‚úÖ\n",
    "\n",
    "Now that we can identify skew, let's learn how to fix it!\n",
    "\n",
    "**Common mitigation strategies:**\n",
    "\n",
    "1. **Salting** - Add random values to break up hot keys\n",
    "2. **Adaptive Query Execution (AQE)** - Let Spark optimize automatically\n",
    "3. **Broadcast Joins** - Avoid shuffling small tables\n",
    "4. **Isolated Processing** - Handle hot keys separately\n",
    "5. **Repartitioning** - Increase parallelism\n",
    "6. **Data Preprocessing** - Fix skew at the source\n",
    "\n",
    "Let's explore each technique!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32830add-c7ef-4134-b8e3-567541e384ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üßÇ Strategy 1: Salting\n",
    "\n",
    "**What is salting?**\n",
    "\n",
    "Add a random \"salt\" value to skewed keys to distribute them across multiple partitions.\n",
    "\n",
    "**How it works:**\n",
    "1. Add a random number (0-N) to the skewed key\n",
    "2. Process data with the salted key\n",
    "3. Aggregate results across salt values\n",
    "\n",
    "**When to use:**\n",
    "* GroupBy operations with hot keys\n",
    "* Aggregations with severe skew\n",
    "* When you can't change the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbdd7a76-1852-4f9d-886a-b4be92f8611a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salting Example"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, floor, concat, lit\n",
    "\n",
    "print(\"üßÇ Applying salting technique...\\n\")\n",
    "\n",
    "# Add a salt column (random number 0-9)\n",
    "# This splits customer 1's data across 10 partitions\n",
    "salted_df = orders_df.withColumn(\n",
    "    \"salt\",\n",
    "    floor(rand() * 10).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"salted_key\",\n",
    "    concat(col(\"customer_id\"), lit(\"_\"), col(\"salt\"))\n",
    ")\n",
    "\n",
    "print(\"Original key distribution (customer 1):\")\n",
    "print(f\"  Customer 1: 500,000 records in 1 partition\\n\")\n",
    "\n",
    "print(\"After salting (customer 1):\")\n",
    "salt_distribution = salted_df.filter(col(\"customer_id\") == 1) \\\n",
    "    .groupBy(\"salted_key\").count() \\\n",
    "    .orderBy(\"salted_key\")\n",
    "\n",
    "display(salt_distribution)\n",
    "\n",
    "print(\"\\n‚úÖ Customer 1's data is now split across 10 keys (~50,000 each)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be7de19-dc47-4d3b-aba0-abe02c2741df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salting: Aggregation"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üßÇ Performing aggregation with salted keys...\\n\")\n",
    "\n",
    "# Step 1: Aggregate by salted key\n",
    "salted_agg = salted_df.groupBy(\"customer_id\", \"salt\").agg(\n",
    "    count(\"*\").alias(\"partial_count\"),\n",
    "    sum(\"order_amount\").alias(\"partial_sum\")\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate across salt values to get final result\n",
    "final_result = salted_agg.groupBy(\"customer_id\").agg(\n",
    "    sum(\"partial_count\").alias(\"total_orders\"),\n",
    "    sum(\"partial_sum\").alias(\"total_spent\")\n",
    ").orderBy(col(\"total_orders\").desc())\n",
    "\n",
    "print(\"‚è≥ Running salted aggregation... This should be faster!\\n\")\n",
    "\n",
    "result = final_result.collect()\n",
    "\n",
    "print(f\"‚úÖ Completed! Processed {len(result):,} customers\")\n",
    "print(\"\\nTop customers:\")\n",
    "display(final_result.limit(5))\n",
    "\n",
    "print(\"\\nüìä Check the Spark UI - tasks should be more evenly distributed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15841639-6d2f-49a4-a483-833652fa941c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚ö° Strategy 2: Adaptive Query Execution (AQE)\n",
    "\n",
    "**What is AQE?**\n",
    "\n",
    "Spark's built-in optimization that automatically handles skew at runtime.\n",
    "\n",
    "**How it works:**\n",
    "* Spark monitors task execution\n",
    "* Detects skewed partitions automatically\n",
    "* Splits large partitions into smaller ones\n",
    "* Reoptimizes the query plan dynamically\n",
    "\n",
    "**When to use:**\n",
    "* Databricks Runtime 7.3+ (enabled by default)\n",
    "* When you want automatic optimization\n",
    "* For complex queries with multiple stages\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18564212-039f-46e6-9422-f7d7e2e97c52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enable AQE"
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚ö° Enabling Adaptive Query Execution (AQE)...\\n\")\n",
    "\n",
    "# Enable AQE (usually enabled by default in Databricks)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# Check current settings\n",
    "print(\"AQE Configuration:\")\n",
    "print(f\"  adaptive.enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"  skewJoin.enabled: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\n",
    "print(f\"  coalescePartitions.enabled: {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\n",
    "\n",
    "print(\"\\n‚úÖ AQE is enabled! Spark will automatically handle skew in joins and aggregations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8577f7a7-644e-40af-9e6b-2b7bd97f16f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "AQE Example"
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚ö° Running aggregation with AQE enabled...\\n\")\n",
    "\n",
    "# Run the same aggregation - AQE will optimize it automatically\n",
    "aqe_result = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    sum(\"order_amount\").alias(\"total_spent\"),\n",
    "    avg(\"order_amount\").alias(\"avg_order_value\")\n",
    ").orderBy(col(\"total_orders\").desc())\n",
    "\n",
    "print(\"‚è≥ Running query with AQE...\\n\")\n",
    "result = aqe_result.collect()\n",
    "\n",
    "print(f\"‚úÖ Completed! AQE automatically optimized the query.\")\n",
    "print(\"\\nüìä Check the Spark UI SQL tab:\")\n",
    "print(\"   Look for 'AQE' annotations in the query plan\")\n",
    "print(\"   You may see 'OptimizeSkewedJoin' or 'CoalesceShufflePartitions'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f731d2c-234e-4356-939f-708a35a3e5ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üì° Strategy 3: Broadcast Join\n",
    "\n",
    "**What is a broadcast join?**\n",
    "\n",
    "Send a small table to all executors instead of shuffling data.\n",
    "\n",
    "**How it works:**\n",
    "* Small table is copied to every executor\n",
    "* No shuffle needed for the small table\n",
    "* Avoids skew in join operations\n",
    "\n",
    "**When to use:**\n",
    "* Joining a large skewed table with a small table\n",
    "* Small table < 10 MB (configurable)\n",
    "* Dimension tables in star schema\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Force broadcast of small table\n",
    "result = large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "* No shuffle for small table\n",
    "* Avoids skew in join keys\n",
    "* Much faster for small dimension tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "863e569d-c278-4d72-bd13-81b76f8038d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ Strategy 4: Isolated Processing\n",
    "\n",
    "**What is isolated processing?**\n",
    "\n",
    "Handle hot keys separately from normal keys.\n",
    "\n",
    "**How it works:**\n",
    "1. Identify hot keys\n",
    "2. Filter data into two DataFrames: hot keys and normal keys\n",
    "3. Process each separately (possibly with different strategies)\n",
    "4. Union the results\n",
    "\n",
    "**When to use:**\n",
    "* Few hot keys with extreme skew\n",
    "* Different processing logic for hot keys\n",
    "* When salting isn't enough\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Separate hot keys\n",
    "hot_key_df = df.filter(col(\"key\") == hot_key_value)\n",
    "normal_df = df.filter(col(\"key\") != hot_key_value)\n",
    "\n",
    "# Process separately\n",
    "hot_result = hot_key_df.groupBy(\"key\").agg(...)\n",
    "normal_result = normal_df.groupBy(\"key\").agg(...)\n",
    "\n",
    "# Combine\n",
    "final_result = hot_result.union(normal_result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400f24e8-d796-4e48-968d-880c7c2a9c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä Comparison of Skew Mitigation Strategies\n",
    "\n",
    "| Strategy | Complexity | Effectiveness | When to Use |\n",
    "|----------|------------|---------------|-------------|\n",
    "| **AQE** | üü¢ Low | üü° Medium-High | Default choice, works automatically |\n",
    "| **Salting** | üü° Medium | üü¢ High | GroupBy with hot keys |\n",
    "| **Broadcast Join** | üü¢ Low | üü¢ High | Joins with small tables |\n",
    "| **Isolated Processing** | üî¥ High | üü¢ High | Few extreme hot keys |\n",
    "| **Repartitioning** | üü¢ Low | üü° Medium | Increase parallelism |\n",
    "| **Data Preprocessing** | üî¥ High | üü¢ High | Fix at source if possible |\n",
    "\n",
    "### üí° Decision Tree\n",
    "\n",
    "```\n",
    "Do you have data skew?\n",
    "‚îî‚îÄ YES ‚Üí Is AQE enabled?\n",
    "    ‚îú‚îÄ NO ‚Üí Enable AQE first!\n",
    "    ‚îî‚îÄ YES ‚Üí Is it a join?\n",
    "        ‚îú‚îÄ YES ‚Üí Is one table small?\n",
    "        ‚îÇ   ‚îú‚îÄ YES ‚Üí Use Broadcast Join\n",
    "        ‚îÇ   ‚îî‚îÄ NO ‚Üí Use Salting or Isolated Processing\n",
    "        ‚îî‚îÄ NO ‚Üí Is it a GroupBy?\n",
    "            ‚îî‚îÄ YES ‚Üí Use Salting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77815ad8-eda7-4c98-88de-950d4468d357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéâ Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ **What is data skew** - Uneven data distribution across partitions  \n",
    "‚úÖ **Why it matters** - Causes slow performance and wasted resources  \n",
    "‚úÖ **How to identify in Spark UI** - Check Jobs, Stages, SQL, and Executors tabs  \n",
    "‚úÖ **How to detect programmatically** - Analyze key distribution and partition sizes  \n",
    "‚úÖ **How to fix it** - Salting, AQE, broadcast joins, and more  \n",
    "\n",
    "---\n",
    "\n",
    "### üìö Best Practices:\n",
    "\n",
    "1. **Enable AQE** - Let Spark handle skew automatically (enabled by default in Databricks)\n",
    "2. **Monitor regularly** - Check Spark UI for skew patterns\n",
    "3. **Detect early** - Use programmatic detection in development\n",
    "4. **Choose the right strategy** - Match the solution to your specific skew pattern\n",
    "5. **Test and measure** - Compare performance before and after optimization\n",
    "6. **Document hot keys** - Keep track of known skewed keys in your data\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "* Apply these techniques to your own datasets\n",
    "* Experiment with different strategies\n",
    "* Monitor Spark UI regularly\n",
    "* Share knowledge with your team\n",
    "* Consider data preprocessing to prevent skew at the source\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Data skew is one of the most common Spark performance issues. \n",
    "Mastering these techniques will make you a more effective data engineer! üí™"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Identifying Data Skew",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
