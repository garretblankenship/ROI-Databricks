{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15587f37-0c11-4ae6-8c8f-8cace77ae6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# COPY INTO & Auto Loader - Incremental Data Ingestion Demo\n",
    "\n",
    "Welcome! This demo will teach you how to efficiently ingest data from cloud storage into Delta Lake.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä The Data Ingestion Challenge\n",
    "\n",
    "**Common scenario:**\n",
    "* New files arrive continuously in cloud storage (S3, ADLS, GCS)\n",
    "* Need to load only NEW files (not reprocess old ones)\n",
    "* Must handle schema changes gracefully\n",
    "* Want reliable, scalable ingestion\n",
    "\n",
    "**Naive approach problems:**\n",
    "```sql\n",
    "-- DON'T DO THIS!\n",
    "SELECT * FROM read_files('/path/to/data/*.csv')  -- Reads ALL files every time!\n",
    "```\n",
    "\n",
    "‚ùå Reprocesses all files every run  \n",
    "‚ùå Wastes time and money  \n",
    "‚ùå No tracking of processed files  \n",
    "‚ùå Doesn't scale  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Databricks Solutions\n",
    "\n",
    "Databricks provides two powerful methods for incremental ingestion:\n",
    "\n",
    "### **1. COPY INTO**\n",
    "* SQL-based command\n",
    "* Idempotent (safe to re-run)\n",
    "* Tracks processed files automatically\n",
    "* Simple syntax\n",
    "* Great for batch ingestion\n",
    "\n",
    "### **2. Auto Loader (cloudFiles)**\n",
    "* Streaming-based approach\n",
    "* Automatic schema inference and evolution\n",
    "* Scalable to millions of files\n",
    "* Built-in error handling\n",
    "* Great for continuous ingestion\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. **COPY INTO** - SQL-based incremental loading\n",
    "2. **Auto Loader** - Streaming-based ingestion\n",
    "3. **Comparison** - When to use each approach\n",
    "4. **Best Practices** - Error handling, monitoring, optimization\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8696904-1cfe-4aa9-b189-8b549c789cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup Demo Environment üõ†Ô∏è\n",
    "\n",
    "We'll create sample data files to demonstrate both COPY INTO and Auto Loader.\n",
    "\n",
    "**Setup steps:**\n",
    "1. Create a Unity Catalog volume for storing files\n",
    "2. Generate sample CSV files (simulating new data arriving)\n",
    "3. Create target Delta tables\n",
    "\n",
    "**Why use Volumes?**\n",
    "* Modern Unity Catalog best practice\n",
    "* Better governance and access control\n",
    "* Works with COPY INTO and Auto Loader\n",
    "* Replaces legacy DBFS paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "104d2c68-09ad-41bd-9bf9-d39cff230a00",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Volume for Demo"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a volume to store our sample data files\n",
    "-- Volumes are the modern way to store files in Unity Catalog\n",
    "\n",
    "CREATE VOLUME IF NOT EXISTS main.default.ingestion_demo_data\n",
    "COMMENT 'Sample data files for COPY INTO and Auto Loader demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10d809ea-ab39-4987-a679-7b03a017364b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Sample Data - Batch 1"
    }
   },
   "outputs": [],
   "source": [
    "# Generate sample customer data as CSV files\n",
    "# We'll create multiple files to simulate incremental data arrival\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"signup_date\", StringType(), True),\n",
    "    StructField(\"total_purchases\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Generate first batch of data (100 customers)\n",
    "data_batch1 = [\n",
    "    (i, f\"Customer_{i}\", f\"customer{i}@example.com\", \n",
    "     random.choice([\"USA\", \"UK\", \"Canada\", \"Germany\", \"France\"]),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "     round(random.uniform(100, 5000), 2))\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "\n",
    "df_batch1 = spark.createDataFrame(data_batch1, schema)\n",
    "\n",
    "# Write to volume as CSV\n",
    "output_path = \"/Volumes/main/default/ingestion_demo_data/customers\"\n",
    "df_batch1.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}/batch1\")\n",
    "\n",
    "print(\"‚úÖ Created batch 1: 100 customers\")\n",
    "print(f\"   Location: {output_path}/batch1/\")\n",
    "display(df_batch1.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1ed5a3b-3cbb-4e0a-aa45-6ce17765b027",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Sample Data - Batch 2"
    }
   },
   "outputs": [],
   "source": [
    "# Generate second batch (simulating new data arriving later)\n",
    "data_batch2 = [\n",
    "    (i, f\"Customer_{i}\", f\"customer{i}@example.com\", \n",
    "     random.choice([\"USA\", \"UK\", \"Canada\", \"Germany\", \"France\"]),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "     round(random.uniform(100, 5000), 2))\n",
    "    for i in range(101, 151)\n",
    "]\n",
    "\n",
    "df_batch2 = spark.createDataFrame(data_batch2, schema)\n",
    "df_batch2.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}/batch2\")\n",
    "\n",
    "print(\"‚úÖ Created batch 2: 50 customers\")\n",
    "print(f\"   Location: {output_path}/batch2/\")\n",
    "display(df_batch2.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df34a8cd-9bd9-4c58-b769-0cd7709d7229",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Sample Data - Batch 3"
    }
   },
   "outputs": [],
   "source": [
    "# Generate third batch with a NEW COLUMN (schema evolution scenario)\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "schema_v2 = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"signup_date\", StringType(), True),\n",
    "    StructField(\"total_purchases\", DoubleType(), True),\n",
    "    StructField(\"is_premium\", BooleanType(), True)  # NEW COLUMN!\n",
    "])\n",
    "\n",
    "data_batch3 = [\n",
    "    (i, f\"Customer_{i}\", f\"customer{i}@example.com\", \n",
    "     random.choice([\"USA\", \"UK\", \"Canada\", \"Germany\", \"France\"]),\n",
    "     (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "     round(random.uniform(100, 5000), 2),\n",
    "     random.choice([True, False]))\n",
    "    for i in range(151, 201)\n",
    "]\n",
    "\n",
    "df_batch3 = spark.createDataFrame(data_batch3, schema_v2)\n",
    "df_batch3.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}/batch3\")\n",
    "\n",
    "print(\"‚úÖ Created batch 3: 50 customers with NEW COLUMN (is_premium)\")\n",
    "print(f\"   Location: {output_path}/batch3/\")\n",
    "print(\"\\n‚ö†Ô∏è  This batch has a different schema - we'll see how each method handles this!\")\n",
    "display(df_batch3.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d1f468d-d875-4666-8c47-b3b9366376b1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Generated Files"
    }
   },
   "outputs": [],
   "source": [
    "# List all the files we created\n",
    "print(\"üìÅ Generated files in volume:\\n\")\n",
    "files = dbutils.fs.ls(output_path)\n",
    "for file_info in files:\n",
    "    print(f\"  {file_info.path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete! We have 3 batches of data ready for ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "405040cf-e119-4968-91ad-013a11fba77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. COPY INTO - SQL-Based Ingestion üìä\n",
    "\n",
    "**What is COPY INTO?**\n",
    "\n",
    "COPY INTO is a SQL command that incrementally loads data from files into a Delta table.\n",
    "\n",
    "**Key features:**\n",
    "* ‚úÖ **Idempotent** - Safe to re-run, won't duplicate data\n",
    "* ‚úÖ **Automatic tracking** - Remembers which files were processed\n",
    "* ‚úÖ **SQL-based** - Familiar syntax, works in SQL warehouses\n",
    "* ‚úÖ **File format support** - CSV, JSON, Parquet, Avro, ORC\n",
    "* ‚úÖ **Pattern matching** - Use wildcards to select files\n",
    "\n",
    "**Basic syntax:**\n",
    "```sql\n",
    "COPY INTO target_table\n",
    "FROM 'source_path'\n",
    "FILEFORMAT = format\n",
    "FORMAT_OPTIONS ('option' = 'value')\n",
    "COPY_OPTIONS ('option' = 'value')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c59814cc-d4fa-465e-8d71-5d650e52a9e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Target Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the target Delta table for COPY INTO\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS main.default.customers_copy_into (\n",
    "  customer_id INT,\n",
    "  name STRING,\n",
    "  email STRING,\n",
    "  country STRING,\n",
    "  signup_date STRING,\n",
    "  total_purchases DOUBLE\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Customer data loaded with COPY INTO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "731fc8f2-550d-4072-8fed-4c868b4ea227",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "First COPY INTO - Load Batch 1"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load the first batch of files\n",
    "-- COPY INTO will track which files it processes\n",
    "\n",
    "COPY INTO main.default.customers_copy_into\n",
    "FROM '/Volumes/main/default/ingestion_demo_data/customers/batch1'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc5a7b5d-2a14-4ca3-b5db-23932871e144",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify First Load"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check what was loaded\n",
    "SELECT COUNT(*) AS row_count FROM main.default.customers_copy_into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c3df70d-eaea-411f-b32d-65cc077710e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Loaded Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View the loaded data\n",
    "SELECT * FROM main.default.customers_copy_into\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f1704be-4429-4bac-9290-a419184d4ee6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Second COPY INTO - Load Batch 2"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load the second batch\n",
    "-- This demonstrates incremental loading\n",
    "\n",
    "COPY INTO main.default.customers_copy_into\n",
    "FROM '/Volumes/main/default/ingestion_demo_data/customers/batch2'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e50119e-dd13-4003-a2b1-2d92a1cbb866",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Incremental Load"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the new count - should be 150 (100 + 50)\n",
    "SELECT COUNT(*) AS row_count FROM main.default.customers_copy_into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8094e760-cb35-4981-bf09-c60f8fe528fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Re-run COPY INTO (Idempotency)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Run COPY INTO again on the same files\n",
    "-- It will NOT reload the same files (idempotent)\n",
    "\n",
    "COPY INTO main.default.customers_copy_into\n",
    "FROM '/Volumes/main/default/ingestion_demo_data/customers/batch1'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5145407-4222-4325-9c1a-e3889bece31b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify No Duplicates"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Count should still be 150 (no duplicates added)\n",
    "SELECT COUNT(*) AS row_count FROM main.default.customers_copy_into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0eb49ed-6273-47a0-b8f8-0b187ae65a4c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "COPY INTO with Pattern Matching"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Use wildcards to load multiple directories at once\n",
    "-- This loads all batches that haven't been processed yet\n",
    "\n",
    "COPY INTO main.default.customers_copy_into\n",
    "FROM '/Volumes/main/default/ingestion_demo_data/customers/'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')  -- Allow schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b1c2841-53df-468d-a709-8ea3abf773bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Final Count"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Should now have all 200 customers (batch1 + batch2 + batch3)\n",
    "-- Note: batch3 has an extra column (is_premium)\n",
    "\n",
    "SELECT COUNT(*) AS row_count FROM main.default.customers_copy_into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b874f27d-a8cd-4bad-b7fb-3697e2778416",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Schema After Evolution"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check if the new column was added\n",
    "DESCRIBE main.default.customers_copy_into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afaf64c6-530d-46c9-9dfd-9e0dc0baa081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìö COPY INTO Options Reference\n",
    "\n",
    "**FORMAT_OPTIONS (file reading):**\n",
    "```sql\n",
    "-- CSV options\n",
    "'header' = 'true'              -- First row is header\n",
    "'inferSchema' = 'true'         -- Infer data types\n",
    "'delimiter' = ','              -- Field delimiter\n",
    "'quote' = '\"'                  -- Quote character\n",
    "'escape' = '\\\\'                -- Escape character\n",
    "'nullValue' = 'NULL'           -- NULL representation\n",
    "\n",
    "-- JSON options\n",
    "'multiLine' = 'true'           -- Multi-line JSON objects\n",
    "'dateFormat' = 'yyyy-MM-dd'    -- Date format\n",
    "```\n",
    "\n",
    "**COPY_OPTIONS (behavior):**\n",
    "```sql\n",
    "'mergeSchema' = 'true'         -- Allow schema evolution\n",
    "'force' = 'true'               -- Reprocess all files (ignore tracking)\n",
    "```\n",
    "\n",
    "**FILE_FORMAT:**\n",
    "* CSV\n",
    "* JSON\n",
    "* PARQUET\n",
    "* AVRO\n",
    "* ORC\n",
    "* BINARYFILE\n",
    "* TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac1fb906-f7bd-4dc1-a6e6-2ae94b4facd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Auto Loader (cloudFiles) ‚ö°\n",
    "\n",
    "**What is Auto Loader?**\n",
    "\n",
    "Auto Loader is Databricks' streaming-based solution for incrementally loading data from cloud storage.\n",
    "\n",
    "**Key features:**\n",
    "* ‚úÖ **Automatic schema inference** - Detects schema from files\n",
    "* ‚úÖ **Schema evolution** - Handles new columns automatically\n",
    "* ‚úÖ **Scalable** - Efficiently processes millions of files\n",
    "* ‚úÖ **Streaming** - Continuous ingestion with low latency\n",
    "* ‚úÖ **File notification** - Uses cloud events (S3 SQS, ADLS Event Grid)\n",
    "* ‚úÖ **Checkpointing** - Tracks progress automatically\n",
    "\n",
    "**Basic syntax:**\n",
    "```python\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n",
    "  .load(source_path)\n",
    "\n",
    "df.writeStream \\\n",
    "  .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "  .toTable(\"target_table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7452f71-47cf-4798-b5e5-48c78ec860d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Auto Loader - Basic Example"
    }
   },
   "outputs": [],
   "source": [
    "# Auto Loader with schema inference\n",
    "# This will automatically detect the schema and load data\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Define paths\n",
    "source_path = \"/Volumes/main/default/ingestion_demo_data/customers/\"\n",
    "checkpoint_path = \"/Volumes/main/default/ingestion_demo_data/checkpoints/autoloader_customers\"\n",
    "target_table = \"main.default.customers_autoloader\"\n",
    "\n",
    "print(\"‚ö° Starting Auto Loader...\\n\")\n",
    "print(f\"Source: {source_path}\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "print(f\"Target: {target_table}\")\n",
    "print(\"\\n‚è≥ This will run as a streaming query...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b68ba8f-1a4b-4040-b12b-eb32445e4506",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Auto Loader - Read Stream"
    }
   },
   "outputs": [],
   "source": [
    "# Read data using Auto Loader (cloudFiles)\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "  .load(source_path)\n",
    "\n",
    "print(\"‚úÖ Auto Loader stream configured\")\n",
    "print(\"\\nInferred schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29d8043f-390c-44ca-bf03-b039d3685092",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Auto Loader - Write Stream"
    }
   },
   "outputs": [],
   "source": [
    "# Write the stream to a Delta table\n",
    "# This starts the streaming query\n",
    "\n",
    "query = df.writeStream \\\n",
    "  .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .trigger(availableNow=True) \\\n",
    "  .toTable(target_table)\n",
    "\n",
    "print(\"‚è≥ Streaming query started...\")\n",
    "print(\"\\nüëâ This will process all files and then stop (trigger=availableNow)\")\n",
    "\n",
    "# Wait for the stream to finish\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"\\n‚úÖ Auto Loader completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9943207e-577e-4617-bc01-8a714932401a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Auto Loader Results"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check what Auto Loader loaded\n",
    "SELECT COUNT(*) AS row_count \n",
    "FROM main.default.customers_autoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b1d2e0-9e01-46c3-a08b-c55c0001f203",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Auto Loader Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View the data loaded by Auto Loader\n",
    "-- Notice it includes the is_premium column from batch3\n",
    "\n",
    "SELECT * \n",
    "FROM main.default.customers_autoloader\n",
    "ORDER BY customer_id\n",
    "LIMIT 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1963bf0-cfa0-4efe-82eb-8fd431f7932a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Schema Evolution"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the schema - should include is_premium column\n",
    "DESCRIBE main.default.customers_autoloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04c47dab-9efd-4166-943d-63eec721606f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Auto Loader Schema Hints\n",
    "\n",
    "**Schema inference modes:**\n",
    "\n",
    "**1. Automatic inference (default):**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "```\n",
    "Auto Loader infers schema from first file.\n",
    "\n",
    "**2. Schema hints (recommended):**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaHints\", \"customer_id INT, name STRING\")\n",
    "```\n",
    "Provide hints for specific columns.\n",
    "\n",
    "**3. Explicit schema:**\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"customer_id\", IntegerType()),\n",
    "  StructField(\"name\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .schema(schema) \\\n",
    "  .load(path)\n",
    "```\n",
    "\n",
    "**Schema evolution modes:**\n",
    "* `addNewColumns` - Add new columns (default)\n",
    "* `rescue` - Put unexpected data in _rescued_data column\n",
    "* `failOnNewColumns` - Fail if schema changes\n",
    "* `none` - No evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29bf5f46-cf83-43a4-851a-0bfaf1e535f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üõ†Ô∏è Auto Loader Advanced Options\n",
    "\n",
    "**File notification modes:**\n",
    "```python\n",
    "# Directory listing (default, works everywhere)\n",
    ".option(\"cloudFiles.useNotifications\", \"false\")\n",
    "\n",
    "# File notification (more efficient for large-scale)\n",
    ".option(\"cloudFiles.useNotifications\", \"true\")\n",
    ".option(\"cloudFiles.queueUrl\", \"s3://bucket/queue\")  # AWS SQS\n",
    "```\n",
    "\n",
    "**Schema evolution:**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    ".option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "```\n",
    "\n",
    "**File filtering:**\n",
    "```python\n",
    ".option(\"cloudFiles.pathGlobFilter\", \"*.csv\")  # Only CSV files\n",
    ".option(\"cloudFiles.modifiedAfter\", \"2024-01-01\")  # Files after date\n",
    "```\n",
    "\n",
    "**Performance:**\n",
    "```python\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\", 1000)  # Limit files per batch\n",
    ".option(\"cloudFiles.maxBytesPerTrigger\", \"10g\")  # Limit data per batch\n",
    "```\n",
    "\n",
    "**Metadata columns:**\n",
    "```python\n",
    ".option(\"cloudFiles.includeExistingFiles\", \"true\")  # Process existing files\n",
    "```\n",
    "\n",
    "Auto Loader automatically adds:\n",
    "* `_metadata.file_path` - Source file path\n",
    "* `_metadata.file_name` - Source file name\n",
    "* `_metadata.file_modification_time` - File timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31722e3-a0a8-4111-8b5d-a015507de62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. COPY INTO vs Auto Loader ü§î\n",
    "\n",
    "Both methods solve the same problem, but have different strengths. Let's compare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f08021f5-9a44-4ce9-b332-ef8990c53045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä Feature Comparison\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader |\n",
    "|---------|-----------|-------------|\n",
    "| **Execution Model** | Batch (SQL) | Streaming (Spark Structured Streaming) |\n",
    "| **Language** | SQL only | Python, Scala, SQL |\n",
    "| **Schema Inference** | Manual or inferSchema | Automatic with evolution |\n",
    "| **Schema Evolution** | mergeSchema option | Built-in, automatic |\n",
    "| **File Tracking** | Automatic (metadata) | Checkpoint files |\n",
    "| **Scalability** | Good (1000s of files) | Excellent (millions of files) |\n",
    "| **Latency** | Minutes (batch) | Seconds (streaming) |\n",
    "| **File Notification** | No | Yes (S3 SQS, ADLS Events) |\n",
    "| **Error Handling** | Fails on error | Rescue columns, dead letter queue |\n",
    "| **Complexity** | Simple | Moderate |\n",
    "| **Cost** | Lower (batch) | Higher (continuous) |\n",
    "| **Use in SQL Warehouse** | ‚úÖ Yes | ‚ùå No (needs cluster) |\n",
    "| **Idempotency** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Metadata Columns** | No | Yes (_metadata.*) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6049980d-e6ec-44d3-b8cb-bab485bf9e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ When to Use COPY INTO\n",
    "\n",
    "**Best for:**\n",
    "\n",
    "‚úÖ **Scheduled batch loads** - Hourly, daily, weekly ingestion  \n",
    "‚úÖ **SQL-only environments** - SQL warehouses, SQL-based pipelines  \n",
    "‚úÖ **Simple use cases** - Straightforward CSV/JSON ingestion  \n",
    "‚úÖ **Small to medium scale** - Up to thousands of files  \n",
    "‚úÖ **Known schema** - Schema doesn't change frequently  \n",
    "‚úÖ **Cost-sensitive** - Lower cost for batch processing  \n",
    "\n",
    "**Example scenarios:**\n",
    "* Daily sales reports from partner systems\n",
    "* Hourly log file ingestion\n",
    "* Weekly data dumps from external sources\n",
    "* One-time historical data loads\n",
    "* SQL-based ETL pipelines\n",
    "\n",
    "**Advantages:**\n",
    "* Simple SQL syntax\n",
    "* Works in SQL warehouses\n",
    "* Easy to understand and debug\n",
    "* Lower cost for batch workloads\n",
    "* No streaming infrastructure needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401dab0f-fa2f-4073-9884-034d41abc605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚ö° When to Use Auto Loader\n",
    "\n",
    "**Best for:**\n",
    "\n",
    "‚úÖ **Continuous ingestion** - Real-time or near-real-time data  \n",
    "‚úÖ **Large scale** - Millions of files  \n",
    "‚úÖ **Schema evolution** - Frequent schema changes  \n",
    "‚úÖ **Complex scenarios** - Need advanced error handling  \n",
    "‚úÖ **Low latency** - Need data available quickly  \n",
    "‚úÖ **Production pipelines** - Enterprise-grade reliability  \n",
    "\n",
    "**Example scenarios:**\n",
    "* IoT sensor data (continuous stream)\n",
    "* Application logs (high volume)\n",
    "* CDC (Change Data Capture) files\n",
    "* Multi-tenant data with varying schemas\n",
    "* Mission-critical data pipelines\n",
    "* Data lakes with millions of files\n",
    "\n",
    "**Advantages:**\n",
    "* Automatic schema inference and evolution\n",
    "* Scales to millions of files\n",
    "* Low latency (seconds)\n",
    "* Built-in error handling (rescue columns)\n",
    "* File notification for efficiency\n",
    "* Metadata columns for lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b1432a-84e0-4434-a9f4-55f6ee568581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üå≥ Decision Tree: Which Should I Use?\n",
    "\n",
    "```\n",
    "Start: Need to ingest files from cloud storage\n",
    "‚îÇ\n",
    "‚îú‚îÄ Do you need real-time/continuous ingestion?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí Use Auto Loader ‚ö°\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Continue...\n",
    "‚îÇ\n",
    "‚îú‚îÄ Do you have millions of files?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí Use Auto Loader ‚ö°\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Continue...\n",
    "‚îÇ\n",
    "‚îú‚îÄ Does your schema change frequently?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí Use Auto Loader ‚ö°\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Continue...\n",
    "‚îÇ\n",
    "‚îú‚îÄ Are you using SQL Warehouse only?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí Use COPY INTO üìä\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Continue...\n",
    "‚îÇ\n",
    "‚îú‚îÄ Is simplicity more important than features?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí Use COPY INTO üìä\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Use Auto Loader ‚ö°\n",
    "‚îÇ\n",
    "‚îî‚îÄ Default recommendation: Auto Loader ‚ö°\n",
    "```\n",
    "\n",
    "**Quick guide:**\n",
    "* **Simple batch loads** ‚Üí COPY INTO\n",
    "* **Everything else** ‚Üí Auto Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5f4d94f-84d7-4059-b130-9980a433393f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîÑ Side-by-Side Code Comparison\n",
    "\n",
    "**COPY INTO (SQL):**\n",
    "```sql\n",
    "-- Simple and straightforward\n",
    "COPY INTO main.default.target_table\n",
    "FROM '/path/to/files/'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "**Auto Loader (Python):**\n",
    "```python\n",
    "# More configuration, more features\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "  .load(source_path)\n",
    "\n",
    "df.writeStream \\\n",
    "  .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .trigger(availableNow=True) \\\n",
    "  .toTable(\"main.default.target_table\") \\\n",
    "  .awaitTermination()\n",
    "```\n",
    "\n",
    "**Key differences:**\n",
    "* COPY INTO: 5 lines of SQL\n",
    "* Auto Loader: 10+ lines of Python with more options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ea75940-0f5b-4db0-9e5d-5c74cb512df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Best Practices ‚úÖ\n",
    "\n",
    "Production-ready patterns for both COPY INTO and Auto Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631208fb-79fa-41e5-aeed-17253dcffeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä COPY INTO Best Practices\n",
    "\n",
    "**1. Use pattern matching for flexibility:**\n",
    "```sql\n",
    "COPY INTO table\n",
    "FROM '/path/to/data/year=2024/month=*/'\n",
    "FILEFORMAT = CSV\n",
    "```\n",
    "\n",
    "**2. Enable schema evolution when needed:**\n",
    "```sql\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "**3. Schedule with jobs:**\n",
    "```sql\n",
    "-- Run COPY INTO on a schedule (hourly, daily)\n",
    "-- Use Databricks Jobs or Workflows\n",
    "```\n",
    "\n",
    "**4. Monitor with COPY_HISTORY:**\n",
    "```sql\n",
    "-- Check what files were loaded\n",
    "SELECT * FROM main.default.customers_copy_into.copy_history\n",
    "```\n",
    "\n",
    "**5. Handle errors gracefully:**\n",
    "```sql\n",
    "-- Use COPY_OPTIONS to control error handling\n",
    "COPY_OPTIONS (\n",
    "  'force' = 'false',           -- Don't reprocess files\n",
    "  'mergeSchema' = 'true'       -- Allow schema changes\n",
    ")\n",
    "```\n",
    "\n",
    "**6. Test with small batches first:**\n",
    "```sql\n",
    "-- Test on a subset before full load\n",
    "COPY INTO table\n",
    "FROM '/path/to/data/batch1/'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc1e23e2-4b28-4f62-ad83-54b5bd219813",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check COPY INTO History"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View the history of COPY INTO operations\n",
    "-- This shows which files were loaded and when\n",
    "\n",
    "DESCRIBE HISTORY main.default.customers_copy_into\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "774df6ca-696d-446f-b5da-322fc67efe6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚ö° Auto Loader Best Practices\n",
    "\n",
    "**1. Always use checkpoints:**\n",
    "```python\n",
    ".option(\"checkpointLocation\", \"/path/to/checkpoint\")\n",
    "```\n",
    "‚ö†Ô∏è Never change checkpoint location - it tracks progress!\n",
    "\n",
    "**2. Use schema location:**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaLocation\", \"/path/to/schema\")\n",
    "```\n",
    "Stores inferred schema for consistency.\n",
    "\n",
    "**3. Enable schema evolution:**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "```\n",
    "\n",
    "**4. Use schema hints for critical columns:**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaHints\", \"id INT, amount DECIMAL(10,2)\")\n",
    "```\n",
    "\n",
    "**5. Use rescue columns for data quality:**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "```\n",
    "Unexpected data goes to `_rescued_data` column.\n",
    "\n",
    "**6. Use file notifications for scale:**\n",
    "```python\n",
    ".option(\"cloudFiles.useNotifications\", \"true\")\n",
    "```\n",
    "Much more efficient for large-scale ingestion.\n",
    "\n",
    "**7. Control batch size:**\n",
    "```python\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\", 1000)\n",
    "```\n",
    "Prevents overwhelming the cluster.\n",
    "\n",
    "**8. Use trigger modes appropriately:**\n",
    "```python\n",
    "# Batch mode (process once and stop)\n",
    ".trigger(availableNow=True)\n",
    "\n",
    "# Continuous mode (keep running)\n",
    ".trigger(processingTime='1 minute')\n",
    "\n",
    "# Micro-batch (process as data arrives)\n",
    ".trigger(once=True)  # Deprecated, use availableNow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03467041-6ede-4e59-be9b-94d6fc114d4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Error Handling Pattern"
    }
   },
   "outputs": [],
   "source": [
    "# Auto Loader with rescue columns for error handling\n",
    "# This captures malformed data instead of failing\n",
    "\n",
    "print(\"üõ°Ô∏è Auto Loader with rescue columns for error handling\\n\")\n",
    "\n",
    "rescue_checkpoint = \"/Volumes/main/default/ingestion_demo_data/checkpoints/rescue_demo\"\n",
    "\n",
    "df_with_rescue = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", rescue_checkpoint) \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\") \\\n",
    "  .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "  .load(source_path)\n",
    "\n",
    "print(\"Schema with rescue column:\")\n",
    "df_with_rescue.printSchema()\n",
    "\n",
    "print(\"\\nüí° Notice the '_rescued_data' column - this captures any data that doesn't fit the schema!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9d35968-3ad3-4a15-a476-9092f865b906",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Monitoring Auto Loader"
    }
   },
   "outputs": [],
   "source": [
    "# Monitor Auto Loader streaming queries\n",
    "# Check active streams and their progress\n",
    "\n",
    "print(\"üìä Monitoring Auto Loader streams\\n\")\n",
    "\n",
    "# List active streaming queries\n",
    "active_streams = spark.streams.active\n",
    "\n",
    "if len(active_streams) > 0:\n",
    "    print(f\"Active streams: {len(active_streams)}\\n\")\n",
    "    for stream in active_streams:\n",
    "        print(f\"Stream ID: {stream.id}\")\n",
    "        print(f\"Name: {stream.name}\")\n",
    "        print(f\"Status: {stream.status}\")\n",
    "        print(f\"Recent progress: {stream.recentProgress}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚úÖ No active streams (all completed)\")\n",
    "    print(\"\\nThis is expected since we used trigger(availableNow=True)\")\n",
    "    print(\"which processes all available data and stops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a39daa9f-cc45-4501-9998-e5a11d2864b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üöÄ Performance Tips\n",
    "\n",
    "### **COPY INTO Performance**\n",
    "\n",
    "‚úÖ **Partition your source data** - Use directory structure  \n",
    "‚úÖ **Use appropriate file sizes** - 128MB-1GB per file ideal  \n",
    "‚úÖ **Limit file patterns** - Be specific with paths  \n",
    "‚úÖ **Schedule during off-peak** - Reduce cluster contention  \n",
    "‚úÖ **Monitor with DESCRIBE HISTORY** - Track load times  \n",
    "\n",
    "### **Auto Loader Performance**\n",
    "\n",
    "‚úÖ **Use file notifications** - Much faster than directory listing  \n",
    "‚úÖ **Set maxFilesPerTrigger** - Control batch size  \n",
    "‚úÖ **Use schema hints** - Avoid inference overhead  \n",
    "‚úÖ **Optimize checkpoint location** - Use fast storage  \n",
    "‚úÖ **Monitor streaming metrics** - Check Spark UI  \n",
    "‚úÖ **Use appropriate trigger intervals** - Balance latency vs cost  \n",
    "\n",
    "### **General Tips**\n",
    "\n",
    "‚úÖ **Use Delta Lake** - Optimized for both methods  \n",
    "‚úÖ **Compress source files** - Reduce I/O  \n",
    "‚úÖ **Use Unity Catalog Volumes** - Modern best practice  \n",
    "‚úÖ **Test with small datasets** - Validate before production  \n",
    "‚úÖ **Monitor costs** - Streaming can be more expensive  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0c7513-8c87-4cc3-8a88-53261bfc47fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls to Avoid\n",
    "\n",
    "### **COPY INTO Pitfalls**\n",
    "\n",
    "‚ùå **Don't use force=true in production** - Reprocesses all files  \n",
    "‚ùå **Don't ignore schema evolution** - Plan for schema changes  \n",
    "‚ùå **Don't forget to schedule** - COPY INTO doesn't run automatically  \n",
    "‚ùå **Don't use for real-time** - It's batch-oriented  \n",
    "\n",
    "### **Auto Loader Pitfalls**\n",
    "\n",
    "‚ùå **Don't change checkpoint location** - Loses progress tracking  \n",
    "‚ùå **Don't skip schema location** - Can cause inconsistencies  \n",
    "‚ùå **Don't use continuous mode unnecessarily** - Costs more  \n",
    "‚ùå **Don't ignore rescue columns** - Monitor for data quality issues  \n",
    "‚ùå **Don't forget to stop streams** - Can run indefinitely  \n",
    "\n",
    "### **General Pitfalls**\n",
    "\n",
    "‚ùå **Don't use read_files() for incremental** - No tracking  \n",
    "‚ùå **Don't mix COPY INTO and Auto Loader** - Use one method per table  \n",
    "‚ùå **Don't ignore file sizes** - Too small = overhead, too large = memory issues  \n",
    "‚ùå **Don't skip testing** - Test schema evolution scenarios  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b1de40-fb3c-41ed-9694-16a3fdd76916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Quick Reference\n",
    "\n",
    "### **COPY INTO Template**\n",
    "```sql\n",
    "COPY INTO catalog.schema.table\n",
    "FROM 'source_path'\n",
    "FILEFORMAT = CSV|JSON|PARQUET\n",
    "FORMAT_OPTIONS (\n",
    "  'header' = 'true',\n",
    "  'inferSchema' = 'true'\n",
    ")\n",
    "COPY_OPTIONS (\n",
    "  'mergeSchema' = 'true',\n",
    "  'force' = 'false'\n",
    ")\n",
    "```\n",
    "\n",
    "### **Auto Loader Template**\n",
    "```python\n",
    "# Read stream\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "  .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(source_path)\n",
    "\n",
    "# Write stream\n",
    "query = df.writeStream \\\n",
    "  .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .trigger(availableNow=True) \\\n",
    "  .toTable(target_table)\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### **Monitoring**\n",
    "```sql\n",
    "-- COPY INTO history\n",
    "DESCRIBE HISTORY table_name\n",
    "\n",
    "-- Check table details\n",
    "DESCRIBE DETAIL table_name\n",
    "```\n",
    "\n",
    "```python\n",
    "# Auto Loader monitoring\n",
    "spark.streams.active  # List active streams\n",
    "query.status  # Check stream status\n",
    "query.recentProgress  # View progress\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc3401a2-3a05-45b4-bc9e-2c7c4bdd3fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the COPY INTO & Auto Loader demo!\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "‚úÖ **COPY INTO** - SQL-based incremental ingestion  \n",
    "‚úÖ **Auto Loader** - Streaming-based ingestion with cloudFiles  \n",
    "‚úÖ **Idempotency** - Both methods track processed files  \n",
    "‚úÖ **Schema Evolution** - Handle schema changes gracefully  \n",
    "‚úÖ **Comparison** - When to use each approach  \n",
    "‚úÖ **Best Practices** - Production-ready patterns  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Never reprocess all files** - Use COPY INTO or Auto Loader\n",
    "2. **COPY INTO for simplicity** - Great for batch loads\n",
    "3. **Auto Loader for scale** - Best for production pipelines\n",
    "4. **Schema evolution matters** - Plan for schema changes\n",
    "5. **Monitor your ingestion** - Use history and streaming metrics\n",
    "\n",
    "---\n",
    "\n",
    "### **Decision Summary:**\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| SQL Warehouse only | COPY INTO |\n",
    "| Simple batch loads | COPY INTO |\n",
    "| Real-time ingestion | Auto Loader |\n",
    "| Millions of files | Auto Loader |\n",
    "| Frequent schema changes | Auto Loader |\n",
    "| Production pipelines | Auto Loader |\n",
    "| Cost-sensitive batch | COPY INTO |\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "* Implement incremental ingestion in your pipelines\n",
    "* Set up file notifications for Auto Loader (S3 SQS)\n",
    "* Create monitoring dashboards\n",
    "* Explore Delta Live Tables (DLT) for declarative pipelines\n",
    "* Learn about Change Data Capture (CDC)\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources:**\n",
    "\n",
    "* [COPY INTO Documentation](https://docs.databricks.com/sql/language-manual/delta-copy-into.html)\n",
    "* [Auto Loader Documentation](https://docs.databricks.com/ingestion/auto-loader/index.html)\n",
    "* [Unity Catalog Volumes](https://docs.databricks.com/data-governance/unity-catalog/volumes.html)\n",
    "* [Delta Lake Best Practices](https://docs.databricks.com/delta/best-practices.html)\n",
    "\n",
    "---\n",
    "\n",
    "**You're now ready to build production-grade data ingestion pipelines!** üöÄ\n",
    "\n",
    "*Happy ingesting!*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "COPY INTO and Auto Loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
