{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d2559d-6c24-42d3-829a-71958b4fad46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks SQL Fundamentals - Interactive Demo\n",
    "\n",
    "Welcome! This demo will teach you the fundamentals of Databricks SQL and important concepts you need to know.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What is Databricks SQL?\n",
    "\n",
    "**Databricks SQL** is a SQL interface built on Apache Spark that provides:\n",
    "* ANSI SQL compliance with Spark extensions\n",
    "* Direct access to Unity Catalog tables\n",
    "* Optimized query execution with Photon engine\n",
    "* Integration with BI tools and dashboards\n",
    "\n",
    "**Key Difference from Standard SQL:**\n",
    "Databricks SQL queries are compiled into Spark execution plans, giving you distributed processing power!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š What You'll Learn\n",
    "\n",
    "1. **SQL Transformations** - SELECT, WHERE, CASE, GROUP BY, ORDER BY\n",
    "2. **Strict Typing** âš ï¸ - Type casting and compatibility gotchas\n",
    "3. **Store Assignment Policy** âš ï¸ - INSERT/CREATE TABLE type enforcement\n",
    "4. **EXPLAIN Plans** - See how SQL becomes Spark execution\n",
    "5. **Unity Catalog Integration** - Create and query tables\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Demo Scenario\n",
    "\n",
    "We'll analyze NYC taxi trip data to create hourly summaries - the same analysis from the PySpark lab, but using pure SQL!\n",
    "\n",
    "**Let's get started!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2754118-d0a2-4dc7-8c76-dca5f89add17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Explore Unity Catalog with SQL ðŸ”\n",
    "\n",
    "**Unity Catalog** is Databricks' unified governance solution for data and AI assets.\n",
    "\n",
    "**Three-level namespace:**\n",
    "* **Catalog** - Top-level container (e.g., `samples`, `main`)\n",
    "* **Schema** - Database within a catalog (e.g., `nyctaxi`, `default`)\n",
    "* **Table** - Actual data table (e.g., `trips`)\n",
    "\n",
    "**Full table reference:** `catalog.schema.table`\n",
    "\n",
    "Let's explore what's available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c220626c-d31a-4892-8589-a4d31e33093a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Available Catalogs"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List all catalogs you have access to\n",
    "SHOW CATALOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060c903b-92b1-49b5-b5f9-d18e2f6ef1f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Schemas in Samples"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List all schemas in the samples catalog\n",
    "SHOW SCHEMAS IN samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f93881-99fd-46e9-a51f-bf65490bcb37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Tables in NYC Taxi Schema"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List all tables in the nyctaxi schema\n",
    "SHOW TABLES IN samples.nyctaxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa36fc4d-638e-4d95-8c83-511e68c11511",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Describe the Trips Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- See the structure of the trips table\n",
    "DESCRIBE samples.nyctaxi.trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8fda34-d8ea-4946-ad47-89448ea045c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preview the Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Look at sample data\n",
    "SELECT *\n",
    "FROM samples.nyctaxi.trips\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3d87c6e-223d-4161-a105-a77e3a024ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. SQL Transformations ðŸ”„\n",
    "\n",
    "Let's build our analysis step by step, just like the PySpark lab!\n",
    "\n",
    "**Transformations we'll apply:**\n",
    "1. **SELECT** - Choose specific columns (like PySpark's `select()`)\n",
    "2. **WHERE** - Filter rows (like PySpark's `filter()`)\n",
    "3. **CASE WHEN** - Add calculated columns (like PySpark's `withColumn()` + `when()`)\n",
    "4. **GROUP BY** - Aggregate data (like PySpark's `groupBy()`)\n",
    "5. **ORDER BY** - Sort results (like PySpark's `orderBy()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3593a9-8b0d-4e50-8d92-cba7e0bbf840",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: SELECT Columns"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SELECT: Choose only the columns we need\n",
    "-- This is equivalent to PySpark's df.select()\n",
    "\n",
    "SELECT \n",
    "  tpep_pickup_datetime,\n",
    "  fare_amount,\n",
    "  trip_distance\n",
    "FROM samples.nyctaxi.trips\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd7701ba-9364-43ed-9a1f-aa1941f34bbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: WHERE Filter"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- WHERE: Filter for valid trips only\n",
    "-- This is equivalent to PySpark's df.filter()\n",
    "\n",
    "SELECT \n",
    "  tpep_pickup_datetime,\n",
    "  fare_amount,\n",
    "  trip_distance\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "  AND fare_amount < 500\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bde7ef8-be70-4038-bcd1-df3bfe89b18c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Add Calculated Columns"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Add calculated columns with CASE WHEN\n",
    "-- This is equivalent to PySpark's withColumn()\n",
    "\n",
    "SELECT \n",
    "  tpep_pickup_datetime,\n",
    "  fare_amount,\n",
    "  trip_distance,\n",
    "  \n",
    "  -- Extract hour from timestamp (like PySpark's hour() function)\n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  \n",
    "  -- Categorize revenue (like PySpark's when().otherwise())\n",
    "  CASE \n",
    "    WHEN fare_amount < 10 THEN 'Low'\n",
    "    WHEN fare_amount <= 30 THEN 'Medium'\n",
    "    ELSE 'High'\n",
    "  END AS revenue_category\n",
    "  \n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "  AND fare_amount < 500\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfb52f6-3795-491b-ac7d-864c8138370c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: GROUP BY Aggregation"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GROUP BY: Aggregate data by hour\n",
    "-- This is equivalent to PySpark's groupBy().agg()\n",
    "\n",
    "SELECT \n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  \n",
    "  -- Aggregation functions\n",
    "  COUNT(*) AS trip_count,\n",
    "  ROUND(SUM(fare_amount), 2) AS total_revenue,\n",
    "  ROUND(AVG(fare_amount), 2) AS avg_fare,\n",
    "  ROUND(AVG(trip_distance), 2) AS avg_distance\n",
    "  \n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "  AND fare_amount < 500\n",
    "GROUP BY HOUR(tpep_pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a74540-7e96-48b0-917e-73eaca6d9af8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: ORDER BY Sort"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ORDER BY: Sort results chronologically\n",
    "-- This is equivalent to PySpark's orderBy()\n",
    "\n",
    "SELECT \n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  COUNT(*) AS trip_count,\n",
    "  ROUND(SUM(fare_amount), 2) AS total_revenue,\n",
    "  ROUND(AVG(fare_amount), 2) AS avg_fare,\n",
    "  ROUND(AVG(trip_distance), 2) AS avg_distance\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "  AND fare_amount < 500\n",
    "GROUP BY HOUR(tpep_pickup_datetime)\n",
    "ORDER BY pickup_hour ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af6533d-f8d2-4869-8edc-f6e179ced17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŽ¯ Complete Query\n",
    "\n",
    "Here's the full query with all transformations combined:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  COUNT(*) AS trip_count,\n",
    "  ROUND(SUM(fare_amount), 2) AS total_revenue,\n",
    "  ROUND(AVG(fare_amount), 2) AS avg_fare,\n",
    "  ROUND(AVG(trip_distance), 2) AS avg_distance\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "  AND fare_amount < 500\n",
    "GROUP BY HOUR(tpep_pickup_datetime)\n",
    "ORDER BY pickup_hour ASC\n",
    "```\n",
    "\n",
    "**SQL vs PySpark Comparison:**\n",
    "\n",
    "| Operation | PySpark | SQL |\n",
    "|-----------|---------|-----|\n",
    "| Select columns | `.select()` | `SELECT` |\n",
    "| Filter rows | `.filter()` | `WHERE` |\n",
    "| Add columns | `.withColumn()` | `CASE WHEN`, functions |\n",
    "| Aggregate | `.groupBy().agg()` | `GROUP BY` + aggregates |\n",
    "| Sort | `.orderBy()` | `ORDER BY` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30623ba9-c260-42a5-9ac0-6c2ad7171fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Gotcha #1: Strict Typing âš ï¸\n",
    "\n",
    "**Important:** Databricks SQL uses **strict typing** - operations between incompatible types will fail!\n",
    "\n",
    "**What is strict typing?**\n",
    "* Type mismatches cause errors (not automatic conversion)\n",
    "* You must explicitly cast types\n",
    "* Prevents silent data corruption\n",
    "\n",
    "**Common scenarios:**\n",
    "* Comparing STRING to INT\n",
    "* Adding STRING to NUMBER\n",
    "* Joining on mismatched types\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de43113e-2e49-417e-bb86-1156649a911e",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768732607480}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Example: Type Mismatch Error"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This may not fail if ansi_mode is set to false (default in some Databricks environments)\n",
    "-- In that case, Databricks implicitly casts STRING to INT or DOUBLE for comparison, this is the behaviour in Serverless Workspaces\n",
    "\n",
    "SELECT \n",
    "  '5' > 10 AS comparison_result,\n",
    "  '5' as orginal_value,\n",
    "  10 as int_value\n",
    "\n",
    "-- If ansi_mode = true, this will fail due to type mismatch\n",
    "-- If ansi_mode = false, this will return false (because '5' is cast to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296b10bf-5fd6-400b-89a8-1323c66f1619",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution: Explicit Casting"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Solution: Cast the string to INT\n",
    "\n",
    "SELECT \n",
    "  CAST('5' AS INT) > 10 AS comparison_result,\n",
    "  CAST('5' AS INT) AS casted_value,\n",
    "  10 AS int_value\n",
    "\n",
    "-- Now it works! Result: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a99d3d87-3af1-418e-a689-ae3fb73a4eed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Real-World Example"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Real scenario: passenger_count might be stored as STRING in some systems\n",
    "-- Let's simulate this\n",
    "\n",
    "SELECT \n",
    "  CAST(passenger_count AS STRING) AS passenger_count_string,\n",
    "  fare_amount\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE fare_amount > 0\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bdf2ff4-84d0-4d64-9f15-556f79abe0a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Type Mismatch in WHERE"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This would FAIL if passenger_count was STRING:\n",
    "-- WHERE passenger_count_string > 2  (comparing STRING to INT)\n",
    "\n",
    "-- Correct approach: Cast before comparison\n",
    "SELECT \n",
    "  passenger_count,\n",
    "  fare_amount\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  CAST(passenger_count AS INT) > 2  -- Explicit cast\n",
    "  AND fare_amount > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dbd5259-f09b-4258-8e60-fe1870f55cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ› ï¸ Common Type Casting Functions\n",
    "\n",
    "**Casting syntax:**\n",
    "```sql\n",
    "CAST(column AS type)\n",
    "-- or\n",
    "column::type  -- PostgreSQL-style shorthand\n",
    "```\n",
    "\n",
    "**Common types:**\n",
    "* `INT` / `BIGINT` - Integer numbers\n",
    "* `DOUBLE` / `DECIMAL(p,s)` - Floating point numbers\n",
    "* `STRING` - Text\n",
    "* `BOOLEAN` - True/False\n",
    "* `DATE` - Date only\n",
    "* `TIMESTAMP` - Date and time\n",
    "\n",
    "**Examples:**\n",
    "```sql\n",
    "CAST('123' AS INT)                    -- String to integer\n",
    "CAST(123 AS STRING)                   -- Integer to string\n",
    "CAST('2024-01-15' AS DATE)           -- String to date\n",
    "CAST(fare_amount AS DECIMAL(10,2))   -- Specify precision\n",
    "'123'::INT                            -- Shorthand syntax\n",
    "```\n",
    "\n",
    "**âš ï¸ Best Practice:**\n",
    "Always cast explicitly when working with different types - don't rely on implicit conversion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c788cda-2a02-4e5f-a91a-34453fb0f110",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Type Checking Query"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check data types in your query results\n",
    "-- Use DESCRIBE to see column types\n",
    "\n",
    "DESCRIBE QUERY (\n",
    "  SELECT \n",
    "    passenger_count,\n",
    "    CAST(passenger_count AS STRING) AS passenger_string,\n",
    "    fare_amount,\n",
    "    CAST(fare_amount AS INT) AS fare_int\n",
    "  FROM samples.nyctaxi.trips\n",
    "  LIMIT 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30511c48-7a11-4b31-af60-70e6c27b1f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Gotcha #2: Store Assignment Policy âš ï¸\n",
    "\n",
    "**What is Store Assignment Policy?**\n",
    "\n",
    "Databricks SQL enforces **strict type checking** when writing data to tables:\n",
    "* INSERT statements must match column types exactly\n",
    "* CREATE TABLE AS SELECT (CTAS) infers types from the query\n",
    "* Type mismatches cause errors, not silent conversions\n",
    "\n",
    "**Why does this matter?**\n",
    "* Prevents data corruption\n",
    "* Ensures data quality\n",
    "* Makes schema evolution explicit\n",
    "\n",
    "**Common scenarios:**\n",
    "* Inserting STRING into INT column\n",
    "* Inserting larger precision into smaller precision\n",
    "* NULL handling in NOT NULL columns\n",
    "\n",
    "Let's see examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "859d8840-a3e2-463d-acef-e8940d9cb751",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a Test Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- First, let's create a test table with specific types\n",
    "-- We'll use this to demonstrate store assignment policy\n",
    "\n",
    "CREATE OR REPLACE TABLE main.default.test_store_assignment (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  amount DECIMAL(10, 2),\n",
    "  created_date DATE\n",
    ")\n",
    "COMMENT 'Test table for store assignment policy demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2849b7dd-9671-44ec-b560-7bbfab086a75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Successful INSERT"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This INSERT works - types match exactly\n",
    "\n",
    "INSERT INTO main.default.test_store_assignment\n",
    "VALUES \n",
    "  (1, 'Item A', 99.99, '2024-01-15'),\n",
    "  (2, 'Item B', 149.50, '2024-01-16')\n",
    "\n",
    "-- Check the data\n",
    "SELECT * FROM main.default.test_store_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b59be926-d942-45ff-89b2-694f7b4de280",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Failed INSERT - Type Mismatch"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This will FAIL - trying to insert STRING into INT column\n",
    "\n",
    "INSERT INTO main.default.test_store_assignment\n",
    "VALUES \n",
    "  ('three', 'Item C', 199.99, '2024-01-17')  -- 'three' is STRING, not INT\n",
    "\n",
    "-- ERROR: Cannot safely cast 'three':STRING to id:INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a9d55f4-da85-46f9-b08a-cdf62c60a255",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Failed INSERT - Precision Mismatch"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This will FAIL - decimal precision mismatch\n",
    "-- Table expects DECIMAL(10,2) but we're providing more precision\n",
    "\n",
    "INSERT INTO main.default.test_store_assignment\n",
    "VALUES \n",
    "  (3, 'Item D', 199.999, '2024-01-17')  -- 3 decimal places, table expects 2\n",
    "\n",
    "-- ERROR: Decimal precision/scale mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73f9a99b-ffc0-4f17-88e7-6f3448ce5a0f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution: Explicit Casting"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Solution: Cast to the correct type\n",
    "\n",
    "INSERT INTO main.default.test_store_assignment\n",
    "VALUES \n",
    "  (CAST('3' AS INT), 'Item C', CAST(199.999 AS DECIMAL(10,2)), CAST('2024-01-17' AS DATE))\n",
    "\n",
    "-- This works! The cast handles the conversion\n",
    "SELECT * FROM main.default.test_store_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30804acd-8978-4214-b073-c06d4dc0322a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CTAS Type Inference"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE TABLE AS SELECT (CTAS) infers types from the query\n",
    "-- Be careful - the inferred types might not be what you expect!\n",
    "\n",
    "CREATE OR REPLACE TABLE main.default.test_ctas_types AS\n",
    "SELECT \n",
    "  1 AS int_col,                    -- Inferred as INT\n",
    "  1.5 AS double_col,                -- Inferred as DOUBLE\n",
    "  '2024-01-15' AS string_col,       -- Inferred as STRING (not DATE!)\n",
    "  CAST('2024-01-15' AS DATE) AS date_col,  -- Explicitly DATE\n",
    "  CURRENT_TIMESTAMP() AS timestamp_col     -- TIMESTAMP\n",
    "\n",
    "-- Check the inferred types\n",
    "DESCRIBE main.default.test_ctas_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e6e3a27-19e3-4190-98c5-b8b1fae40e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### âœ… Store Assignment Policy Best Practices\n",
    "\n",
    "**1. Always specify types explicitly in CREATE TABLE:**\n",
    "```sql\n",
    "CREATE TABLE my_table (\n",
    "  id INT NOT NULL,\n",
    "  amount DECIMAL(10,2),\n",
    "  name STRING\n",
    ")\n",
    "```\n",
    "\n",
    "**2. Use CAST in INSERT statements:**\n",
    "```sql\n",
    "INSERT INTO my_table\n",
    "VALUES (\n",
    "  CAST(value AS INT),\n",
    "  CAST(amount AS DECIMAL(10,2)),\n",
    "  CAST(name AS STRING)\n",
    ")\n",
    "```\n",
    "\n",
    "**3. Be explicit in CTAS:**\n",
    "```sql\n",
    "CREATE TABLE my_table AS\n",
    "SELECT \n",
    "  CAST(id AS INT) AS id,\n",
    "  CAST(amount AS DECIMAL(10,2)) AS amount,\n",
    "  CAST(date_string AS DATE) AS date_col\n",
    "FROM source_table\n",
    "```\n",
    "\n",
    "**4. Check types before inserting:**\n",
    "```sql\n",
    "DESCRIBE target_table  -- See expected types\n",
    "DESCRIBE QUERY (SELECT ...)  -- See query result types\n",
    "```\n",
    "\n",
    "**âš ï¸ Key Takeaway:**\n",
    "Databricks SQL will NOT silently convert types - you must be explicit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "894ef931-f43a-473b-93f0-d6d84182a966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. SQL is Just an Abstraction for Spark! âš¡\n",
    "\n",
    "**Key Insight:** Databricks SQL queries are compiled into Spark execution plans!\n",
    "\n",
    "**What does this mean?**\n",
    "* Your SQL query is translated into Spark operations\n",
    "* Executed on the same Spark engine as PySpark/Scala\n",
    "* Benefits from Spark optimizations (Catalyst, Photon)\n",
    "* Can see the execution plan with `EXPLAIN`\n",
    "\n",
    "**Why is this important?**\n",
    "* Understanding the plan helps debug performance issues\n",
    "* See what Spark is actually doing\n",
    "* Identify optimization opportunities\n",
    "* Compare different query approaches\n",
    "\n",
    "Let's explore the execution plan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4e82546-a21f-4bc8-9d42-6b82e3759bd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Query EXPLAIN"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- EXPLAIN shows the execution plan for a query\n",
    "-- Let's start with a simple query\n",
    "\n",
    "EXPLAIN\n",
    "SELECT \n",
    "  passenger_count,\n",
    "  fare_amount\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE fare_amount > 50\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296830f0-8b3d-41ee-a4a4-9e6ede825dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ“Š Understanding EXPLAIN Output\n",
    "\n",
    "**The execution plan shows:**\n",
    "\n",
    "1. **Physical Plan** - How Spark will actually execute the query\n",
    "2. **Operations** - Each step in the execution (scan, filter, project, etc.)\n",
    "3. **Data Flow** - Bottom-up execution (read from bottom to top)\n",
    "\n",
    "**Common operations you'll see:**\n",
    "\n",
    "* **Scan** - Reading data from a table\n",
    "* **Filter** - WHERE clause conditions\n",
    "* **Project** - SELECT column selection\n",
    "* **HashAggregate** - GROUP BY aggregations\n",
    "* **Sort** - ORDER BY sorting\n",
    "* **Exchange** - Data shuffle between nodes\n",
    "* **BroadcastExchange** - Broadcasting small tables\n",
    "\n",
    "**Reading the plan:**\n",
    "```\n",
    "== Physical Plan ==\n",
    "(3) GlobalLimit 10              â† LIMIT 10\n",
    "(2) Filter (fare_amount > 50)   â† WHERE clause\n",
    "(1) Scan table                  â† FROM clause (start here)\n",
    "```\n",
    "\n",
    "Read from bottom to top!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "093109d3-c888-4707-a8be-0f31d1bfdbfb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex Query EXPLAIN"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- EXPLAIN for our aggregation query\n",
    "-- This shows a more complex plan with shuffles\n",
    "\n",
    "EXPLAIN\n",
    "SELECT \n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  COUNT(*) AS trip_count,\n",
    "  ROUND(SUM(fare_amount), 2) AS total_revenue,\n",
    "  ROUND(AVG(fare_amount), 2) AS avg_fare\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "GROUP BY HOUR(tpep_pickup_datetime)\n",
    "ORDER BY pickup_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f6f6f7-a901-44e1-aea4-62492034e7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ” EXPLAIN Variations\n",
    "\n",
    "Databricks SQL provides different EXPLAIN modes:\n",
    "\n",
    "**1. EXPLAIN (default) - Physical Plan**\n",
    "```sql\n",
    "EXPLAIN\n",
    "SELECT * FROM table\n",
    "```\n",
    "Shows the actual execution plan.\n",
    "\n",
    "**2. EXPLAIN EXTENDED - Detailed Information**\n",
    "```sql\n",
    "EXPLAIN EXTENDED\n",
    "SELECT * FROM table\n",
    "```\n",
    "Shows parsed logical plan, analyzed plan, optimized plan, and physical plan.\n",
    "\n",
    "**3. EXPLAIN FORMATTED - Pretty Print**\n",
    "```sql\n",
    "EXPLAIN FORMATTED\n",
    "SELECT * FROM table\n",
    "```\n",
    "Shows plan in a more readable tree format.\n",
    "\n",
    "**4. EXPLAIN COST - Cost-Based Optimization**\n",
    "```sql\n",
    "EXPLAIN COST\n",
    "SELECT * FROM table\n",
    "```\n",
    "Shows estimated costs for each operation.\n",
    "\n",
    "Let's try EXPLAIN EXTENDED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2887a8-a523-47cf-aff0-df404a297104",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EXPLAIN EXTENDED Example"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- EXPLAIN EXTENDED shows all optimization stages\n",
    "\n",
    "EXPLAIN EXTENDED\n",
    "SELECT \n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  COUNT(*) AS trip_count\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE fare_amount > 0\n",
    "GROUP BY HOUR(tpep_pickup_datetime)\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e30567fd-adb2-4020-a50c-d67721b36997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’¡ Key Insights from EXPLAIN\n",
    "\n",
    "**What to look for in execution plans:**\n",
    "\n",
    "**1. Scan Operations**\n",
    "* **ColumnarToRow** - Reading from columnar format (Parquet/Delta)\n",
    "* **Partition filters** - Pushed down to storage\n",
    "* **Data skipping** - Using statistics to skip files\n",
    "\n",
    "**2. Shuffle Operations (Exchange)**\n",
    "* **HashPartitioning** - Data redistributed by hash of key\n",
    "* **RoundRobinPartitioning** - Even distribution\n",
    "* **SinglePartition** - All data to one partition\n",
    "* âš ï¸ Shuffles are expensive - minimize them!\n",
    "\n",
    "**3. Aggregations**\n",
    "* **HashAggregate** - Hash-based aggregation (fast)\n",
    "* **Partial** - Pre-aggregation before shuffle\n",
    "* **Final** - Final aggregation after shuffle\n",
    "\n",
    "**4. Optimizations**\n",
    "* **Predicate pushdown** - Filters applied early\n",
    "* **Column pruning** - Only read needed columns\n",
    "* **Constant folding** - Evaluate constants at compile time\n",
    "* **AQE** - Adaptive Query Execution optimizations\n",
    "\n",
    "**ðŸŽ¯ Pro Tip:**\n",
    "Compare EXPLAIN output for different query approaches to choose the most efficient one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88a93b98-4d78-4ef7-970e-ca6f9cf43625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Create the Summary Table ðŸ’¾\n",
    "\n",
    "Now let's create our final table in Unity Catalog using **CREATE TABLE AS SELECT (CTAS)**.\n",
    "\n",
    "**CTAS Benefits:**\n",
    "* Creates table and inserts data in one statement\n",
    "* Automatically infers schema from query\n",
    "* Efficient - no separate INSERT needed\n",
    "* Perfect for ETL pipelines\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE catalog.schema.table_name AS\n",
    "SELECT ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b4c21e-f65a-49ee-b47e-ae9005db8d48",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Hourly Summary Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the taxi hourly summary table\n",
    "-- This combines all our transformations!\n",
    "\n",
    "CREATE OR REPLACE TABLE main.default.taxi_hourly_summary AS\n",
    "SELECT \n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  COUNT(*) AS trip_count,\n",
    "  ROUND(SUM(fare_amount), 2) AS total_revenue,\n",
    "  ROUND(AVG(fare_amount), 2) AS avg_fare,\n",
    "  ROUND(AVG(trip_distance), 2) AS avg_distance\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "  AND fare_amount < 500\n",
    "GROUP BY HOUR(tpep_pickup_datetime)\n",
    "ORDER BY pickup_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e438289-61cb-45f7-93f1-3b146d80470d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Table Creation"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify the table was created successfully\n",
    "\n",
    "SELECT * FROM main.default.taxi_hourly_summary\n",
    "ORDER BY pickup_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b27ab23-20e1-4113-bf08-2afd3345eb59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Table Schema"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the schema that was inferred from our query\n",
    "\n",
    "DESCRIBE main.default.taxi_hourly_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92eb6542-7b78-41e1-b04e-9fc871193e76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Table Properties"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- See detailed table information\n",
    "\n",
    "DESCRIBE EXTENDED main.default.taxi_hourly_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6428bba8-6683-4f41-87f2-ada6db9bac9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EXPLAIN the CTAS"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Let's see the execution plan for our complete query\n",
    "-- This shows how Spark executes the entire transformation pipeline\n",
    "\n",
    "EXPLAIN\n",
    "SELECT \n",
    "  HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "  COUNT(*) AS trip_count,\n",
    "  ROUND(SUM(fare_amount), 2) AS total_revenue,\n",
    "  ROUND(AVG(fare_amount), 2) AS avg_fare,\n",
    "  ROUND(AVG(trip_distance), 2) AS avg_distance\n",
    "FROM samples.nyctaxi.trips\n",
    "WHERE \n",
    "  fare_amount > 0\n",
    "  AND trip_distance > 0\n",
    "  AND fare_amount < 500\n",
    "GROUP BY HOUR(tpep_pickup_datetime)\n",
    "ORDER BY pickup_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c60e1ca-67c7-4789-806f-dd35f99082e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ” Analyzing the Execution Plan\n",
    "\n",
    "**In the plan above, you should see:**\n",
    "\n",
    "1. **Scan** - Reading from `samples.nyctaxi.trips`\n",
    "   * Columnar scan (efficient Parquet/Delta reading)\n",
    "   * Partition pruning if applicable\n",
    "\n",
    "2. **Filter** - WHERE clause conditions\n",
    "   * `fare_amount > 0 AND trip_distance > 0 AND fare_amount < 500`\n",
    "   * Applied early (predicate pushdown)\n",
    "\n",
    "3. **Project** - Extracting HOUR and selecting columns\n",
    "   * `HOUR(tpep_pickup_datetime)`\n",
    "\n",
    "4. **HashAggregate (Partial)** - Pre-aggregation\n",
    "   * Happens before shuffle\n",
    "   * Reduces data movement\n",
    "\n",
    "5. **Exchange** - Shuffle by pickup_hour\n",
    "   * Data redistributed across executors\n",
    "   * âš ï¸ This is where skew could occur!\n",
    "\n",
    "6. **HashAggregate (Final)** - Final aggregation\n",
    "   * Completes the GROUP BY\n",
    "\n",
    "7. **Sort** - ORDER BY pickup_hour\n",
    "   * Final sorting step\n",
    "\n",
    "**ðŸ’¡ Key Insight:**\n",
    "This is the SAME execution plan you'd get with PySpark! SQL is just a different syntax for the same Spark operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3276992-8a5a-41d2-9405-794f416ce6ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ”„ SQL vs PySpark: Same Result, Different Syntax\n",
    "\n",
    "Let's compare how we achieved the same result in both languages:\n",
    "\n",
    "### **Complete Comparison**\n",
    "\n",
    "| Operation | SQL | PySpark |\n",
    "|-----------|-----|----------|\n",
    "| **Read data** | `SELECT * FROM table` | `spark.table(\"table\")` |\n",
    "| **Select columns** | `SELECT col1, col2` | `.select(\"col1\", \"col2\")` |\n",
    "| **Filter rows** | `WHERE condition` | `.filter(condition)` |\n",
    "| **Add column** | `CASE WHEN ... END AS col` | `.withColumn(\"col\", when()...)` |\n",
    "| **Extract hour** | `HOUR(timestamp)` | `hour(\"timestamp\")` |\n",
    "| **Aggregate** | `GROUP BY col` | `.groupBy(\"col\").agg(...)` |\n",
    "| **Count** | `COUNT(*)` | `count(\"*\")` |\n",
    "| **Sum** | `SUM(col)` | `sum(\"col\")` |\n",
    "| **Average** | `AVG(col)` | `avg(\"col\")` |\n",
    "| **Round** | `ROUND(col, 2)` | `round(col, 2)` |\n",
    "| **Sort** | `ORDER BY col` | `.orderBy(\"col\")` |\n",
    "| **Create table** | `CREATE TABLE AS SELECT` | `.write.saveAsTable()` |\n",
    "| **Explain plan** | `EXPLAIN query` | `df.explain()` |\n",
    "\n",
    "### **Key Insight:**\n",
    "\n",
    "Both SQL and PySpark compile to the **same Spark execution plan**! Choose based on:\n",
    "* **SQL** - Familiar syntax, great for analytics, BI tool integration\n",
    "* **PySpark** - Programmatic control, complex logic, Python ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f976dbc-0689-486f-b014-019d8710e83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… Databricks SQL Best Practices\n",
    "\n",
    "### **1. Type Safety**\n",
    "\n",
    "âœ… Always use explicit CAST for type conversions  \n",
    "âœ… Check types with DESCRIBE before INSERT  \n",
    "âœ… Use DESCRIBE QUERY to verify result types  \n",
    "âŒ Don't rely on implicit type conversion  \n",
    "\n",
    "### **2. Store Assignment Policy**\n",
    "\n",
    "âœ… Define explicit schemas in CREATE TABLE  \n",
    "âœ… Cast values in INSERT statements  \n",
    "âœ… Be explicit in CTAS with CAST  \n",
    "âŒ Don't assume type inference matches your intent  \n",
    "\n",
    "### **3. Performance**\n",
    "\n",
    "âœ… Use EXPLAIN to understand query execution  \n",
    "âœ… Look for expensive shuffles (Exchange)  \n",
    "âœ… Minimize data movement  \n",
    "âœ… Use partition pruning when possible  \n",
    "âœ… Enable AQE for automatic optimization  \n",
    "\n",
    "### **4. Unity Catalog**\n",
    "\n",
    "âœ… Use three-level namespace: `catalog.schema.table`  \n",
    "âœ… Leverage Unity Catalog for governance  \n",
    "âœ… Use SHOW commands to explore  \n",
    "âœ… Use DESCRIBE to understand schemas  \n",
    "\n",
    "### **5. Query Development**\n",
    "\n",
    "âœ… Test with LIMIT during development  \n",
    "âœ… Use display() for interactive exploration  \n",
    "âœ… Add comments to complex queries  \n",
    "âœ… Use CTEs for readability  \n",
    "âœ… Compare EXPLAIN plans for optimization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43bf13ba-619a-411f-a831-4bd70c575f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âš ï¸ Common Databricks SQL Gotchas\n",
    "\n",
    "### **1. Strict Typing**\n",
    "\n",
    "**Problem:**\n",
    "```sql\n",
    "SELECT '5' > 10  -- ERROR: Cannot compare STRING to INT\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```sql\n",
    "SELECT CAST('5' AS INT) > 10  -- Works!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Store Assignment Policy**\n",
    "\n",
    "**Problem:**\n",
    "```sql\n",
    "CREATE TABLE t (id INT);\n",
    "INSERT INTO t VALUES ('123');  -- ERROR: STRING cannot be inserted into INT\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```sql\n",
    "INSERT INTO t VALUES (CAST('123' AS INT));  -- Works!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Date/Timestamp Strings**\n",
    "\n",
    "**Problem:**\n",
    "```sql\n",
    "CREATE TABLE t AS SELECT '2024-01-15' AS date_col;\n",
    "-- date_col is STRING, not DATE!\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```sql\n",
    "CREATE TABLE t AS SELECT CAST('2024-01-15' AS DATE) AS date_col;\n",
    "-- Now date_col is DATE type\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. NULL Handling**\n",
    "\n",
    "**Problem:**\n",
    "```sql\n",
    "CREATE TABLE t (id INT NOT NULL);\n",
    "INSERT INTO t VALUES (NULL);  -- ERROR: NULL in NOT NULL column\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```sql\n",
    "-- Either allow NULLs or provide default values\n",
    "CREATE TABLE t (id INT);  -- Allows NULL\n",
    "-- or\n",
    "INSERT INTO t VALUES (COALESCE(NULL, 0));  -- Provide default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b04fe13b-9a13-440e-942f-c2e255785da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“š Quick Reference Guide\n",
    "\n",
    "### **Exploring Unity Catalog**\n",
    "```sql\n",
    "SHOW CATALOGS\n",
    "SHOW SCHEMAS IN catalog_name\n",
    "SHOW TABLES IN catalog.schema\n",
    "DESCRIBE catalog.schema.table\n",
    "DESCRIBE EXTENDED catalog.schema.table\n",
    "```\n",
    "\n",
    "### **Type Casting**\n",
    "```sql\n",
    "CAST(column AS INT)\n",
    "CAST(column AS DECIMAL(10,2))\n",
    "CAST(column AS DATE)\n",
    "CAST(column AS TIMESTAMP)\n",
    "column::INT  -- Shorthand\n",
    "```\n",
    "\n",
    "### **Aggregations**\n",
    "```sql\n",
    "COUNT(*), COUNT(DISTINCT col)\n",
    "SUM(col), AVG(col)\n",
    "MIN(col), MAX(col)\n",
    "ROUND(col, decimals)\n",
    "```\n",
    "\n",
    "### **Date/Time Functions**\n",
    "```sql\n",
    "HOUR(timestamp), DAY(date), MONTH(date), YEAR(date)\n",
    "DATE_TRUNC('hour', timestamp)\n",
    "DATE_ADD(date, days)\n",
    "DATEDIFF(date1, date2)\n",
    "CURRENT_DATE(), CURRENT_TIMESTAMP()\n",
    "```\n",
    "\n",
    "### **Conditional Logic**\n",
    "```sql\n",
    "CASE \n",
    "  WHEN condition1 THEN value1\n",
    "  WHEN condition2 THEN value2\n",
    "  ELSE default_value\n",
    "END\n",
    "```\n",
    "\n",
    "### **Table Operations**\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE catalog.schema.table AS SELECT ...\n",
    "INSERT INTO catalog.schema.table VALUES (...)\n",
    "INSERT INTO catalog.schema.table SELECT ...\n",
    "DROP TABLE catalog.schema.table\n",
    "```\n",
    "\n",
    "### **Debugging**\n",
    "```sql\n",
    "EXPLAIN query\n",
    "EXPLAIN EXTENDED query\n",
    "DESCRIBE QUERY (SELECT ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5cdcc9-0ee2-43c6-abdc-b6d03dc41498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the Databricks SQL Fundamentals demo!\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "âœ… **SQL Transformations** - SELECT, WHERE, CASE, GROUP BY, ORDER BY  \n",
    "âœ… **Strict Typing** - Explicit casting required for type safety  \n",
    "âœ… **Store Assignment Policy** - Type enforcement in INSERT/CREATE  \n",
    "âœ… **EXPLAIN Plans** - SQL queries compile to Spark execution plans  \n",
    "âœ… **Unity Catalog** - Modern data governance and table management  \n",
    "âœ… **SQL = Spark** - Same engine, different syntax  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Databricks SQL is Spark** - Your queries run on the same distributed engine\n",
    "2. **Type safety matters** - Always cast explicitly to avoid errors\n",
    "3. **EXPLAIN is your friend** - Use it to understand and optimize queries\n",
    "4. **Unity Catalog is the standard** - Use three-level namespace\n",
    "5. **SQL and PySpark are equivalent** - Choose based on your use case\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "* Practice writing complex SQL queries\n",
    "* Explore advanced SQL functions (window functions, CTEs, etc.)\n",
    "* Learn about Delta Lake features (MERGE, time travel, etc.)\n",
    "* Study query optimization techniques\n",
    "* Build SQL-based data pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources:**\n",
    "\n",
    "* [Databricks SQL Documentation](https://docs.databricks.com/sql/)\n",
    "* [SQL Language Reference](https://docs.databricks.com/sql/language-manual/)\n",
    "* [Unity Catalog Guide](https://docs.databricks.com/data-governance/unity-catalog/)\n",
    "* [Query Optimization](https://docs.databricks.com/optimizations/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy querying!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8102779815291020,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks SQL Fundamentals",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
